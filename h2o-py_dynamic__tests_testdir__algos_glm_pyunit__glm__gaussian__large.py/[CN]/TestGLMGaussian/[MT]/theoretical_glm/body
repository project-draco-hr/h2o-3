def theoretical_glm(self, training_data_file, test_data_file, has_categorical, true_one_hot, validation_data_file=''):
    '\n        This function is written to load in a training/test data sets with predictors followed by the response\n        as the last column.  We then calculate the weights/bias and the p-values using derived formulae\n        off the web.\n\n        :param training_data_file: string representing the training data set filename\n        :param test_data_file:  string representing the test data set filename\n        :param has_categorical: bool indicating if the data set contains mixed predictors (both enum and real)\n        :param true_one_hot:  bool True: true one hot encoding is used.  False: reference level plus one hot\n        encoding is used\n        :param validation_data_file: optional string, denoting validation file so that we can concatenate\n         training and validation data sets into a big training set since H2O model is using a training\n         and a validation data set.\n\n        :return: a tuple containing weights, p-values, training data set MSE and test data set MSE\n\n        '
    training_data_xy = np.asmatrix(np.genfromtxt(training_data_file, delimiter=',', dtype=None))
    test_data_xy = np.asmatrix(np.genfromtxt(test_data_file, delimiter=',', dtype=None))
    if (len(validation_data_file) > 0):
        temp_data_xy = np.asmatrix(np.genfromtxt(validation_data_file, delimiter=',', dtype=None))
        training_data_xy = np.concatenate((training_data_xy, temp_data_xy), axis=0)
    if has_categorical:
        training_data_xy = pyunit_utils.encode_enum_dataset(training_data_xy, self.enum_level_vec, self.enum_col, true_one_hot, np.any(training_data_xy))
        test_data_xy = pyunit_utils.encode_enum_dataset(test_data_xy, self.enum_level_vec, self.enum_col, true_one_hot, np.any(training_data_xy))
    if np.isnan(training_data_xy).any():
        inds = np.where(np.isnan(training_data_xy))
        col_means = np.asarray(np.nanmean(training_data_xy, axis=0))[0]
        training_data_xy[inds] = np.take(col_means, inds[1])
        if np.isnan(test_data_xy).any():
            inds = np.where(np.isnan(test_data_xy))
            test_data_xy = pyunit_utils.replace_nan_with_mean(test_data_xy, inds, col_means)
    (num_row, num_col) = training_data_xy.shape
    dof = (num_row - num_col)
    response_y = training_data_xy[:, (num_col - 1)]
    training_data = training_data_xy[:, range(0, (num_col - 1))]
    temp_ones = np.asmatrix(np.ones(num_row)).transpose()
    x_mat = np.concatenate((temp_ones, training_data), axis=1)
    mat_inv = np.linalg.pinv((x_mat.transpose() * x_mat))
    t_weights = ((mat_inv * x_mat.transpose()) * response_y)
    t_predict_y = (x_mat * t_weights)
    delta = (t_predict_y - response_y)
    mse_train = (delta.transpose() * delta)
    mysd = (mse_train / dof)
    se = np.sqrt((mysd * np.diag(mat_inv)))
    tval = abs((t_weights.transpose() / se))
    p_values = (scipy.stats.t.sf(tval, dof) * 2)
    test_response_y = test_data_xy[:, (num_col - 1)]
    test_data = test_data_xy[:, range(0, (num_col - 1))]
    t_predict = pyunit_utils.generate_response_glm(t_weights, test_data, 0, self.family)
    (num_row_t, num_col_t) = test_data.shape
    temp = (t_predict - test_response_y)
    mse_test = ((temp.transpose() * temp) / num_row_t)
    return (np.array(t_weights.transpose())[0].tolist(), np.array(p_values)[0].tolist(), (mse_train[(0, 0)] / num_row), mse_test[(0, 0)])
