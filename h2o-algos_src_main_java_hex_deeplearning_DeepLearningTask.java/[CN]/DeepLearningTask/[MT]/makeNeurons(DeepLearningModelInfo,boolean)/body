{
  DataInfo dinfo=minfo.data_info();
  final DeepLearningParameters params=minfo.get_params();
  final int[] h=params._hidden;
  Neurons[] neurons=new Neurons[h.length + 2];
  neurons[0]=new Neurons.Input(minfo.units[0],dinfo);
  for (int i=0; i < h.length + (params._autoencoder ? 1 : 0); i++) {
    int n=params._autoencoder && i == h.length ? minfo.units[0] : h[i];
switch (params._activation) {
case Tanh:
      neurons[i + 1]=new Neurons.Tanh(n);
    break;
case TanhWithDropout:
  neurons[i + 1]=params._autoencoder && i == h.length ? new Neurons.Tanh(n) : new Neurons.TanhDropout(n);
break;
case Rectifier:
neurons[i + 1]=new Neurons.Rectifier(n);
break;
case RectifierWithDropout:
neurons[i + 1]=params._autoencoder && i == h.length ? new Neurons.Rectifier(n) : new Neurons.RectifierDropout(n);
break;
case Maxout:
neurons[i + 1]=new Neurons.Maxout(n);
break;
case MaxoutWithDropout:
neurons[i + 1]=params._autoencoder && i == h.length ? new Neurons.Maxout(n) : new Neurons.MaxoutDropout(n);
break;
}
}
if (!params._autoencoder) {
if (minfo._classification) neurons[neurons.length - 1]=new Neurons.Softmax(minfo.units[minfo.units.length - 1]);
 else neurons[neurons.length - 1]=new Neurons.Linear();
}
for (int i=0; i < neurons.length; i++) {
neurons[i].init(neurons,i,params,minfo,training);
neurons[i]._input=neurons[0];
}
return neurons;
}
