{
  Neurons[] neurons=DeepLearningTask.makeNeuronsForTraining(model_info());
  long seed=0;
  ((Neurons.Input)neurons[0]).setInput(seed,myRow.numVals,myRow.nBins,myRow.binIds);
  DeepLearningTask.step(seed,neurons,model_info(),null,false,new double[]{myRow.response[0]},0);
  double loss;
  if (_parms._loss == DeepLearningParameters.Loss.CrossEntropy) {
    if (_parms._balance_classes)     throw H2O.unimpl();
    int actual=(int)myRow.response[0];
    double pred=neurons[neurons.length - 1]._a.get(actual);
    loss=-Math.log(Math.max(1e-15,pred));
  }
 else {
    if (_parms._autoencoder)     throw H2O.unimpl();
    double pred=neurons[neurons.length - 1]._a.raw()[0];
    double actual=myRow.response[0];
    DataInfo di=model_info().data_info();
    if (di._normRespMul != null) {
      pred=(pred / di._normRespMul[0] + di._normRespSub[0]);
      actual=(actual / di._normRespMul[0] + di._normRespSub[0]);
    }
    Distribution dist=new Distribution(model_info.get_params()._distribution,model_info.get_params()._tweedie_power);
    loss=dist.deviance(1,actual,dist.linkInv(pred));
  }
  for (int i=0; i < _parms._hidden.length + 1; ++i) {
    if (neurons[i]._w == null)     continue;
    for (int row=0; row < neurons[i]._w.rows(); ++row) {
      for (int col=0; col < neurons[i]._w.cols(); ++col) {
        loss+=_parms._l1 * Math.abs(neurons[i]._w.get(row,col));
        loss+=0.5 * _parms._l2 * Math.pow(neurons[i]._w.get(row,col),2);
      }
    }
    for (int row=0; row < neurons[i]._w.rows(); ++row) {
      loss+=_parms._l1 * Math.abs(neurons[i]._b.get(row));
      loss+=0.5 * _parms._l2 * Math.pow(neurons[i]._b.get(row),2);
    }
  }
  return loss;
}
