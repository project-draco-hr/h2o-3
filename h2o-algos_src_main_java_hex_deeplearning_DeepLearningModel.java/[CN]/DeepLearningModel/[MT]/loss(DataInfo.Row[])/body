{
  double loss=0;
  Neurons[] neurons=DeepLearningTask.makeNeuronsForTraining(model_info());
  final double prefactor=_parms._distribution == Distribution.Family.laplace ? 1 : 0.5;
  for (  DataInfo.Row myRow : myRows) {
    if (myRow == null)     continue;
    long seed=-1;
    for (int i=0; i < neurons.length - 1; ++i) {
      Storage.DenseVector e=neurons[i]._e;
      if (e == null)       continue;
      assert(ArrayUtils.sum(e.raw()) == 0);
    }
    ((Neurons.Input)neurons[0]).setInput(seed,myRow.numIds,myRow.numVals,myRow.nBins,myRow.binIds);
    DeepLearningTask.step(seed,neurons,model_info(),null,false,null,myRow.offset);
    for (int i=0; i < neurons.length - 1; ++i) {
      Storage.DenseVector e=neurons[i]._e;
      if (e == null)       continue;
      assert(ArrayUtils.sum(e.raw()) == 0);
    }
    if (get_params()._loss == DeepLearningParameters.Loss.CrossEntropy) {
      if (_parms._balance_classes)       throw H2O.unimpl();
      int actual=(int)myRow.response[0];
      double pred=neurons[neurons.length - 1]._a.get(actual);
      loss+=-Math.log(Math.max(1e-15,pred));
    }
 else {
      if (model_info.get_params()._autoencoder)       throw H2O.unimpl();
      double pred=neurons[neurons.length - 1]._a.get(0);
      double actual=myRow.response[0];
      Distribution dist=new Distribution(model_info.get_params());
      pred=dist.linkInv(pred);
      loss+=prefactor * dist.deviance(1,actual,pred);
    }
    for (int i=0; i < _parms._hidden.length + 1; ++i) {
      if (neurons[i]._w == null)       continue;
      for (int row=0; row < neurons[i]._w.rows(); ++row) {
        for (int col=0; col < neurons[i]._w.cols(); ++col) {
          loss+=_parms._l1 * Math.abs(neurons[i]._w.get(row,col));
          loss+=0.5 * _parms._l2 * Math.pow(neurons[i]._w.get(row,col),2);
        }
      }
      for (int row=0; row < neurons[i]._b.size(); ++row) {
        loss+=_parms._l1 * Math.abs(neurons[i]._b.get(row));
        loss+=0.5 * _parms._l2 * Math.pow(neurons[i]._b.get(row),2);
      }
    }
  }
  return loss;
}
