def setup(self):
    '\n        This function performs all initializations necessary:\n        1. generates all the random values for our dynamic tests like the Binomial\n        noise std, column count and row count for training data set;\n        2. generate the training/validation/test data sets with only real values;\n        3. insert missing values into training/valid/test data sets.\n        4. taken the training/valid/test data sets, duplicate random certain columns,\n            each duplicated column is repeated for a random number of times and randomly scaled;\n        5. generate the training/validation/test data sets with predictors containing enum\n            and real values as well***.\n        6. insert missing values into the training/validation/test data sets with predictors\n            containing enum and real values as well\n\n        *** according to Tomas, when working with mixed predictors (contains both enum/real\n        value columns), the encoding used is different when regularization is enabled or disabled.\n        When regularization is enabled, true one hot encoding is enabled to encode the enum\n        values to binary bits.  When regularization is disabled, a reference level plus one hot encoding\n        is enabled when encoding the enum values to binary bits.  One data set is generated\n        when we work with mixed predictors.\n        '
    self.sandbox_dir = pyunit_utils.make_Rsandbox_dir(self.current_dir, self.test_name, True)
    self.noise_std = random.uniform(0, math.sqrt((pow((self.max_p_value - self.min_p_value), 2) / 12)))
    self.noise_var = (self.noise_std * self.noise_std)
    self.train_col_count = random.randint(3, self.max_col_count)
    self.train_row_count = round((self.train_col_count * random.uniform(self.min_col_count_ratio, self.max_col_count_ratio)))
    self.enum_col = random.randint(1, (self.train_col_count - 1))
    self.enum_level_vec = np.random.random_integers(2, (self.enum_levels - 1), [self.enum_col, 1])
    pyunit_utils.write_syn_floating_point_dataset_glm(self.training_data_file, self.validation_data_file, self.test_data_file, self.weight_data_file, self.train_row_count, self.train_col_count, self.data_type, self.max_p_value, self.min_p_value, self.max_w_value, self.min_w_value, self.noise_std, self.family, self.train_row_count, self.train_row_count, class_number=self.class_number, class_method=[self.class_method, self.class_method, self.test_class_method], class_margin=[self.margin, self.margin, self.test_class_margin])
    (self.duplicate_col_indices, self.duplicate_col_scales) = pyunit_utils.random_col_duplication(self.train_col_count, self.duplicate_threshold, self.duplicate_col_counts, True, self.duplicate_max_scale)
    dup_col_indices = self.duplicate_col_indices
    dup_col_indices.append(self.train_col_count)
    dup_col_scale = self.duplicate_col_scales
    dup_col_scale.append(1.0)
    print('duplication column and duplication scales are: ')
    print(dup_col_indices)
    print(dup_col_scale)
    pyunit_utils.duplicate_scale_cols(dup_col_indices, dup_col_scale, self.training_data_file, self.training_data_file_duplicate)
    pyunit_utils.duplicate_scale_cols(dup_col_indices, dup_col_scale, self.test_data_file, self.test_data_file_duplicate)
    pyunit_utils.insert_nan_in_data(self.training_data_file, self.training_data_file_nans, self.nan_fraction)
    pyunit_utils.insert_nan_in_data(self.test_data_file, self.test_data_file_nans, self.nan_fraction)
    pyunit_utils.write_syn_mixed_dataset_glm(self.training_data_file_enum, self.training_data_file_enum_true_one_hot, self.validation_data_file_enum, self.validation_data_file_enum_true_one_hot, self.test_data_file_enum, self.test_data_file_enum_true_one_hot, self.weight_data_file_enum, self.train_row_count, self.train_col_count, self.max_p_value, self.min_p_value, self.max_w_value, self.min_w_value, self.noise_std, self.family, self.train_row_count, self.train_row_count, self.enum_col, self.enum_level_vec, class_number=self.class_number, class_method=[self.class_method, self.class_method, self.test_class_method], class_margin=[self.margin, self.margin, self.test_class_margin])
    pyunit_utils.insert_nan_in_data(self.training_data_file_enum, self.training_data_file_enum_nans, self.nan_fraction)
    pyunit_utils.insert_nan_in_data(self.validation_data_file_enum, self.validation_data_file_enum_nans, self.nan_fraction)
    pyunit_utils.insert_nan_in_data(self.test_data_file_enum, self.test_data_file_enum_nans, self.nan_fraction)
    pyunit_utils.insert_nan_in_data(self.training_data_file_enum_true_one_hot, self.training_data_file_enum_nans_true_one_hot, self.nan_fraction)
    pyunit_utils.insert_nan_in_data(self.validation_data_file_enum_true_one_hot, self.validation_data_file_enum_nans_true_one_hot, self.nan_fraction)
    pyunit_utils.insert_nan_in_data(self.test_data_file_enum_true_one_hot, self.test_data_file_enum_nans_true_one_hot, self.nan_fraction)
    self.training_data = h2o.import_file(pyunit_utils.locate(self.training_data_file))
    self.y_index = (self.training_data.ncol - 1)
    self.x_indices = list(range(self.y_index))
    self.training_data[self.y_index] = self.training_data[self.y_index].round().asfactor()
    if (self.training_data[self.y_index].nlevels()[0] < self.class_number):
        print('Response classes are not represented in training dataset.')
        sys.exit(0)
    self.valid_data = h2o.import_file(pyunit_utils.locate(self.validation_data_file))
    self.valid_data[self.y_index] = self.valid_data[self.y_index].round().asfactor()
    self.test_data = h2o.import_file(pyunit_utils.locate(self.test_data_file))
    self.test_data[self.y_index] = self.test_data[self.y_index].round().asfactor()
    self.training_data_grid = self.training_data.rbind(self.valid_data)
    for ind in range(self.class_number):
        self.sklearn_class_weight[ind] = 1.0
    pyunit_utils.remove_csv_files(self.current_dir, '.csv', action='copy', new_dir_path=self.sandbox_dir)
