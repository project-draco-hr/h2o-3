{
  final DTree[] ktrees=new DTree[_nclass];
  int[] leaves=new int[_nclass];
  new ComputePredAndRes().doAll(_train,_parms._build_tree_one_node);
  for (int k=0; k < _nclass; k++) {
    if (DEV_DEBUG && ktrees[k] != null) {
      System.out.println("Updated predictions in WORK col for class " + k + ":\n"+ new Frame(new String[]{"WORK"},new Vec[]{vec_work(_train,k)}).toString());
    }
  }
  growTrees(ktrees,leaves,_rand);
  for (int k=0; k < _nclass; k++) {
    if (DEV_DEBUG && ktrees[k] != null) {
      System.out.println("Grew trees. Updated NIDs for class " + k + ":\n"+ new Frame(new String[]{"NIDS"},new Vec[]{vec_nids(_train,k)}).toString());
    }
  }
  GammaPass gp=new GammaPass(ktrees,leaves,_parms._distribution).doAll(_train);
  if (_parms._distribution == Distribution.Family.laplace) {
    fitBestConstantsQuantile(ktrees,leaves[0],0.5);
  }
 else   if (_parms._distribution == Distribution.Family.quantile) {
    fitBestConstantsQuantile(ktrees,leaves[0],_parms._quantile_alpha);
  }
 else {
    fitBestConstants(ktrees,leaves,gp);
  }
  if (_parms._distribution == Distribution.Family.gamma || _parms._distribution == Distribution.Family.poisson || _parms._distribution == Distribution.Family.tweedie) {
    assert(_nclass == 1);
    truncatePreds(ktrees[0],leaves[0],_parms._distribution);
  }
  new AddTreeContributions(ktrees).doAll(_train);
  for (int k=0; k < _nclass; k++) {
    if (ktrees[k] != null)     assert(vec_nids(_train,k).mean() == 0);
  }
  _model._output.addKTrees(ktrees);
  boolean converged=effective_learning_rate() < 1e-6;
  if (converged) {
    Log.warn("Effective learning rate dropped below 1e-6 (" + _parms._learn_rate + " * "+ _parms._learn_rate_annealing+ "^"+ (_model._output._ntrees - 1)+ ") - stopping the model now.");
  }
  return converged;
}
