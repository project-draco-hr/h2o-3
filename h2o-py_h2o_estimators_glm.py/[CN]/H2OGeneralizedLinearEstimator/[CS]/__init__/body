def __init__(self, model_id=None, max_iterations=None, beta_epsilon=None, solver=None, standardize=None, family=None, link=None, tweedie_variance_power=None, tweedie_link_power=None, alpha=None, prior=None, lambda_search=None, nlambdas=None, lambda_min_ratio=None, beta_constraints=None, nfolds=None, fold_assignment=None, keep_cross_validation_predictions=None, intercept=None, Lambda=None, max_active_predictors=None, checkpoint=None, objective_epsilon=None, gradient_epsilon=None, non_negative=False, compute_p_values=False, remove_colinear_columns=False):
    '\n    Build a Generalized Linear Model\n    Fit a generalized linear model, specified by a response variable, a set of predictors,\n    and a description of the error distribution.\n\n    Parameters\n    ----------\n    model_id : str, optional\n      The unique id assigned to the resulting model. If none is given, an id will\n      automatically be generated.\n    max_iterations : int\n      A non-negative integer specifying the maximum number of iterations.\n    beta_epsilon : int\n      A non-negative number specifying the magnitude of the maximum difference between\n      the coefficient estimates from successive iterations. Defines the convergence\n      criterion.\n    solver : str\n      A character string specifying the solver used: IRLSM (supports more features),\n      L_BFGS (scales better for datasets with many columns)\n    standardize : bool\n      Indicates whether the numeric predictors should be standardized to have a mean of\n      0 and a variance of 1 prior to training the models.\n    family : str\n      A character string specifying the distribution of the model:\n     gaussian, binomial, poisson, gamma, tweedie.\n    link : str\n      A character string specifying the link function.\n      The default is the canonical link for the family.\n      The supported links for each of the family specifications are:\n          "gaussian": "identity", "log", "inverse"\n          "binomial": "logit", "log"\n          "poisson": "log", "identity"\n          "gamma": "inverse", "log", "identity"\n          "tweedie": "tweedie"\n\n    tweedie_variance_power : int\n      numeric specifying the power for the variance function when family = "tweedie".\n    tweedie_link_power : int\n      A numeric specifying the power for the link function when family = "tweedie".\n    alpha : float\n      A numeric in [0, 1] specifying the elastic-net mixing parameter.\n\n      The elastic-net penalty is defined to be:\n      eqn{P(\x07lpha,\x08eta) = (1-\x07lpha)/2||\x08eta||_2^2 +\n      \x07lpha||\x08eta||_1 = \\sum_j [(1-\x07lpha)/2 \x08eta_j^2 + \x07lpha|\x08eta_j|],\n      making alpha = 1 the lasso penalty and alpha = 0 the ridge penalty.\n\n    Lambda : float\n      A non-negative shrinkage parameter for the elastic-net, which multiplies\n      \\eqn{P(\x07lpha,\x08eta) in the objective function.\n      When Lambda = 0, no elastic-net penalty is applied and ordinary generalized linear\n      models are fit.\n    prior : float, optional\n      A numeric specifying the prior probability of class 1 in the response when\n      family = "binomial". The default prior is the observational frequency of class 1.\n    lambda_search : bool\n      A logical value indicating whether to conduct a search over the space of lambda\n      values starting from the lambda max, given lambda is interpreted as lambda minself.\n    nlambdas : int\n      The number of lambda values to use when lambda_search = TRUE.\n    lambda_min_ratio : float\n      Smallest value for lambda as a fraction of lambda.max. By default if the number of\n      observations is greater than the the number of variables then\n      lambda_min_ratio = 0.0001; if the number of observations is less than the number\n      of variables then lambda_min_ratio = 0.01.\n    beta_constraints : H2OFrame\n      A data.frame or H2OParsedData object with the columns\n      ["names", "lower_bounds", "upper_bounds", "beta_given"],\n      where each row corresponds to a predictor in the GLM.\n      "names" contains the predictor names, "lower"/"upper_bounds",\n      are the lower and upper bounds of beta, and "beta_given" is some supplied starting\n      values.\n    nfolds : int, optional\n      Number of folds for cross-validation. If nfolds >= 2, then validation must\n      remain empty.\n    fold_assignment : str\n      Cross-validation fold assignment scheme, if fold_column is not specified Must be\n      "AUTO", "Random" or "Modulo"\n    keep_cross_validation_predictions : bool\n      Whether to keep the predictions of the cross-validation models\n    intercept : bool\n      Logical, include constant term (intercept) in the model\n    max_active_predictors : int, optional\n      Convergence criteria for number of predictors when using L1 penalty.\n\n    Returns\n    -------\n      A subclass of ModelBase is returned. The specific subclass depends on the machine\n      learning task at hand (if it\'s binomial classification, then an H2OBinomialModel\n      is returned, if it\'s regression then a H2ORegressionModel is returned). The default\n      print-out of the models is shown, but further GLM-specifc information can be\n      queried out of the object. Upon completion of the GLM, the resulting object has\n      coefficients, normalized coefficients, residual/null deviance, aic, and a host of\n      model metrics including MSE, AUC (for logistic regression), degrees of freedom, and\n      confusion matrices.\n    '
    super(H2OGeneralizedLinearEstimator, self).__init__()
    self._parms = locals()
    self._parms = {k: v for (k, v) in self._parms.items() if (k != 'self')}
    self._parms['lambda'] = self._parms.pop('Lambda')
