def test_naivebayes_grid_search_over_params(self):
    "\n        test_naivebayes_grid_search_over_params the following:\n        a. grab all truely griddable parameters and randomly or manually set the parameter values.\n        b. Next, build H2O naivebayes models using grid search.  Count and make sure models\n           are only built for hyper-parameters set to legal values.  No model is built for bad hyper-parameters\n           values.  We should instead get a warning/error message printed out.\n        c. For each model built using grid search, we will extract the parameters used in building\n           that model and manually build a H2O naivebayes model.  Logloss are calculated from a test set\n           to compare the performance of grid search model and our manually built model.  If their metrics\n           differ by too much, print a warning message but don't fail the test.\n        d. we will check and make sure the models are built within the max_runtime_secs time limit that was set\n           for it as well.  If max_runtime_secs was exceeded, declare test failure as well.\n        "
    print('*******************************************************************************************')
    print('test_naivebayes_grid_search_over_params for naivebayes ')
    h2o.cluster_info()
    try:
        print('Hyper-parameters used here is {0}'.format(self.final_hyper_params))
        grid_model = H2OGridSearch(H2ONaiveBayesEstimator(nfolds=self.nfolds), hyper_params=self.final_hyper_params)
        grid_model.train(x=self.x_indices, y=self.y_index, training_frame=self.training1_data)
        self.correct_model_number = len(grid_model)
        if (not (self.correct_model_number == self.possible_number_models)):
            self.test_failed += 1
            print('test_naivebayes_grid_search_over_params for naivebayes failed: number of models built by gridsearch {0} does not equal to all possible combinations of hyper-parameters {1}'.format(self.correct_model_number, self.possible_number_models))
        else:
            params_dict = dict()
            params_dict['nfolds'] = self.nfolds
            total_run_time_limits = 0.0
            true_run_time_limits = 0.0
            manual_run_runtime = 0.0
            gridsearch_runtime = 0.0
            for each_model in grid_model:
                params_list = grid_model.get_hyperparams_dict(each_model._id)
                params_list.update(params_dict)
                model_params = dict()
                if ('max_runtime_secs' in params_list):
                    model_params['max_runtime_secs'] = params_list['max_runtime_secs']
                    max_runtime = params_list['max_runtime_secs']
                    del params_list['max_runtime_secs']
                else:
                    max_runtime = 0
                if ('validation_frame' in params_list):
                    model_params['validation_frame'] = params_list['validation_frame']
                    del params_list['validation_frame']
                if ('eps_prob' in params_list):
                    model_params['eps_prob'] = params_list['eps_prob']
                    del params_list['eps_prob']
                if ('min_prob' in params_list):
                    model_params['min_prob'] = params_list['min_prob']
                    del params_list['min_prob']
                each_model_runtime = pyunit_utils.find_grid_runtime([each_model])
                gridsearch_runtime += each_model_runtime
                manual_model = H2ONaiveBayesEstimator(**params_list)
                manual_model.train(x=self.x_indices, y=self.y_index, training_frame=self.training1_data, **model_params)
                model_runtime = pyunit_utils.find_grid_runtime([manual_model])
                manual_run_runtime += model_runtime
                if (max_runtime > 0):
                    if (max_runtime < self.model_run_time):
                        total_run_time_limits += model_runtime
                    else:
                        total_run_time_limits += max_runtime
                true_run_time_limits += max_runtime
                test_grid_model_metrics = each_model.model_performance(test_data=self.training1_data)._metric_json[self.training_metric]
                test_manual_model_metrics = manual_model.model_performance(test_data=self.training1_data)._metric_json[self.training_metric]
                if (abs((test_grid_model_metrics - test_manual_model_metrics)) > self.allowed_diff):
                    print('test_naivebayes_grid_search_over_params for naivebayes WARNING\ngrid search model {0}: {1}, time taken to build (secs): {2}\n and manually built H2O model {3}: {4}, time taken to build (secs): {5}\ndiffer too much!'.format(self.training_metric, test_grid_model_metrics, each_model_runtime, self.training_metric, test_manual_model_metrics, model_runtime))
            print('Time taken for gridsearch to build all models (sec): {0}\n Time taken to manually build all models (sec): {1}, total run time limits (sec): {2}'.format(gridsearch_runtime, manual_run_runtime, total_run_time_limits))
            total_run_time_limits = (max(total_run_time_limits, true_run_time_limits) * (1 + self.extra_time_fraction))
            if (not (manual_run_runtime <= total_run_time_limits)):
                self.test_failed += 1
                print('test_naivebayes_grid_search_over_params for naivebayes failed: time taken to manually build models is {0}.  Maximum allowed time is {1}'.format(manual_run_runtime, total_run_time_limits))
            if (self.test_failed == 0):
                print('test_naivebayes_grid_search_over_params for naivebayes has passed!')
    except:
        if (self.possible_number_models > 0):
            print('test_naivebayes_grid_search_over_params for naivebayes failed: exception was thrown for no reason.')
            self.test_failed += 1
