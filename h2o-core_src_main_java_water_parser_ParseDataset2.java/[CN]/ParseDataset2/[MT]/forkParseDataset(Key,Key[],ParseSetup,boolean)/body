{
  HashSet<String> conflictingNames=setup.checkDupColumnNames();
  for (  String x : conflictingNames)   throw new IllegalArgumentException("Found duplicate column name " + x);
  long sum=0;
  for (int i=0; i < keys.length; i++) {
    Key k=keys[i];
    if (dest.equals(k))     throw new IllegalArgumentException("Destination key " + dest + " must be different from all sources");
    if (delete_on_done)     for (int j=i + 1; j < keys.length; j++)     if (k == keys[j])     throw new IllegalArgumentException("Source key " + k + " appears twice, delete_on_done must be false");
    sum+=getByteVec(k).length();
  }
  long memsz=H2O.CLOUD.memsz();
  if (sum > memsz * 4)   throw new IllegalArgumentException("Total input file size of " + PrettyPrint.bytes(sum) + " is much larger than total cluster memory of "+ PrettyPrint.bytes(memsz)+ ", please use either a larger cluster or smaller data.");
  ParseDataset2 job=new ParseDataset2(dest);
  new Frame(job.dest(),new String[0],new Vec[0]).delete_and_lock(job._key);
  for (  Key k : keys)   Lockable.read_lock(k,job._key);
  ParserFJTask fjt=new ParserFJTask(job,keys,setup,delete_on_done);
  job.start(fjt,sum);
  return job;
}
