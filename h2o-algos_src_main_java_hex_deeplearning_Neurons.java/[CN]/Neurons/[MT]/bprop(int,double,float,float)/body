{
  if (_shortcut && partial_grad == 0f)   return;
  final float rho=(float)params._rho;
  final float eps=(float)params._epsilon;
  final float l1=(float)params._l1;
  final float l2=(float)params._l2;
  final float max_w2=params._max_w2;
  final boolean have_momenta=_minfo.has_momenta();
  final boolean have_ada=_minfo.adaDelta();
  final boolean nesterov=params._nesterov_accelerated_gradient;
  final boolean update_prev=_previous._e != null;
  final boolean fast_mode=params._fast_mode;
  final int cols=_previous._a.size();
  double avg_grad2=0;
  final int idx=row * cols;
  for (int col=0; col < cols; col++) {
    int w=idx + col;
    if (_k != 0)     w=_k * w + _maxIncoming[row];
    final double weight=_w.raw()[w];
    if (update_prev)     _previous._e.add(col,partial_grad * weight);
    final double previous_a=_previous._a.get(col);
    if (fast_mode && previous_a == 0)     continue;
    double grad=partial_grad * previous_a - Math.signum(weight) * l1 - weight * l2;
    if (_wEA != null) {
      grad-=params._elastic_averaging_regularization * (_w.raw()[w] - _wEA.raw()[w]);
    }
    if (DeepLearningModelInfo.gradientCheck != null)     DeepLearningModelInfo.gradientCheck.apply(_index,row,col,-grad);
    if (have_ada) {
      final double grad2=grad * grad;
      avg_grad2+=grad2;
      float brate=computeAdaDeltaRateForWeight(grad,w,_ada_dx_g,rho,eps);
      _w.raw()[w]+=brate * grad;
    }
 else {
      if (!nesterov) {
        final double delta=rate * grad;
        _w.raw()[w]+=delta;
        if (have_momenta) {
          _w.raw()[w]+=momentum * _wm.raw()[w];
          _wm.raw()[w]=(float)delta;
        }
      }
 else {
        double tmp=grad;
        if (have_momenta) {
          _wm.raw()[w]*=momentum;
          _wm.raw()[w]+=tmp;
          tmp=_wm.raw()[w];
        }
        _w.raw()[w]+=rate * tmp;
      }
    }
  }
  if (max_w2 != Float.POSITIVE_INFINITY)   rescale_weights(_w,row,max_w2);
  if (have_ada)   avg_grad2/=cols;
  update_bias(_b,_bEA,_bm,row,partial_grad,avg_grad2,rate,momentum);
}
