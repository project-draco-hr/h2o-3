{
  final float rho=(float)params._rho;
  final float eps=(float)params._epsilon;
  final float l1=(float)params._l1;
  final float l2=(float)params._l2;
  final float max_w2=params._max_w2;
  final boolean have_momenta=_minfo.has_momenta();
  final boolean have_ada=_minfo.adaDelta();
  final boolean nesterov=params._nesterov_accelerated_gradient;
  final boolean fast_mode=params._fast_mode;
  final int cols=_previous._a[0].size();
  assert(partial_grad.length == n);
  double avg_grad2=0;
  final int idx=row * cols;
  for (int mb=0; mb < n; mb++) {
    if (_shortcut && partial_grad[mb] == 0f)     return;
    final boolean update_prev=_previous._e != null && _previous._e[mb] != null;
    for (int col=0; col < cols; col++) {
      int w=idx + col;
      if (_k != 0)       w=_k * w + _maxIncoming[mb][row];
      final double weight=_w.raw()[w];
      if (update_prev)       _previous._e[mb].add(col,partial_grad[mb] * weight);
      final double previous_a=_previous._a[mb].get(col);
      if (fast_mode && previous_a == 0)       continue;
      double grad=partial_grad[mb] * previous_a + Math.signum(weight) * l1 + weight * l2;
      if (_wEA != null) {
        grad+=params._elastic_averaging_regularization * (_w.raw()[w] - _wEA.raw()[w]);
      }
      if (DeepLearningModelInfo.gradientCheck != null)       DeepLearningModelInfo.gradientCheck.apply(_index,row,col,grad);
      if (have_ada) {
        final double grad2=grad * grad;
        avg_grad2+=grad2;
        float brate=computeAdaDeltaRateForWeight(grad,w,_ada_dx_g,rho,eps);
        _w.raw()[w]-=brate * grad;
      }
 else {
        if (!nesterov) {
          final double delta=-rate * grad;
          _w.raw()[w]+=delta;
          if (have_momenta) {
            _w.raw()[w]+=momentum * _wm.raw()[w];
            _wm.raw()[w]=(float)delta;
          }
        }
 else {
          double tmp=-grad;
          if (have_momenta) {
            _wm.raw()[w]*=momentum;
            _wm.raw()[w]+=tmp;
            tmp=_wm.raw()[w];
          }
          _w.raw()[w]+=rate * tmp;
        }
      }
    }
  }
  if (max_w2 != Float.POSITIVE_INFINITY)   for (int mb=0; mb < n; mb++)   rescale_weights(_w,row,max_w2,mb);
  if (have_ada)   avg_grad2/=cols * n;
  for (int mb=0; mb < n; mb++) {
    update_bias(_b,_bEA,_bm,row,partial_grad,avg_grad2,rate,momentum,mb);
  }
}
