{
  if (((Job)DKV.getGet(_jobKey)).isCancelledOrCrashed())   return;
  AppendableVec[] avs=new AppendableVec[_setup._number_columns];
  for (int i=0; i < avs.length; ++i)   avs[i]=new AppendableVec(_vg.vecKey(_vecIdStart + i),_espc,_startChunkIdx);
  FVecParseReader din=new FVecParseReader(in);
  FVecParseWriter dout;
  Parser p;
switch (_setup._parse_type) {
case ARFF:
case CSV:
    Categorical[] enums=enums(_eKey,_setup._number_columns);
  p=new CsvParser(_setup,_jobKey);
dout=new FVecParseWriter(_vg,_startChunkIdx + in.cidx(),enums,_setup._column_types,_setup._chunk_size,avs);
break;
case SVMLight:
p=new SVMLightParser(_setup,_jobKey);
dout=new SVMLightFVecParseWriter(_vg,_vecIdStart,in.cidx() + _startChunkIdx,_setup._chunk_size,avs);
break;
default :
throw H2O.unimpl();
}
p.parseChunk(in.cidx(),din,dout);
(_dout=dout).close(_fs);
Job.update(in._len,_jobKey);
freeMem(in);
}
