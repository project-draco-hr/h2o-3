{
  if (_jobKey.get().stop_requested())   return;
  ParseSetup localSetup=_parseSetup._parse_type == ParserType.AVRO ? new AvroParser.AvroParseSetup(_parseSetup,((AvroParser.AvroParseSetup)_parseSetup).header) : new ParseSetup(_parseSetup);
  ByteVec vec=getByteVec(key);
  final int chunkStartIdx=_fileChunkOffsets[_lo];
  Log.trace("Begin a map stage of a file parse with start index " + chunkStartIdx + ".");
  byte[] zips=vec.getFirstBytes();
  ZipUtil.Compression cpr=ZipUtil.guessCompressionMethod(zips);
  if (localSetup._check_header == ParseSetup.HAS_HEADER)   localSetup._check_header=localSetup.parser(_jobKey).fileHasHeader(ZipUtil.unzipBytes(zips,cpr,localSetup._chunk_size),localSetup);
  try {
switch (cpr) {
case NONE:
      if (_parseSetup._parse_type._parallelParseSupported) {
        new DistributedParse(_vg,localSetup,_vecIdStart,chunkStartIdx,this,key,vec.nChunks()).doAll(vec);
        for (int i=0; i < vec.nChunks(); ++i)         _chunk2ParseNodeMap[chunkStartIdx + i]=vec.chunkKey(i).home_node().index();
      }
 else {
        InputStream bvs=vec.openStream(_jobKey);
        _dout[_lo]=streamParse(bvs,localSetup,makeDout(localSetup,chunkStartIdx,vec.nChunks()),bvs);
        _errors=_dout[_lo].removeErrors();
        chunksAreLocal(vec,chunkStartIdx,key);
      }
    break;
case ZIP:
{
    InputStream bvs=vec.openStream(_jobKey);
    ZipInputStream zis=new ZipInputStream(bvs);
    ZipEntry ze=zis.getNextEntry();
    if (ze != null && !ze.isDirectory())     _dout[_lo]=streamParse(zis,localSetup,makeDout(localSetup,chunkStartIdx,vec.nChunks()),bvs);
    _errors=_dout[_lo].removeErrors();
    ZipEntry ze2=zis.getNextEntry();
    if (ze2 != null && !ze.isDirectory()) {
      Log.warn("Only single file zip archives are currently supported, only file: " + ze.getName() + " has been parsed.  Remaining files have been ignored.");
    }
 else     zis.close();
    chunksAreLocal(vec,chunkStartIdx,key);
    break;
  }
case GZIP:
{
  InputStream bvs=vec.openStream(_jobKey);
  _dout[_lo]=streamParse(new GZIPInputStream(bvs),localSetup,makeDout(localSetup,chunkStartIdx,vec.nChunks()),bvs);
  _errors=_dout[_lo].removeErrors();
  chunksAreLocal(vec,chunkStartIdx,key);
  break;
}
}
Log.trace("Finished a map stage of a file parse with start index " + chunkStartIdx + ".");
}
 catch (IOException ioe) {
throw new RuntimeException(ioe);
}
catch (H2OParseException pe0) {
throw pe0.resetMsg(pe0.getMessage() + " for " + key);
}
}
