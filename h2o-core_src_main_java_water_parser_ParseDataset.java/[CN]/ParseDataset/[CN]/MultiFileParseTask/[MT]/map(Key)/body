{
  ParseSetup localSetup=new ParseSetup(_parseSetup);
  ByteVec vec=getByteVec(key);
  final int chunkStartIdx=_fileChunkOffsets[_lo];
  byte[] zips=vec.getFirstBytes();
  ZipUtil.Compression cpr=ZipUtil.guessCompressionMethod(zips);
  if (localSetup._check_header == ParseSetup.HAS_HEADER)   localSetup._check_header=localSetup.parser().fileHasHeader(ZipUtil.unzipBytes(zips,cpr,localSetup._chunk_size),localSetup);
  try {
switch (cpr) {
case NONE:
      if (_parseSetup._parse_type._parallelParseSupported) {
        DistributedParse dp=new DistributedParse(_vg,localSetup,_vecIdStart,chunkStartIdx,this,key,vec.nChunks());
        addToPendingCount(1);
        dp.setCompleter(this);
        dp.asyncExec(vec);
        for (int i=0; i < vec.nChunks(); ++i)         _chunk2Enum[chunkStartIdx + i]=vec.chunkKey(i).home_node().index();
      }
 else {
        InputStream bvs=vec.openStream(_job_key);
        _dout[_lo]=streamParse(bvs,localSetup,makeDout(localSetup,chunkStartIdx,vec.nChunks()),bvs);
        chunksAreLocal(vec,chunkStartIdx,key);
      }
    break;
case ZIP:
{
    InputStream bvs=vec.openStream(_job_key);
    ZipInputStream zis=new ZipInputStream(bvs);
    ZipEntry ze=zis.getNextEntry();
    if (ze != null && !ze.isDirectory())     _dout[_lo]=streamParse(zis,localSetup,makeDout(localSetup,chunkStartIdx,vec.nChunks()),bvs);
    ZipEntry ze2=zis.getNextEntry();
    if (ze2 != null && !ze.isDirectory()) {
      Log.warn("Only single file zip archives are currently supported, only file: " + ze.getName() + " has been parsed.  Remaining files have been ignored.");
    }
 else     zis.close();
    chunksAreLocal(vec,chunkStartIdx,key);
    break;
  }
case GZIP:
{
  InputStream bvs=vec.openStream(_job_key);
  _dout[_lo]=streamParse(new GZIPInputStream(bvs),localSetup,makeDout(localSetup,chunkStartIdx,vec.nChunks()),bvs);
  chunksAreLocal(vec,chunkStartIdx,key);
  break;
}
}
}
 catch (IOException ioe) {
throw new RuntimeException(ioe);
}
catch (H2OParseException pe) {
throw new H2OParseException(key,pe);
}
}
