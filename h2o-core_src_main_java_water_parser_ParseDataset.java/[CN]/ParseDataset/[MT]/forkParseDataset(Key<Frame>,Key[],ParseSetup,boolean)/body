{
  final ParseSetup setup=parseSetup.getFinalSetup(keys,parseSetup);
  HashSet<String> conflictingNames=setup.checkDupColumnNames();
  for (  String x : conflictingNames)   if (x != null && !x.equals(""))   throw new IllegalArgumentException("Found duplicate column name " + x);
  long totalParseSize=0;
  for (int i=0; i < keys.length; i++) {
    Key k=keys[i];
    if (dest.equals(k))     throw new IllegalArgumentException("Destination key " + dest + " must be different from all sources");
    if (deleteOnDone)     for (int j=i + 1; j < keys.length; j++)     if (k == keys[j])     throw new IllegalArgumentException("Source key " + k + " appears twice, deleteOnDone must be false");
    totalParseSize+=getByteVec(k).length();
  }
  Log.info("Total file size: " + PrettyPrint.bytes(totalParseSize));
  for (int i=0; i < keys.length; ++i) {
    Iced ice=DKV.getGet(keys[i]);
    if (ice instanceof FileVec) {
      ((FileVec)ice).setChunkSize(setup._chunk_size);
      Log.info("Parse chunk size " + setup._chunk_size);
    }
 else     if (ice instanceof Frame && ((Frame)ice).vec(0) instanceof FileVec) {
      ((FileVec)((Frame)ice).vec(0)).setChunkSize((Frame)ice,setup._chunk_size);
      Log.info("Parse chunk size " + setup._chunk_size);
    }
  }
  long memsz=H2O.CLOUD.free_mem();
  if (totalParseSize > memsz * 4)   throw new IllegalArgumentException("Total input file size of " + PrettyPrint.bytes(totalParseSize) + " is much larger than total cluster memory of "+ PrettyPrint.bytes(memsz)+ ", please use either a larger cluster or smaller data.");
  if (H2O.GA != null)   GAUtils.logParse(totalParseSize,keys.length,setup._number_columns);
  ParseDataset pds=new ParseDataset(dest);
  new Frame(pds._job._result,new String[0],new Vec[0]).delete_and_lock(pds._job);
  for (  Key k : keys)   Lockable.read_lock(k,pds._job);
  ParserFJTask fjt=new ParserFJTask(pds,keys,setup,deleteOnDone);
  pds._job.start(fjt,totalParseSize);
  return pds;
}
