{
  HashSet<String> conflictingNames=setup.checkDupColumnNames();
  for (  String x : conflictingNames)   throw new IllegalArgumentException("Found duplicate column name " + x);
  long totalParseSize=0;
  ByteVec bv;
  float dcr, maxDecompRatio=0;
  for (int i=0; i < keys.length; i++) {
    Key k=keys[i];
    if (dest.equals(k))     throw new IllegalArgumentException("Destination key " + dest + " must be different from all sources");
    if (delete_on_done)     for (int j=i + 1; j < keys.length; j++)     if (k == keys[j])     throw new IllegalArgumentException("Source key " + k + " appears twice, delete_on_done must be false");
    bv=getByteVec(k);
    dcr=ZipUtil.decompressionRatio(bv);
    if (dcr > maxDecompRatio)     maxDecompRatio=dcr;
    totalParseSize+=bv.length() * maxDecompRatio;
  }
  setup._chunkSize=Vec.calcOptimalChunkSize(totalParseSize,setup._ncols);
  Log.info("Chunk size " + setup._chunkSize);
  Vec update;
  Iced ice;
  for (int i=0; i < keys.length; ++i) {
    ice=DKV.getGet(keys[i]);
    update=(Vec)(ice instanceof Vec ? ice : ((Frame)ice).vec(0));
    update.setChunkSize(setup._chunkSize);
    DKV.put(update._key,update);
  }
  long memsz=H2O.CLOUD.memsz();
  if (totalParseSize > memsz * 4)   throw new IllegalArgumentException("Total input file size of " + PrettyPrint.bytes(totalParseSize) + " is much larger than total cluster memory of "+ PrettyPrint.bytes(memsz)+ ", please use either a larger cluster or smaller data.");
  ParseDataset job=new ParseDataset(dest);
  new Frame(job.dest(),new String[0],new Vec[0]).delete_and_lock(job._key);
  for (  Key k : keys)   Lockable.read_lock(k,job._key);
  ParserFJTask fjt=new ParserFJTask(job,keys,setup,delete_on_done);
  job.start(fjt,totalParseSize);
  return job;
}
