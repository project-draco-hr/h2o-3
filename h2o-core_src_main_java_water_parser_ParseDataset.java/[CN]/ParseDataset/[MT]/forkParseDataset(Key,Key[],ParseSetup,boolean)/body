{
  HashSet<String> conflictingNames=setup.checkDupColumnNames();
  for (  String x : conflictingNames)   if (!x.equals(""))   throw new IllegalArgumentException("Found duplicate column name " + x);
  long totalParseSize=0;
  for (int i=0; i < keys.length; i++) {
    Key k=keys[i];
    if (dest.equals(k))     throw new IllegalArgumentException("Destination key " + dest + " must be different from all sources");
    if (deleteOnDone)     for (int j=i + 1; j < keys.length; j++)     if (k == keys[j])     throw new IllegalArgumentException("Source key " + k + " appears twice, deleteOnDone must be false");
    totalParseSize+=getByteVec(k).length();
  }
  Log.info("Total file size: " + PrettyPrint.bytes(totalParseSize));
  for (int i=0; i < keys.length; ++i) {
    Iced ice=DKV.getGet(keys[i]);
    if (ice instanceof FileVec) {
      ((FileVec)ice).setChunkSize(setup._chunk_size);
      Log.info("Parse chunk size " + setup._chunk_size);
    }
 else     if (ice instanceof Frame && ((Frame)ice).vec(0) instanceof FileVec) {
      ((FileVec)((Frame)ice).vec(0)).setChunkSize((Frame)ice,setup._chunk_size);
      Log.info("Parse chunk size " + setup._chunk_size);
    }
  }
  long memsz=H2O.CLOUD.free_mem();
  if (totalParseSize > memsz * 4)   throw new IllegalArgumentException("Total input file size of " + PrettyPrint.bytes(totalParseSize) + " is much larger than total cluster memory of "+ PrettyPrint.bytes(memsz)+ ", please use either a larger cluster or smaller data.");
  if (H2O.GA != null)   GAUtils.logParse(totalParseSize,keys.length,setup._number_columns);
  ParseDataset job=new ParseDataset(dest);
  new Frame(job.dest(),new String[0],new Vec[0]).delete_and_lock(job._key);
  for (  Key k : keys)   Lockable.read_lock(k,job._key);
  ParserFJTask fjt=new ParserFJTask(job,keys,setup,deleteOnDone);
  job.start(fjt,totalParseSize,true);
  return job;
}
