def test_deeplearning_grid_search_over_params(self):
    "\n        test_deeplearning_fieldnames performs the following:\n        a. build H2O deeplearning models using grid search.  Count and make sure models\n           are only built for hyper-parameters set to legal values.  No model is built for bad hyper-parameters\n           values.  We should instead get a warning/error message printed out.\n        c. For each model built using grid search, we will extract the parameters used in building\n           that model and manually build a H2O deeplearning model.  Training metrics are calculated from the\n           gridsearch model and the manually built model.  If their metrics\n           differ by too much, print a warning message but don't fail the test.\n        d. we will check and make sure the models are built within the max_runtime_secs time limit that was set\n           for it as well.  If max_runtime_secs was exceeded, declare test failure.\n        "
    print('*******************************************************************************************')
    print(('test_deeplearning_fieldnames for deeplearning ' + self.family))
    h2o.cluster_info()
    try:
        print('Hyper-parameters used here is {0}'.format(self.final_hyper_params))
        grid_model = H2OGridSearch(H2ODeepLearningEstimator(nfolds=self.nfolds), hyper_params=self.final_hyper_params)
        grid_model.train(x=self.x_indices, y=self.y_index, training_frame=self.training1_data)
        params_dict = dict()
        params_dict['distribution'] = self.family
        params_dict['nfolds'] = self.nfolds
        total_run_time_limits = 0.0
        true_run_time_limits = 0.0
        manual_run_runtime = 0.0
        for each_model in grid_model:
            params_list = grid_model.get_hyperparams_dict(each_model._id)
            params_list.update(params_dict)
            model_params = dict()
            if ('max_runtime_secs' in params_list):
                model_params['max_runtime_secs'] = params_list['max_runtime_secs']
                max_runtime = params_list['max_runtime_secs']
                del params_list['max_runtime_secs']
            else:
                max_runtime = 0
            if ('elastic_averaging_moving_rate' in params_list):
                model_params['elastic_averaging_moving_rate'] = params_list['elastic_averaging_moving_rate']
                del params_list['elastic_averaging_moving_rate']
            if ('validation_frame' in params_list):
                model_params['validation_frame'] = params_list['validation_frame']
                del params_list['validation_frame']
            if ('elastic_averaging_regularization' in params_list):
                model_params['elastic_averaging_regularization'] = params_list['elastic_averaging_regularization']
                del params_list['elastic_averaging_regularization']
            if ('hidden' in params_list):
                temp = params_list['hidden']
                params_list['hidden'] = [temp]
            if ('hidden_dropout_ratios' in params_list):
                temp = params_list['hidden_dropout_ratios']
                params_list['hidden_dropout_ratios'] = [temp]
            manual_model = H2ODeepLearningEstimator(**params_list)
            manual_model.train(x=self.x_indices, y=self.y_index, training_frame=self.training1_data, **model_params)
            model_runtime = pyunit_utils.find_grid_runtime([manual_model])
            manual_run_runtime += model_runtime
            summary_list = manual_model._model_json['output']['scoring_history']
            if (len(summary_list.cell_values) < 3):
                num_iterations = 1
            else:
                num_iterations = summary_list['iterations'][2]
            if (max_runtime > 0):
                if ((max_runtime < self.min_runtime_per_iteration) or (num_iterations <= 1)):
                    total_run_time_limits += model_runtime
                else:
                    total_run_time_limits += max_runtime
            true_run_time_limits += max_runtime
            grid_model_metrics = each_model.model_performance()._metric_json[self.training_metric]
            manual_model_metrics = manual_model.model_performance()._metric_json[self.training_metric]
            if (not ((type(grid_model_metrics) == str) or (type(manual_model_metrics) == str))):
                if ((abs(grid_model_metrics) > 0) and ((abs((grid_model_metrics - manual_model_metrics)) / grid_model_metrics) > self.allowed_diff)):
                    print('test_deeplearning_fieldnames for deeplearning warning: grid search model metric ({0}) and manually built H2O model metric ({1}) differ too much!'.format(grid_model_metrics, manual_model_metrics))
        total_run_time_limits = (max(total_run_time_limits, true_run_time_limits) * (1 + self.extra_time_fraction))
        if (not (manual_run_runtime <= total_run_time_limits)):
            self.test_failed += 1
            print('test_deeplearning_fieldnames for deeplearning failed: time taken to manually build models is {0}.  Maximum allowed time is {1}'.format(manual_run_runtime, total_run_time_limits))
        else:
            print('time taken to manually build all models is {0}. Maximum allowed time is {1}'.format(manual_run_runtime, total_run_time_limits))
        if (self.test_failed == 0):
            print('test_deeplearning_fieldnames for deeplearning has passed!')
    except Exception as e:
        if (len(grid_model) > 0):
            print('test_deeplearning_fieldnames for deeplearning failed: exception ({0}) was thrown for no reason.'.format(e))
            self.test_failed += 1
