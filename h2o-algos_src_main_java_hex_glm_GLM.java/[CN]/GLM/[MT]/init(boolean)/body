{
  super.init(expensive);
  hide("_balance_classes","Not applicable since class balancing is not required for GLM.");
  hide("_max_after_balance_size","Not applicable since class balancing is not required for GLM.");
  hide("_class_sampling_factors","Not applicable since class balancing is not required for GLM.");
  _parms.validate(this);
  if (_response != null) switch (_parms._family) {
case binomial:
    if (!_response.isBinary() && _nclass != 2)     error("_family",H2O.technote(2,"Binomial requires the response to be a 2-class categorical or a binary column (0/1)"));
  break;
case multinomial:
if (_nclass <= 2) error("_family",H2O.technote(2,"Multinomial requires a categorical response with at least 3 levels (for 2 class problem use family=binomial."));
break;
case poisson:
if (_nclass != 1) error("_family","Poisson requires the response to be numeric.");
if (_response.min() < 0) error("_family","Poisson requires response >= 0");
if (!_response.isInt()) warn("_family","Poisson expects non-negative integer response, got floats.");
break;
case gamma:
if (_nclass != 1) error("_distribution",H2O.technote(2,"Gamma requires the response to be numeric."));
if (_response.min() <= 0) error("_family","Gamma requires positive respone");
break;
case tweedie:
if (_nclass != 1) error("_family",H2O.technote(2,"Tweedie requires the response to be numeric."));
break;
case gaussian:
break;
default :
error("_family","Invalid distribution: " + _parms._distribution);
}
if (expensive) {
if (error_count() > 0) return;
_lsc=new LambdaSearchScoringHistory();
_sc=new ScoringHistory();
if (_parms._alpha == null) _parms._alpha=new double[]{_parms._solver == Solver.IRLSM || _parms._solver == Solver.COORDINATE_DESCENT_NAIVE ? .5 : 0};
_train.bulkRollups();
_sc=new ScoringHistory();
_t0=System.currentTimeMillis();
if (_parms._lambda_search || !_parms._intercept || _parms._lambda == null || _parms._lambda[0] > 0) _parms._use_all_factor_levels=true;
if (_parms._max_active_predictors == -1) _parms._max_active_predictors=_parms._solver == Solver.IRLSM ? 7000 : 100000000;
if (_parms._link == Link.family_default) _parms._link=_parms._family.defaultLink;
_dinfo=new DataInfo(_train.clone(),_valid,1,_parms._use_all_factor_levels || _parms._lambda_search,_parms._standardize ? DataInfo.TransformType.STANDARDIZE : DataInfo.TransformType.NONE,DataInfo.TransformType.NONE,true,false,false,hasWeightCol(),hasOffsetCol(),hasFoldCol());
checkMemoryFootPrint(_dinfo);
if (_parms._max_iterations == -1) {
int numclasses=_parms._family == Family.multinomial ? nclasses() : 1;
if (_parms._solver == Solver.IRLSM) {
_parms._max_iterations=_parms._lambda_search ? numclasses * 10 * _parms._nlambdas : numclasses * 50;
}
 else {
_parms._max_iterations=numclasses * Math.max(20,_dinfo.fullN() >> 2);
if (_parms._lambda_search) _parms._max_iterations=_parms._nlambdas * 100 * numclasses;
}
}
BetaConstraint bc=(_parms._beta_constraints != null) ? new BetaConstraint(_parms._beta_constraints.get()) : new BetaConstraint();
if (_valid != null) _validDinfo=_dinfo.validDinfo(_valid);
_state=new ComputationState(_job._key,_parms,_dinfo,bc,nclasses());
boolean skippingRows=true;
if (hasWeightCol() || skippingRows) {
boolean setWeights=skippingRows && _parms._lambda_search && _parms._alpha[0] > 0;
if (setWeights) {
Vec wc=_weights == null ? _dinfo._adaptedFrame.anyVec().makeCon(1) : _weights.makeCopy();
_dinfo.setWeights(_generatedWeights="__glm_gen_weights",wc);
}
YMUTask ymt=new YMUTask(_dinfo,_parms._family == Family.multinomial ? nclasses() : 1,!_parms._stdOverride,false,setWeights,true).doAll(_dinfo._adaptedFrame);
if (ymt._wsum == 0) throw new IllegalArgumentException("No rows left in the dataset after filtering out rows with missing values. Ignore columns with many NAs or impute your missing values prior to calling glm.");
Log.info(LogMsg("using " + ymt._nobs + " nobs out of "+ _dinfo._adaptedFrame.numRows()+ " total"));
_nobs=ymt._nobs;
if (_parms._obj_reg == -1) _parms._obj_reg=1.0 / ymt._wsum;
_dinfo.updateWeightedSigmaAndMean(ymt._basicStats.sigma(),ymt._basicStats.mean());
_state._ymu=_parms._intercept ? ymt._yMu : new double[]{_parms.linkInv(0)};
}
 else {
_nobs=_train.numRows();
if (_parms._obj_reg == -1) _parms._obj_reg=1.0 / _nobs;
if (_parms._family == Family.multinomial) {
_state._ymu=MemoryManager.malloc8d(_nclass);
for (int i=0; i < _state._ymu.length; ++i) _state._ymu[i]=_priorClassDist[i];
}
 else _state._ymu=new double[]{_parms._intercept ? _train.lastVec().mean() : 0};
}
if (hasOffsetCol() && _parms._intercept) {
GLMGradientSolver gslvr=new GLMGradientSolver(_job._key,_parms,_dinfo.filterExpandedColumns(new int[0]),0,_state.activeBC());
double[] x=new L_BFGS().solve(gslvr,new double[]{-_offset.mean()}).coefs;
x[0]=_parms.linkInv(x[0]);
_state._ymu=x;
Log.info(LogMsg("fitted intercept = " + x[0]));
}
if (_parms._prior > 0) _iceptAdjust=-Math.log(_state._ymu[0] * (1 - _parms._prior) / (_parms._prior * (1 - _state._ymu[0])));
ArrayList<Vec> vecs=new ArrayList<>();
if (_weights != null) vecs.add(_weights);
if (_offset != null) vecs.add(_offset);
vecs.add(_response);
if (_valid != null) {
ArrayList<Vec> testVecs=new ArrayList<>();
boolean testWeights=false;
boolean testOffset=false;
testVecs.add(_valid.vec(_parms._response_column));
}
_model=new GLMModel(_result,_parms,GLM.this,_state._ymu,_dinfo._adaptedFrame.lastVec().sigma(),_lmax,_nobs);
String[] warns=_model.adaptTestForTrain(_valid,true,true);
for (String s : warns) warn("_validation_frame",s);
if (_parms._lambda_min_ratio == -1) _parms._lambda_min_ratio=(_nobs >> 4) > _dinfo.fullN() ? 1e-4 : 1e-2;
double[] beta=getNullBeta();
GLMGradientInfo ginfo=new GLMGradientSolver(_job._key,_parms,_dinfo,0,_state.activeBC()).getGradient(beta);
_state.updateState(beta,ginfo);
if (_parms._lambda == null) {
_lmax=lmax(_state.ginfo()._gradient);
if (_parms._lambda_search) {
if (_parms._nlambdas == -1) _parms._nlambdas=100;
_parms._lambda=new double[_parms._nlambdas];
double dec=Math.pow(_parms._lambda_min_ratio,1.0 / (_parms._nlambdas - 1));
_parms._lambda[0]=_lmax;
double l=_lmax;
for (int i=1; i < _parms._nlambdas; ++i) _parms._lambda[i]=(l*=dec);
}
 else _parms._lambda=new double[]{10 * _parms._lambda_min_ratio * _lmax};
}
_model.clone2().delete_and_lock(_job._key);
}
}
