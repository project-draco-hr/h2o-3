{
  super.init(expensive);
  if (expensive && _parms._seed == -1)   _parms._seed=RandomUtils.getRNG(System.nanoTime()).nextLong();
  if (H2O.ARGS.client && _parms._build_tree_one_node)   error("_build_tree_one_node","Cannot run on a single node in client mode");
  if (_parms._min_rows < 0)   error("_min_rows","Requested min_rows must be greater than 0");
  if (_parms._ntrees < 0 || _parms._ntrees > MAX_NTREES)   error("_ntrees","Requested ntrees must be between 1 and " + MAX_NTREES);
  _ntrees=_parms._ntrees;
  if (_parms.hasCheckpoint()) {
    Value cv=DKV.get(_parms._checkpoint);
    if (cv != null) {
      M checkpointModel=cv.get();
      try {
        _parms.validateWithCheckpoint(checkpointModel._parms);
        if (isClassifier() != checkpointModel._output.isClassifier())         throw new IllegalArgumentException("Response type must be the same as for the checkpointed model.");
        if (!Arrays.equals(_train.names(),checkpointModel._output._names)) {
          throw new IllegalArgumentException("The columns of the training data must be the same as for the checkpointed model");
        }
        if (!Arrays.deepEquals(_train.domains(),checkpointModel._output._domains)) {
          throw new IllegalArgumentException("Categorical factor levels of the training data must be the same as for the checkpointed model");
        }
      }
 catch (      H2OIllegalArgumentException e) {
        error(e.values.get("argument").toString(),e.values.get("value").toString());
      }
      if (_parms._ntrees < checkpointModel._output._ntrees + 1)       error("_ntrees","If checkpoint is specified then requested ntrees must be higher than " + (checkpointModel._output._ntrees + 1));
      _ntrees=_parms._ntrees - checkpointModel._output._ntrees;
    }
  }
  if (_parms._nbins <= 1)   error("_nbins","nbins must be > 1.");
  if (_parms._nbins >= 1 << 16)   error("_nbins","nbins must be < " + (1 << 16));
  if (_parms._nbins_cats <= 1)   error("_nbins_cats","nbins_cats must be > 1.");
  if (_parms._nbins_cats >= 1 << 16)   error("_nbins_cats","nbins_cats must be < " + (1 << 16));
  if (_parms._nbins_top_level < _parms._nbins)   error("_nbins_top_level","nbins_top_level must be >= nbins (" + _parms._nbins + ").");
  if (_parms._nbins_top_level >= 1 << 16)   error("_nbins_top_level","nbins_top_level must be < " + (1 << 16));
  if (_parms._max_depth <= 0)   error("_max_depth","_max_depth must be > 0.");
  if (_parms._min_rows <= 0)   error("_min_rows","_min_rows must be > 0.");
  if (!(0.0 < _parms._sample_rate && _parms._sample_rate <= 1.0))   error("_sample_rate","sample_rate should be in interval ]0,1] but it is " + _parms._sample_rate);
  if (!(0.0 < _parms._col_sample_rate_per_tree && _parms._col_sample_rate_per_tree <= 1.0))   error("_col_sample_rate_per_tree","col_sample_rate_per_tree should be in interval ]0,1] but it is " + _parms._col_sample_rate_per_tree);
  if (_train != null) {
    double sumWeights=_train.numRows() * (hasWeightCol() ? _train.vec(_parms._weights_column).mean() : 1);
    if (sumWeights < 2 * _parms._min_rows)     error("_min_rows","The dataset size is too small to split for min_rows=" + _parms._min_rows + ": must have at least "+ 2 * _parms._min_rows + " (weighted) rows, but have only " + sumWeights + ".");
  }
  if (_train != null)   _ncols=_train.numCols() - 1 - numSpecialCols();
}
