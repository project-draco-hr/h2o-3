def __init__(self, model_id=None, overwrite_with_best_model=None, checkpoint=None, use_all_factor_levels=None, standardize=None, activation=None, hidden=None, epochs=None, train_samples_per_iteration=None, seed=None, adaptive_rate=None, rho=None, epsilon=None, rate=None, rate_annealing=None, rate_decay=None, momentum_start=None, momentum_ramp=None, momentum_stable=None, nesterov_accelerated_gradient=None, input_dropout_ratio=None, hidden_dropout_ratios=None, l1=None, l2=None, max_w2=None, initial_weight_distribution=None, initial_weight_scale=None, loss=None, distribution=None, tweedie_power=None, score_interval=None, score_training_samples=None, score_validation_samples=None, score_duty_cycle=None, classification_stop=None, regression_stop=None, quiet_mode=None, max_confusion_matrix_size=None, max_hit_ratio_k=None, balance_classes=None, class_sampling_factors=None, max_after_balance_size=None, score_validation_sampling=None, diagnostics=None, variable_importances=None, fast_mode=None, ignore_const_cols=None, force_load_balance=None, replicate_training_data=None, single_node_mode=None, shuffle_training_data=None, sparse=None, col_major=None, average_activation=None, sparsity_beta=None, max_categorical_features=None, reproducible=None, export_weights_and_biases=None, nfolds=None, fold_assignment=None, keep_cross_validation_predictions=None, stopping_rounds=None, stopping_metric=None, stopping_tolerance=None):
    '\n    Build a supervised Deep Learning model\n    Performs Deep Learning neural networks on an H2OFrame\n\n    Parameters\n    ----------\n    model_id : str, optional\n      The unique id assigned to the resulting model. If none is given, an id will\n      automatically be generated.\n    overwrite_with_best_model : bool\n      If True, overwrite the final model with the best model found during training.\n      Defaults to True.\n    checkpoint : H2ODeepLearningModel, optional\n      Model checkpoint (either key or H2ODeepLearningModel) to resume training with.\n    use_all_factor_levels : bool\n      Use all factor levels of categorical variance. Otherwise the first factor level is\n      omitted (without loss of accuracy). Useful for variable importances and auto-enabled\n      for autoencoder.\n    standardize : bool\n      If enabled, automatically standardize the data. If disabled, the user must\n      provide properly scaled input data.\n    activation : str\n      A string indicating the activation function to use.\n      Must be either "Tanh", "TanhWithDropout", "Rectifier", "RectifierWithDropout",\n      "Maxout", or "MaxoutWithDropout"\n    hidden : list\n      Hidden layer sizes (e.g. [100,100])\n    epochs : float\n      How many times the dataset should be iterated (streamed), can be fractional\n    train_samples_per_iteration : int\n      Number of training samples (globally) per MapReduce iteration.\n      Special values are: 0 one epoch; -1 all available data\n      (e.g., replicated training data); or -2 auto-tuning (default)\n    seed : int\n      Seed for random numbers (affects sampling) - Note: only reproducible when\n      running single threaded\n    adaptive_rate : bool\n      Adaptive learning rate (ADAELTA)\n    rho : float\n      Adaptive learning rate time decay factor (similarity to prior updates)\n    epsilon : float\n      Adaptive learning rate parameter, similar to learn rate annealing during initial\n      training phase. Typical values are between 1.0e-10 and 1.0e-4\n    rate : float\n      Learning rate (higher => less stable, lower => slower convergence)\n    rate_annealing : float\n      Learning rate annealing: \\eqn{(rate)/(1 + rate_annealing*samples)\n    rate_decay : float\n      Learning rate decay factor between layers (N-th layer: \\eqn{rate*\x07lpha^(N-1))\n    momentum_start : float\n      Initial momentum at the beginning of training (try 0.5)\n    momentum_ramp : float\n      Number of training samples for which momentum increases\n    momentum_stable : float\n      Final momentum after the amp is over (try 0.99)\n    nesterov_accelerated_gradient : bool\n      Logical. Use Nesterov accelerated gradient (recommended)\n    input_dropout_ratio : float\n      A fraction of the features for each training row to be omitted from training in\n      order to improve generalization (dimension sampling).\n    hidden_dropout_ratios : float\n      Input layer dropout ratio (can improve generalization) specify one value per hidden\n      ayer, defaults to 0.5\n    l1 : float\n      L1 regularization (can add stability and improve generalization,\n      causes many weights to become 0)\n    l2 : float\n      L2 regularization (can add stability and improve generalization,\n      causes many weights to be small)\n    max_w2 : float\n      Constraint for squared sum of incoming weights per unit (e.g. Rectifier)\n    initial_weight_distribution : str\n      Can be "Uniform", "UniformAdaptive", or "Normal"\n    initial_weight_scale : str\n      Uniform: -value ... value, Normal: stddev\n    loss : str\n      Loss function: "Automatic", "CrossEntropy" (for classification only),\n      "Quadratic", "Absolute" (experimental) or "Huber" (experimental)\n    distribution : str\n       A character string. The distribution function of the response.\n       Must be "AUTO", "bernoulli", "multinomial", "poisson", "gamma",\n       "tweedie", "laplace", "huber" or "gaussian"\n    tweedie_power : float\n      Tweedie power (only for Tweedie distribution, must be between 1 and 2)\n    score_interval : int\n      Shortest time interval (in secs) between model scoring\n    score_training_samples : int\n      Number of training set samples for scoring (0 for all)\n    score_validation_samples : int\n      Number of validation set samples for scoring (0 for all)\n    score_duty_cycle : float\n      Maximum duty cycle fraction for scoring (lower: more training, higher: more scoring)\n    classification_stop : float\n      Stopping criterion for classification error fraction on training data\n      (-1 to disable)\n    regression_stop : float\n      Stopping criterion for regression error (MSE) on training data (-1 to disable)\n    stopping_rounds : int\n      Early stopping based on convergence of stopping_metric.\n      Stop if simple moving average of length k of the stopping_metric does not improve\n      (by stopping_tolerance) for k=stopping_rounds scoring events.\n      Can only trigger after at least 2k scoring events. Use 0 to disable.\n    stopping_metric : str\n      Metric to use for convergence checking, only for _stopping_rounds > 0\n      Can be one of "AUTO", "deviance", "logloss", "MSE", "AUC", "r2", "misclassification".\n    stopping_tolerance : float\n      Relative tolerance for metric-based stopping criterion (stop if relative improvement is not at least this much)\n    quiet_mode : bool\n      Enable quiet mode for less output to standard output\n    max_confusion_matrix_size : int\n      Max. size (number of classes) for confusion matrices to be shown\n    max_hit_ratio_k : float\n      Max number (top K) of predictions to use for hit ratio computation\n      (for multi-class only, 0 to disable)\n    balance_classes : bool\n      Balance training data class counts via over/under-sampling (for imbalanced data)\n    class_sampling_factors : list\n      Desired over/under-sampling ratios per class (in lexicographic order).\n      If not specified, sampling factors will be automatically computed to obtain class\n      balance during training. Requires balance_classes.\n    max_after_balance_size : float\n      Maximum relative size of the training data after balancing class counts\n      (can be less than 1.0)\n    score_validation_sampling :\n      Method used to sample validation dataset for scoring\n    diagnostics : bool\n      Enable diagnostics for hidden layers\n    variable_importances : bool\n      Compute variable importances for input features (Gedeon method) - can be slow\n      for large networks)\n    fast_mode : bool\n      Enable fast mode (minor approximations in back-propagation)\n    ignore_const_cols : bool\n      Ignore constant columns (no information can be gained anyway)\n    force_load_balance : bool\n      Force extra load balancing to increase training speed for small datasets\n      (to keep all cores busy)\n    replicate_training_data : bool\n      Replicate the entire training dataset onto every node for faster training\n    single_node_mode : bool\n      Run on a single node for fine-tuning of model parameters\n    shuffle_training_data : bool\n      Enable shuffling of training data (recommended if training data is replicated and\n      train_samples_per_iteration is close to \\eqn{numRows*numNodes\n    sparse : bool\n      Sparse data handling (Experimental)\n    col_major : bool\n      Use a column major weight matrix for input layer. Can speed up forward propagation,\n      but might slow down back propagation (Experimental)\n    average_activation : float\n      Average activation for sparse auto-encoder (Experimental)\n    sparsity_beta : bool\n      Sparsity regularization (Experimental)\n    max_categorical_features : int\n      Max. number of categorical features, enforced via hashing Experimental)\n    reproducible : bool\n      Force reproducibility on small data (will be slow - only uses 1 thread)\n    export_weights_and_biases : bool\n      Whether to export Neural Network weights and biases to H2O Frames"\n    nfolds : int, optional\n      Number of folds for cross-validation. If nfolds >= 2, then validation must remain\n      empty.\n    fold_assignment : str\n      Cross-validation fold assignment scheme, if fold_column is not specified\n      Must be "AUTO", "Random" or "Modulo"\n    keep_cross_validation_predictions : bool\n      Whether to keep the predictions of the cross-validation models\n\n    Examples\n    --------\n      >>> import h2o as ml\n      >>> from h2o.estimators.deeplearning import H2ODeepLearningEstimator\n      >>> ml.init()\n      >>> rows=[[1,2,3,4,0],[2,1,2,4,1],[2,1,4,2,1],[0,1,2,34,1],[2,3,4,1,0]]*50\n      >>> fr = ml.H2OFrame(rows)\n      >>> fr[4] = fr[4].asfactor()\n      >>> model = H2ODeepLearningEstimator()\n      >>> model.train(x=range(4), y=4, training_frame=fr)\n    '
    super(H2ODeepLearningEstimator, self).__init__()
    self._parms = locals()
    self._parms = {k: v for (k, v) in self._parms.items() if (k != 'self')}
    self._parms['autoencoder'] = isinstance(self, H2OAutoEncoderEstimator)
