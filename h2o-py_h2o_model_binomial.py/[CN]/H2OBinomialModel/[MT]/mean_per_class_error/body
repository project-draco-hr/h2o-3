def mean_per_class_error(self, thresholds=None, train=False, valid=False, xval=False):
    '\n    Get the mean per class error for a set of thresholds.\n    If all are False (default), then return the training metric value.\n    If more than one options is set to True, then return a dictionary of metrics where the keys are "train", "valid",\n    and "xval"\n\n    Parameters\n    ----------\n      thresholds : list, optional\n        If None, then the thresholds in this set of metrics will be used.\n\n      train : bool, optional\n        If True, return the mean_per_class_error value for the training data.\n\n      valid : bool, optional\n        If True, return the mean_per_class_error value for the validation data.\n\n      xval : bool, optional\n        If True, return the mean_per_class_error value for each of the cross-validated splits.\n\n    Returns\n    -------\n      The mean_per_class_error values for the specified key(s).\n    '
    tm = ModelBase._get_metrics(self, train, valid, xval)
    m = {}
    for (k, v) in zip(list(tm.keys()), list(tm.values())):
        m[k] = (None if (v is None) else [[mpca[0], (1 - mpca[1])] for mpca in v.metric('mean_per_class_accuracy', thresholds=thresholds)])
    return (list(m.values())[0] if (len(m) == 1) else m)
