{
  DeepLearningModel cp=null;
  if (_parms._checkpoint == null) {
    cp=new DeepLearningModel(dest(),_parms,new DeepLearningModel.DeepLearningModelOutput(DeepLearning.this),_train,_valid,nclasses());
    cp.model_info().initializeMembers();
  }
 else {
    final DeepLearningModel previous=DKV.getGet(_parms._checkpoint);
    if (previous == null)     throw new IllegalArgumentException("Checkpoint not found.");
    Log.info("Resuming from checkpoint.");
    _job.update(0,"Resuming from checkpoint");
    if (isClassifier() != previous._output.isClassifier())     throw new H2OIllegalArgumentException("Response type must be the same as for the checkpointed model.");
    if (isSupervised() != previous._output.isSupervised())     throw new H2OIllegalArgumentException("Model type must be the same as for the checkpointed model.");
    DeepLearningParameters.Sanity.checkIfParameterChangeAllowed(previous._parms,_parms);
    DataInfo dinfo;
    try {
      for (      String st : previous.adaptTestForTrain(_train,true,false))       Log.warn(st);
      for (      String st : previous.adaptTestForTrain(_valid,true,false))       Log.warn(st);
      dinfo=makeDataInfo(_train,_valid,_parms,nclasses());
      DKV.put(dinfo);
      cp=new DeepLearningModel(dest(),_parms,previous,false,dinfo);
      cp.write_lock(_job);
      if (!Arrays.equals(cp._output._names,previous._output._names)) {
        throw new H2OIllegalArgumentException("The columns of the training data must be the same as for the checkpointed model. Check ignored columns (or disable ignore_const_cols).");
      }
      if (!Arrays.deepEquals(cp._output._domains,previous._output._domains)) {
        throw new H2OIllegalArgumentException("Categorical factor levels of the training data must be the same as for the checkpointed model.");
      }
      if (dinfo.fullN() != previous.model_info().data_info().fullN()) {
        throw new H2OIllegalArgumentException("Total number of predictors is different than for the checkpointed model.");
      }
      if (_parms._epochs <= previous.epoch_counter) {
        throw new H2OIllegalArgumentException("Total number of epochs must be larger than the number of epochs already trained for the checkpointed model (" + previous.epoch_counter + ").");
      }
      final DeepLearningParameters actualParms=cp.model_info().get_params();
      assert(actualParms != previous.model_info().get_params());
      assert(actualParms != _parms);
      assert(actualParms != previous._parms);
      DeepLearningParameters.Sanity.updateParametersDuringCheckpointRestart(_parms,previous._parms,false,false);
      DeepLearningParameters.Sanity.updateParametersDuringCheckpointRestart(_parms,cp.model_info().get_params(),true,true);
      DeepLearningParameters.Sanity.modifyParms(_parms,cp.model_info().get_params(),nclasses());
      Log.info("Continuing training after " + String.format("%.3f",previous.epoch_counter) + " epochs from the checkpointed model.");
      cp.update(_job);
    }
 catch (    H2OIllegalArgumentException ex) {
      if (cp != null) {
        cp.unlock(_job);
        cp.delete();
        cp=null;
      }
      throw ex;
    }
 finally {
      if (cp != null)       cp.unlock(_job);
    }
  }
  trainModel(cp);
  List<Key> keep=new ArrayList<>();
  try {
    if (_parms._export_weights_and_biases && cp._output.weights != null && cp._output.biases != null) {
      for (      Key k : Arrays.asList(cp._output.weights)) {
        keep.add(k);
        for (        Vec vk : ((Frame)DKV.getGet(k)).vecs()) {
          keep.add(vk._key);
        }
      }
      for (      Key k : Arrays.asList(cp._output.biases)) {
        keep.add(k);
        for (        Vec vk : ((Frame)DKV.getGet(k)).vecs()) {
          keep.add(vk._key);
        }
      }
    }
  }
  finally {
    Scope.exit(keep.toArray(new Key[keep.size()]));
  }
}
