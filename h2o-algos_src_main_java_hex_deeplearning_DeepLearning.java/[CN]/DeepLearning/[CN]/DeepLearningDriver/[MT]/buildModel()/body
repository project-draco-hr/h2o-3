{
  Scope.enter();
  DeepLearningModel cp=null;
  if (_parms._checkpoint == null) {
    cp=new DeepLearningModel(dest(),_parms,new DeepLearningModel.DeepLearningModelOutput(DeepLearning.this),_train,_valid,nclasses());
    cp.model_info().initializeMembers();
  }
 else {
    final DeepLearningModel previous=DKV.getGet(_parms._checkpoint);
    if (previous == null)     throw new IllegalArgumentException("Checkpoint not found.");
    Log.info("Resuming from checkpoint.");
    new ProgressUpdate("Resuming from checkpoint").fork(_progressKey);
    if (isClassifier() != previous._output.isClassifier())     throw new H2OIllegalArgumentException("Response type must be the same as for the checkpointed model.");
    if (isSupervised() != previous._output.isSupervised())     throw new H2OIllegalArgumentException("Model type must be the same as for the checkpointed model.");
    DeepLearningParameters oldP=previous._parms;
    DeepLearningParameters newP=_parms;
    DeepLearningParameters oldP2=(DeepLearningParameters)oldP.clone();
    DeepLearningParameters newP2=(DeepLearningParameters)newP.clone();
    DeepLearningParameters.Sanity.modifyParms(oldP,oldP2,nclasses());
    DeepLearningParameters.Sanity.modifyParms(newP,newP2,nclasses());
    DeepLearningParameters.Sanity.checkpoint(oldP2,newP2);
    DataInfo dinfo;
    try {
      dinfo=makeDataInfo(_train,_valid,_parms);
      DKV.put(dinfo);
      cp=new DeepLearningModel(dest(),_parms,previous,false,dinfo);
      cp._output._end_time=0;
      cp.write_lock(self());
      if (!Arrays.equals(cp._output._names,previous._output._names)) {
        throw new H2OIllegalArgumentException("The columns of the training data must be the same as for the checkpointed model. Check ignored columns (or disable ignore_const_cols).");
      }
      if (!Arrays.deepEquals(cp._output._domains,previous._output._domains)) {
        throw new H2OIllegalArgumentException("Categorical factor levels of the training data must be the same as for the checkpointed model.");
      }
      if (dinfo.fullN() != previous.model_info().data_info().fullN()) {
        throw new H2OIllegalArgumentException("Total number of predictors is different than for the checkpointed model.");
      }
      if (_parms._epochs <= previous.epoch_counter) {
        throw new H2OIllegalArgumentException("Total number of epochs must be larger than the number of epochs already trained for the checkpointed model (" + previous.epoch_counter + ").");
      }
      final DeepLearningParameters actualNewP=cp.model_info().get_params();
      assert(actualNewP != previous.model_info().get_params());
      assert(actualNewP != newP);
      assert(actualNewP != oldP);
      DeepLearningParameters.Sanity.update(actualNewP,newP,nclasses());
      Log.info("Continuing training after " + String.format("%.3f",previous.epoch_counter) + " epochs from the checkpointed model.");
      cp.update(self());
    }
 catch (    H2OIllegalArgumentException ex) {
      if (cp != null) {
        cp.unlock(self());
        cp.delete();
        cp=null;
      }
      throw ex;
    }
 finally {
      if (cp != null)       cp.unlock(self());
    }
  }
  trainModel(cp);
  List<Key> keep=new ArrayList<>();
  try {
    keep.add(dest());
    keep.add(cp.model_info().data_info()._key);
    keep.add(cp._output._training_metrics._key);
    if (cp._output._validation_metrics != null) {
      keep.add(cp._output._validation_metrics._key);
    }
    if (cp._output.weights != null && cp._output.biases != null) {
      for (      Key k : Arrays.asList(cp._output.weights)) {
        keep.add(k);
        for (        Vec vk : ((Frame)DKV.getGet(k)).vecs()) {
          keep.add(vk._key);
        }
      }
      for (      Key k : Arrays.asList(cp._output.biases)) {
        keep.add(k);
        for (        Vec vk : ((Frame)DKV.getGet(k)).vecs()) {
          keep.add(vk._key);
        }
      }
    }
  }
  finally {
    Scope.exit(keep.toArray(new Key[0]));
  }
}
