def __init__(self, x=None, y=None, training_frame=None, key=None, override_with_best_model=True, n_folds=0, checkpoint=None, autoencoder=False, use_all_factor_levels=True, activation=('Rectifier', 'Tanh', 'TanhWithDropout', 'RectifierWithDropout', 'Maxout', 'MaxoutWithDropout'), hidden=(200, 200), epochs=10, train_samples_per_iteration=(-2), seed=None, adaptive_rate=True, rho=0.99, epsilon=1e-08, rate=0.005, rate_annealing=1e-06, rate_decay=1.0, momentum_start=0, momentum_ramp=1000000.0, momentum_stable=0, nesterov_accelerated_gradient=True, input_dropout_ratio=0, hidden_dropout_ratios=None, l1=0, l2=0, max_w2=float('inf'), validation_frame=None, initial_weight_distribution=('UniformAdaptive', 'Uniform', 'Normal'), initial_weight_scale=1, loss=('Automatic', 'MeanSquare', 'CrossEntropy'), score_interval=5, score_training_samples=10000, score_validation_samples=0, score_duty_cycle=0.1, classification_stop=0, regression_stop=1e-06, quiet_mode=False, max_confusion_matrix_size=None, max_hit_ratio_k=0, balance_classes=False, max_after_balance_size=None, score_validation_sampling=('Uniform', 'Stratified'), diagnostics=True, variable_importances=False, fast_mode=True, ignore_const_cols=True, force_load_balance=True, replicate_training_data=True, single_node_mode=False, shuffle_training_data=False, sparse=False, col_major=False, **kwargs):
    '\n\n        :param x: Predictor columns (may be indices or strings)\n        :param y: Response column (may be an index or a string)\n        :param training_frame: An object of type H2OFrame\n        :param key: The output name of the model. If None, one will be generated.\n        :param override_with_best_model: If True, then H2O will override the final model\n                                         with the best model found during training.\n        :param n_folds: Number of folds for cross-validation. If n_folds >= 2, then\n                        validation_frame must be None.\n        :param checkpoint: Either a model key or a fitted H2ODeeplearningBuilder object\n                           to resume training with.\n        :param autoencoder: Enable auto-encoder for model building\n        :param use_all_factor_levels: Use all factor levels of categorical variance.\n                                      Otherwise the first factor level is omitted\n                                      (without loss of accuracy). Useful for variable\n                                      importances and auto-enabled for autoencoder.\n        :param activation: A string indicating which activation function to use.\n        :param hidden: A list/tuple indicating the sizes of hidden layers.\n        :param epochs: How many times the dataset should be iterated (streamed), may be\n                       fractional.\n        :param train_samples_per_iteration: Number of training samples (globally) per\n                                            MapReduce iteration.\n                                            Special values are:\n                                            0 for one epoch;\n                                           -1 for all available data\n                                               (e.g., replicated training data); or\n                                           -2 for auto-tuning (default)\n        :param seed: Seed for random numbers (affects sampling).\n                     Note: only reproducible when running single threaded.\n        :param adaptive_rate: Adaptive learning rate. (ADADELTA)\n        :param rho: Adaptive learning rate time decay factor (similarity to prior updates)\n        :param epsilon: Threshold.\n        :param rate: Learning rate (higher => less stable, lower => slower convergence).\n        :param rate_annealing: Learning rate annealing:(rate)/(1 + rate_annealing*samples)\n        :param rate_decay: Learning rate decay between layers (layer N: rate*alpha^(N-1)).\n        :param momentum_start: Initial momentum at the beginning of training (try 0.5).\n        :param momentum_ramp: Number of training samples for which momentum increases.\n        :param momentum_stable: Final momentum after their amp is over (try 0.99).\n        :param nesterov_accelerated_gradient: Use Nesterov\'s accelerated gradient.\n        :param input_dropout_ratio: A fraction of the features for each training row to\n                                    be omitted from training in order to improve\n                                    generalization (dimension sampling).\n        :param hidden_dropout_ratios: A fraction of the inputs for each hidden layer to be\n                                      omitted from training in order to improve\n                                      generalization. Defaults to 0.5 for each hidden\n                                      layer if omitted.\n        :param l1: A regularization method that constrains the absolute value of the\n                   weights and has the net effect of dropping some weights (setting them\n                   to zero) from a model to reduce complexity and avoid overfitting.\n        :param l2: A regularization method that constrains the sum of the squared\n                   weights. This method introduces bias into parameter estimates, but\n                   frequently produces substantial gains in modeling as estimate variance\n                   is reduced.\n        :param max_w2: A maximum on the sum of the squared incoming weights into any one\n                       neuron. This tuning parameter is especially useful for unbound\n                       activation functions such as Maxout or Rectifier.\n        :param validation_frame:  An object of type H2OFrame.\n        :param initial_weight_distribution: The distribution from which initial weights\n                                            are to be drawn. The default option is an\n                                            optimized initialization that considers the\n                                            size of the network. The "uniform" option uses\n                                            a uniform distribution with a mean of 0 and a\n                                            given interval. The "normal" option draws\n                                            weights from the standard normal distribution\n                                            with a mean of 0 and given standard deviation.\n\n        :param initial_weight_scale: The scale of the distribution function for Uniform or\n                                     Normal distributions. For Uniform, the values are\n                                     drawn uniformly from\n                                     -initial_weight_scale...initial_weight_scale. For\n                                     Normal, the values are drawn from a Normal\n                                     distribution with a standard deviation of\n                                    initial_weight_scale.\n        :param loss: The loss (error) function to be minimized by the model. Cross Entropy\n                     loss is used when the model output consists of independent\n                     hypotheses, and the outputs can be interpreted as the probability\n                     that each hypothesis is true. Cross entropy is the recommended loss\n                     function when the target values are class labels, and especially for\n                     imbalanced data. It strongly penalizes error in the prediction of the\n                     actual class label. Mean Square loss is used when the model output\n                     are continuous real values, but can be used for classification as\n                     well (where it emphasizes the error on all output classes, not just\n                     for the actual class).\n        :param score_interval: The minimum time (in seconds) to elapse between model\n                               scoring. The actual interval is determined by the number of\n                               training samples per iteration and the scoring duty cycle.\n        :param score_training_samples: The number of training dataset points to be used\n                                       for scoring. Will be randomly sampled. Use 0 for\n                                       selecting the entire training dataset.\n        :param score_validation_samples: The number of validation dataset points to be\n                                         used for scoring. Can be randomly sampled or\n                                         stratified (if "balance classes" is set and\n                                         "score validation sampling" is set to stratify).\n                                         Use 0 for selecting the entire training dataset.\n        :param score_duty_cycle: Maximum fraction of wall clock time spent on model\n                                 scoring on training and validation samples, and on\n                                 diagnostics such as computation of feature importances\n                                 (i.e., not on training).\n        :param classification_stop: The stopping criteria in terms of classification error\n                                    (1-accuracy) on the training data scoring dataset.\n                                    When the error is at or below this threshold, training\n                                    stops.\n        :param regression_stop: The stopping criteria in terms of regression error (MSE)\n                                on the training data scoring dataset. When the error is at\n                                or below this threshold, training stops.\n        :param quiet_mode: Enable quiet mode for less output to standard output.\n        :param max_confusion_matrix_size:\n        :param max_hit_ratio_k: Max number (top K) of predictions to use for hit ration\n                                computation(for multi-class only, 0 to disable)\n        :param balance_classes: Balance training data class counts via over/under-sampling\n                                (for imbalanced data).\n        :param max_after_balance_size: Maximum relative size of the training data after\n                                       balancing class counts (can be less than 1.0).\n        :param score_validation_sampling: Method used to sample the validation dataset for\n                                          scoring, see Score Validation Samples above.\n        :param diagnostics: Gather diagnostics for hidden layers, such as mean and RMS\n                            values of learning rate, momentum, weights and biases\n        :param variable_importances: Compute variable importances for input features\n                                     (Gedeon method) - can be slow for large networks).\n        :param fast_mode: Enable fast mode (minor approximation in back-propagation),\n                          should not affect results significantly.\n        :param ignore_const_cols: Ignore constant training columns.\n        :param force_load_balance: Increase training speed on small datasets by splitting\n                                   it into many chunks to allow utilization of all cores.\n        :param replicate_training_data: Replicate the entire training dataset onto every\n                                        node for faster training on small datasets.\n        :param single_node_mode: Run on a single node for fine-tuning of model parameters.\n                                 Can be useful for checkpoint resumes after training on\n                                 multiple nodes for fast initial convergence.\n        :param shuffle_training_data: Enable shuffling of training data (on each node).\n                                      This option is recommended if training data is\n                                      replicated on N nodes, and the number of training\n                                      samples per iteration is close to N times the\n                                      dataset size, where all nodes train will (almost)\n                                      all the data. It is automatically enabled if the\n                                      number of training samples per iteration is set to\n                                      -1 (or to N times the dataset size or larger).\n        :param sparse: Sparse data handling (experimental).\n        :param col_major: Use a column major weight matrix for input layer. Can speed up\n                          forward propagation, but might slow down back-propagation\n                          (Experimental).\n        :param kwargs: Any additional arguments to pass.\n        :return: A new H2ODeeplearningBuilder object\n        '
    super(H2ODeeplearningMBuilder, self).__init__(locals(), self.SELF, training_frame)
    self.__dict__.update(locals())
    self.activation = ('Rectifier' if isinstance(activation, tuple) else activation)
    self.initial_weight_distribution = ('UniformAdaptive' if isinstance(initial_weight_distribution, tuple) else initial_weight_distribution)
    self.loss = ('Automatic' if isinstance(loss, tuple) else loss)
    self.score_validation_sampling = ('Uniform' if isinstance(score_validation_sampling, tuple) else score_validation_sampling)
