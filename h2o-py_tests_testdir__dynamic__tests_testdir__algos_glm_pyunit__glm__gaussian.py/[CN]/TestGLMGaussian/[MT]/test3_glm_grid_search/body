def test3_glm_grid_search(self, solver_name):
    '\n        This test is used to test GridSearch with the following parameters:\n\n        1. Lambda = best_lambda value from test2\n        2. alpha = [0 0.5 0.99]\n        3. cross-validation with nfolds = 5, fold_assignment = "Random"\n\n        We will look at the best results from the grid search and compare it with test 1\n        results.\n\n        :param solver_name: string specify name of solver we want to choose for our GLM model\n\n        :return: None\n        '
    print('*******************************************************************************************')
    print(('Test3: explores various parameter settings in training the GLM using GridSearch using solver ' + solver_name))
    hyper_parameters = {'alpha': [0, 0.5, 0.99], }
    model_h2o_grid_search = H2OGridSearch(H2OGeneralizedLinearEstimator(family=self.family, Lambda=self.best_lambda, nfolds=5, fold_assignment='Random', solver=solver_name), hyper_parameters)
    model_h2o_grid_search.train(x=self.x_indices, y=self.y_index, training_frame=self.training_data_grid)
    temp_model = model_h2o_grid_search.sort_by('mse(xval=True)')
    best_model_id = temp_model['Model Id'][0]
    best_xval_mse = temp_model['mse(xval=True)'][0]
    best_model = h2o.get_model(best_model_id)
    best_model_test_metrics = best_model.model_performance(test_data=self.test_data)
    if (self.best_grid_mse < 0):
        self.best_grid_mse = best_xval_mse
        self.best_alpha = model_h2o_grid_search.get_hyperparams(best_model_id)
        self.best_solver = solver_name
    elif (best_xval_mse < self.best_grid_mse):
        self.best_grid_mse = best_xval_mse
        self.best_alpha = model_h2o_grid_search.get_hyperparams(best_model_id)
        self.best_solver = solver_name
    num_test_failed = self.test_failed
    (_, _, _, _, _, _, self.test_failed) = pyunit_utils.extract_comparison_attributes_and_print(best_model, best_model_test_metrics, '\nTest3 Done!', False, False, False, self.test1_weight, None, self.test1_mse_train, self.test1_mse_test, 'Comparing intercept and weights ....', 'H2O best model from gridsearch intercept and weights: ', 'h2o test1 intercept and weights: ', 'Intercept and weights are not equal!', 'Intercept and weights are close enough!', '', '', '', '', '', 'Comparing training MSEs ....', 'H2O best model from gridsearch training MSE: ', 'Test1 training MSE: ', 'Training MSEs are not equal!', 'Training MSEs are close enough!', 'Comparing test MSEs ....', 'H2O best model from gridsearch test MSE: ', 'Test1 test MSE: ', 'Test MSEs are not equal!', 'Test MSEs are close enough!', self.test_failed, self.ignored_eps, self.allowed_diff, self.noise_var, False)
    self.test_failed_array[self.test_num] += pyunit_utils.show_test_results('test_glm_grid_search_over_params', num_test_failed, self.test_failed)
    self.test_num += 1
