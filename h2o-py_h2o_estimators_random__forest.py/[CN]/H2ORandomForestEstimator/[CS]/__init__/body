def __init__(self, model_id=None, mtries=None, sample_rate=None, build_tree_one_node=None, ntrees=None, max_depth=None, min_rows=None, nbins=None, nbins_cats=None, binomial_double_trees=None, balance_classes=None, max_after_balance_size=None, seed=None, nfolds=None, fold_assignment=None, stopping_rounds=None, stopping_metric=None, stopping_tolerance=None, score_each_iteration=None, keep_cross_validation_predictions=None, checkpoint=None):
    'Builds a Random Forest Model on an H2OFrame\n\n    Parameters\n    ----------\n    model_id : str, optional\n      The unique id assigned to the resulting model. If none is given, an id will\n      automatically be generated.\n    mtries : int\n      Number of variables randomly sampled as candidates at each split. If set to -1,\n      defaults to sqrt{p} for classification, and p/3 for regression, where p is the\n      number of predictors.\n    sample_rate : float\n      Sample rate, from 0 to 1.0.\n    build_tree_one_node : bool\n      Run on one node only; no network overhead but fewer CPUs used.\n      Suitable for small datasets.\n    ntrees : int\n      A non-negative integer that determines the number of trees to grow.\n    max_depth : int\n      Maximum depth to grow the tree.\n    min_rows : int\n      Minimum number of rows to assign to terminal nodes.\n    nbins : int\n      For numerical columns (real/int), build a histogram of (at least) this many bins,\n      then split at the best point.\n    nbins_top_level : int\n      For numerical columns (real/int), build a histogram of (at most) this many bins at\n      the root level, then decrease by factor of two per level.\n    nbins_cats : int\n      For categorical columns (factors), build a histogram of this many bins, then split\n      at the best point. Higher values can lead to more overfitting.\n    binomial_double_trees : bool\n      or binary classification: Build 2x as many trees (one per class) - can lead to\n      higher accuracy.\n    balance_classes : bool\n      logical, indicates whether or not to balance training data class counts via\n      over/under-sampling (for imbalanced data)\n    max_after_balance_size : float\n      Maximum relative size of the training data after balancing class counts\n      (can be less than 1.0). Ignored if balance_classes is False,\n      which is the default behavior.\n    seed : int\n      Seed for random numbers (affects sampling) - Note: only reproducible when\n      running single threaded\n    nfolds : int, optional\n      Number of folds for cross-validation. If nfolds >= 2, then validation must\n      remain empty.\n    fold_assignment : str\n      Cross-validation fold assignment scheme, if fold_column is not specified\n      Must be "AUTO", "Random" or "Modulo"\n    keep_cross_validation_predictions : bool\n      Whether to keep the predictions of the cross-validation models\n    score_each_iteration : bool\n      Attempts to score each tree.\n    stopping_rounds : int\n      Early stopping based on convergence of stopping_metric.\n      Stop if simple moving average of length k of the stopping_metric does not improve\n      (by stopping_tolerance) for k=stopping_rounds scoring events.\n      Can only trigger after at least 2k scoring events. Use 0 to disable.\n    stopping_metric : str\n      Metric to use for convergence checking, only for _stopping_rounds > 0\n      Can be one of "AUTO", "deviance", "logloss", "MSE", "AUC", "r2", "misclassification".\n    stopping_tolerance : float\n      Relative tolerance for metric-based stopping criterion (stop if relative improvement is not at least this much)\n    '
    super(H2ORandomForestEstimator, self).__init__()
    self._parms = locals()
    self._parms = {k: v for (k, v) in self._parms.iteritems() if (k != 'self')}
