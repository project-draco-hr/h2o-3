def test6_enum_missing_values(self):
    '\n        Test parameter missing_values_handling="MeanImputation" with\n        mixed predictors (categorical/real value columns).  We first generate a data set that\n        contains a random number of columns of categorical and real value columns.  Next, we\n        encode the categorical columns.  Then, we generate the random data set using the formula\n        Y = W^T * X+ E as before.  Next, we go into the predictor matrix and randomly\n        decide to change a value to be nan and create missing values.  Since no regularization\n        is enabled in this case, we are able to calculate a theoretical weight/p-values/MSEs\n        where we can compare our H2O models with.\n        '
    print('*******************************************************************************************')
    print('Test6: test the GLM with enum/real values.')
    try:
        (weight_theory, p_values_theory, mse_train_theory, mse_test_theory) = self.theoretical_glm(self.training_data_file_enum_nans, self.test_data_file_enum_nans, True, False)
    except:
        print('Bad data set.  Problem with lin-alg.')
        sys.exit(0)
    training_data = h2o.import_file(pyunit_utils.locate(self.training_data_file_enum_nans))
    test_data = h2o.import_file(pyunit_utils.locate(self.test_data_file_enum_nans))
    for ind in range(self.enum_col):
        training_data[ind] = training_data[ind].round().asfactor()
        test_data[ind] = test_data[ind].round().asfactor()
    num_col = training_data.ncol
    y_index = (num_col - 1)
    x_indices = list(range(y_index))
    model_h2o = H2OGeneralizedLinearEstimator(family=self.family, Lambda=0, compute_p_values=True, missing_values_handling='MeanImputation')
    model_h2o.train(x=x_indices, y=y_index, training_frame=training_data)
    h2o_model_test_metrics = model_h2o.model_performance(test_data=test_data)
    num_test_failed = self.test_failed
    (_, _, _, _, _, _, self.test_failed) = pyunit_utils.extract_comparison_attributes_and_print(model_h2o, h2o_model_test_metrics, '\nTest6 Done!', True, False, False, weight_theory, p_values_theory, mse_train_theory, mse_test_theory, 'Comparing intercept and weights with enum and missing values....', 'H2O enum missing values no regularization intercept and weights: ', 'Theoretical intercept and weights: ', 'Intercept and weights are not equal!', 'Intercept and weights are close enough!', 'Comparing p-values ....', 'H2O enum missing values no regularization p-values: ', 'Theoretical p-values: ', 'P-values are not equal!', 'P-values are close enough!', 'Comparing training MSEs ....', 'H2O enum missing values no regularization training MSE: ', 'Theoretical training MSE: ', 'Training MSEs are not equal!', 'Training MSEs are close enough!', 'Comparing test MSEs ....', 'H2O enum missing values no regularization test MSE: ', 'Theoretical test MSE: ', 'Test MSEs are not equal!', 'Test MSEs are close enough!', self.test_failed, self.ignored_eps, self.allowed_diff, self.noise_var, False, attr3_bool=False)
    self.test_failed_array[self.test_num] += pyunit_utils.show_test_results('test6_enum_missing_values', num_test_failed, self.test_failed)
    self.test_num += 1
