{
  _classification=nClasses > 1;
  _train=train;
  _valid=valid;
  data_info=dinfo;
  parameters=(DeepLearningParameters)params.clone();
  DeepLearningParameters.Sanity.modifyParms(parameters,parameters,nClasses);
  final int num_input=dinfo.fullN();
  final int num_output=get_params()._autoencoder ? num_input : (_classification ? train.lastVec().cardinality() : 1);
  if (!get_params()._autoencoder)   assert(num_output == nClasses);
  _saw_missing_cats=dinfo._cats > 0 ? new boolean[data_info._cats] : null;
  assert(num_input > 0);
  assert(num_output > 0);
  if (has_momenta() && adaDelta())   throw new IllegalArgumentException("Cannot have non-zero momentum and adaptive rate at the same time.");
  final int layers=get_params()._hidden.length;
  units=new int[layers + 2];
  if (get_params()._max_categorical_features <= Integer.MAX_VALUE - dinfo._nums)   units[0]=Math.min(dinfo._nums + get_params()._max_categorical_features,num_input);
 else   units[0]=num_input;
  System.arraycopy(get_params()._hidden,0,units,1,layers);
  units[layers + 1]=num_output;
  boolean printLevels=units[0] > 1000L;
  boolean warn=units[0] > 100000L;
  if (printLevels) {
    final String[][] domains=dinfo._adaptedFrame.domains();
    int[] levels=new int[domains.length];
    for (int i=0; i < levels.length; ++i) {
      levels[i]=domains[i] != null ? domains[i].length : 0;
    }
    Arrays.sort(levels);
    if (warn) {
      Log.warn("===================================================================================================================================");
      Log.warn(num_input + " input features" + (dinfo._cats > 0 ? " (after categorical one-hot encoding)" : "")+ ". Can be slow and require a lot of memory.");
    }
    if (levels[levels.length - 1] > 0) {
      int levelcutoff=levels[levels.length - 1 - Math.min(10,levels.length - 1)];
      int count=0;
      for (int i=0; i < dinfo._adaptedFrame.numCols() - (get_params()._autoencoder ? 0 : 1) && count < 10; ++i) {
        if (dinfo._adaptedFrame.domains()[i] != null && dinfo._adaptedFrame.domains()[i].length >= levelcutoff) {
          if (warn) {
            Log.warn("Categorical feature '" + dinfo._adaptedFrame._names[i] + "' has cardinality "+ dinfo._adaptedFrame.domains()[i].length+ ".");
          }
 else {
            Log.info("Categorical feature '" + dinfo._adaptedFrame._names[i] + "' has cardinality "+ dinfo._adaptedFrame.domains()[i].length+ ".");
          }
        }
        count++;
      }
    }
    if (warn) {
      Log.warn("Suggestions:");
      Log.warn(" *) Limit the size of the first hidden layer");
      if (dinfo._cats > 0) {
        Log.warn(" *) Limit the total number of one-hot encoded features with the parameter 'max_categorical_features'");
        Log.warn(" *) Run h2o.interaction(...,pairwise=F) on high-cardinality categorical columns to limit the factor count, see http://learn.h2o.ai");
      }
      Log.warn("===================================================================================================================================");
    }
  }
  dense_row_weights=new Storage.DenseRowMatrix[layers + 1];
  dense_row_weights[0]=new Storage.DenseRowMatrix(units[1],units[0]);
  for (int i=1; i <= layers; ++i)   dense_row_weights[i]=new Storage.DenseRowMatrix(units[i + 1],units[i]);
  biases=new Storage.DenseVector[layers + 1];
  for (int i=0; i <= layers; ++i)   biases[i]=new Storage.DenseVector(units[i + 1]);
  if (get_params()._autoencoder && get_params()._sparsity_beta > 0) {
    avg_activations=new Storage.DenseVector[layers];
    mean_a=new double[layers];
    for (int i=0; i < layers; ++i)     avg_activations[i]=new Storage.DenseVector(units[i + 1]);
  }
  allocateHelperArrays();
  mean_rate=new double[units.length - 1];
  rms_rate=new double[units.length - 1];
  mean_bias=new double[units.length - 1];
  rms_bias=new double[units.length - 1];
  mean_weight=new double[units.length - 1];
  rms_weight=new double[units.length - 1];
}
