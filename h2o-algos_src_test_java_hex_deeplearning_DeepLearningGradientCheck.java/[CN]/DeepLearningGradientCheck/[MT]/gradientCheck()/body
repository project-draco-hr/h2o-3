{
  Frame tfr=null;
  DeepLearningModel dl=null;
  try {
    tfr=parse_test_file("smalldata/glm_test/cancar_logIn.csv");
    for (    String s : new String[]{"Merit","Class"}) {
      Vec f=tfr.vec(s).toCategoricalVec();
      tfr.remove(s).remove();
      tfr.add(s,f);
    }
    DKV.put(tfr);
    tfr.add("Binary",tfr.anyVec().makeZero());
    new MRTask(){
      public void map(      Chunk[] c){
        for (int i=0; i < c[0]._len; ++i)         if (c[0].at8(i) == 1)         c[1].set(i,1);
      }
    }
.doAll(tfr.vecs(new String[]{"Class","Binary"}));
    Vec cv=tfr.vec("Binary").toCategoricalVec();
    tfr.remove("Binary").remove();
    tfr.add("Binary",cv);
    DKV.put(tfr);
    Random rng=new Random(0xDECAF);
    int count=0;
    int failedcount=0;
    double maxRelErr=0;
    double meanRelErr=0;
    for (    Distribution.Family dist : new Distribution.Family[]{Distribution.Family.gaussian,Distribution.Family.laplace,Distribution.Family.quantile,Distribution.Family.huber,Distribution.Family.modified_huber,Distribution.Family.gamma,Distribution.Family.poisson,Distribution.Family.AUTO,Distribution.Family.tweedie,Distribution.Family.multinomial,Distribution.Family.bernoulli}) {
      for (      DeepLearningParameters.Activation act : new DeepLearningParameters.Activation[]{DeepLearningParameters.Activation.Tanh,DeepLearningParameters.Activation.Rectifier}) {
        for (        String response : new String[]{"Binary","Class","Cost"}) {
          for (          boolean adaptive : new boolean[]{true,false}) {
            for (            int miniBatchSize : new int[]{1}) {
              if (response.equals("Class")) {
                if (dist != Distribution.Family.multinomial && dist != Distribution.Family.AUTO)                 continue;
              }
 else               if (response.equals("Binary")) {
                if (dist != Distribution.Family.modified_huber && dist != Distribution.Family.bernoulli && dist != Distribution.Family.AUTO)                 continue;
              }
 else {
                if (dist == Distribution.Family.multinomial || dist == Distribution.Family.modified_huber || dist == Distribution.Family.bernoulli)                 continue;
              }
              DeepLearningParameters parms=new DeepLearningParameters();
              parms._huber_alpha=rng.nextDouble() + 0.1;
              parms._tweedie_power=1.01 + rng.nextDouble() * 0.9;
              parms._quantile_alpha=0.05 + rng.nextDouble() * 0.9;
              parms._train=tfr._key;
              parms._epochs=100;
              parms._l1=1e-3;
              parms._l2=1e-3;
              parms._force_load_balance=false;
              parms._hidden=new int[]{10,10,10};
              parms._fast_mode=false;
              parms._response_column=response;
              parms._distribution=dist;
              parms._max_w2=10;
              parms._seed=0xaaabbb;
              parms._activation=act;
              parms._adaptive_rate=adaptive;
              parms._rate=1e-4;
              parms._momentum_start=0.9;
              parms._momentum_stable=0.99;
              parms._mini_batch_size=miniBatchSize;
              DeepLearningModelInfo.gradientCheck=new DeepLearningModelInfo.GradientCheck(0,0,0);
              DeepLearning job=new DeepLearning(parms);
              try {
                dl=job.trainModel().get();
                boolean classification=response.equals("Class") || response.equals("Binary");
                if (!classification) {
                  Frame p=dl.score(tfr);
                  hex.ModelMetrics mm=hex.ModelMetrics.getFromDKV(dl,tfr);
                  double resdev=((ModelMetricsRegression)mm)._mean_residual_deviance;
                  Log.info("Mean residual deviance: " + resdev);
                  p.delete();
                }
                DeepLearningModelInfo modelInfo=dl.model_info().deep_clone();
                long before=dl.model_info().checksum_impl();
                float meanLoss=0;
                for (int rId=0; rId < tfr.numRows(); rId+=1) {
                  dl.set_model_info(modelInfo.deep_clone());
                  final DataInfo di=dl.model_info().data_info();
                  final DataInfo.Row[] rowsMiniBatch=new DataInfo.Row[miniBatchSize];
                  for (int i=0; i < rowsMiniBatch.length; ++i) {
                    if (0 <= rId + i && rId + i < tfr.numRows()) {
                      rowsMiniBatch[i]=new FrameTask.ExtractDenseRow(di,rId + i).doAll(di._adaptedFrame)._row;
                    }
                  }
                  long cs=dl.model_info().checksum_impl();
                  double loss=dl.meanLoss(rowsMiniBatch);
                  assert(cs == before);
                  assert(before == dl.model_info().checksum_impl());
                  meanLoss+=loss;
                  for (int layer=0; layer <= parms._hidden.length; ++layer) {
                    int rows=dl.model_info().get_weights(layer).rows();
                    assert(dl.model_info().get_biases(layer).size() == rows);
                    for (int row=0; row < rows; ++row) {
                      if (true) {
                        dl.set_model_info(modelInfo.deep_clone());
                        Neurons[] neurons=DeepLearningTask.makeNeuronsForTraining(dl.model_info());
                        double[] responses=new double[miniBatchSize];
                        double[] offsets=new double[miniBatchSize];
                        int n=0;
                        for (                        DataInfo.Row myRow : rowsMiniBatch) {
                          if (myRow == null)                           continue;
                          ((Neurons.Input)neurons[0]).setInput(-1,myRow.numIds,myRow.numVals,myRow.nBins,myRow.binIds,n);
                          responses[n]=myRow.response(0);
                          offsets[n]=myRow.offset;
                          n++;
                        }
                        DeepLearningTask.fpropMiniBatch(-1,neurons,dl.model_info(),null,true,responses,offsets,n);
                        long after=dl.model_info().checksum_impl();
                        assert(after == before);
                        DeepLearningModelInfo.gradientCheck=new DeepLearningModelInfo.GradientCheck(layer,row,-1);
                        DeepLearningTask.bpropMiniBatch(neurons,n);
                        assert(before != dl.model_info().checksum_impl());
                        dl.set_model_info(modelInfo.deep_clone());
                        assert(before == dl.model_info().checksum_impl());
                        double bpropGradient=DeepLearningModelInfo.gradientCheck.gradient;
                        final double bias=dl.model_info().get_biases(layer).get(row);
                        double eps=1e-4 * Math.abs(bias);
                        if (eps == 0)                         eps=1e-6;
                        dl.model_info().get_biases(layer).set(row,bias + eps);
                        double up=dl.meanLoss(rowsMiniBatch);
                        dl.model_info().get_biases(layer).set(row,bias - eps);
                        double down=dl.meanLoss(rowsMiniBatch);
                        if (Math.abs(up - down) / Math.abs(up + down) < 1e-8) {
                          continue;
                        }
                        double gradient=((up - down) / (2. * eps));
                        double relError=2 * Math.abs(bpropGradient - gradient) / (Math.abs(gradient) + Math.abs(bpropGradient));
                        count++;
                        if (Math.abs(gradient) < 1e-7 || Math.abs(bpropGradient) < 1e-7) {
                          if (Math.abs(bpropGradient - gradient) < 1e-7)                           continue;
                        }
                        meanRelErr+=relError;
                        if (relError > MAX_TOLERANCE) {
                          Log.info("\nDistribution: " + dl._parms._distribution);
                          Log.info("\nRow: " + rId);
                          Log.info("bias (layer " + layer + ", row "+ row+ "): "+ bias+ " +/- "+ eps);
                          Log.info("loss: " + loss);
                          Log.info("losses up/down: " + up + " / "+ down);
                          Log.info("=> Finite differences gradient: " + gradient);
                          Log.info("=> Back-propagation gradient  : " + bpropGradient);
                          Log.info("=> Relative error             : " + PrettyPrint.formatPct(relError));
                          failedcount++;
                        }
                      }
                      int cols=dl.model_info().get_weights(layer).cols();
                      for (int col=0; col < cols; ++col) {
                        if (rng.nextFloat() >= SAMPLE_RATE)                         continue;
                        dl.set_model_info(modelInfo.deep_clone());
                        Neurons[] neurons=DeepLearningTask.makeNeuronsForTraining(dl.model_info());
                        double[] responses=new double[miniBatchSize];
                        double[] offsets=new double[miniBatchSize];
                        int n=0;
                        for (                        DataInfo.Row myRow : rowsMiniBatch) {
                          if (myRow == null)                           continue;
                          ((Neurons.Input)neurons[0]).setInput(-1,myRow.numIds,myRow.numVals,myRow.nBins,myRow.binIds,n);
                          responses[n]=myRow.response(0);
                          offsets[n]=myRow.offset;
                          n++;
                        }
                        DeepLearningTask.fpropMiniBatch(-1,neurons,dl.model_info(),null,true,responses,offsets,n);
                        long after=dl.model_info().checksum_impl();
                        assert(after == before);
                        DeepLearningModelInfo.gradientCheck=new DeepLearningModelInfo.GradientCheck(layer,row,col);
                        DeepLearningTask.bpropMiniBatch(neurons,n);
                        assert(before != dl.model_info().checksum_impl());
                        dl.set_model_info(modelInfo.deep_clone());
                        assert(before == dl.model_info().checksum_impl());
                        double bpropGradient=DeepLearningModelInfo.gradientCheck.gradient;
                        final float weight=dl.model_info().get_weights(layer).get(row,col);
                        double eps=1e-4 * Math.abs(weight);
                        if (eps == 0)                         eps=1e-6;
                        dl.model_info().get_weights(layer).set(row,col,(float)(weight + eps));
                        double up=dl.meanLoss(rowsMiniBatch);
                        dl.model_info().get_weights(layer).set(row,col,(float)(weight - eps));
                        double down=dl.meanLoss(rowsMiniBatch);
                        if (Math.abs(up - down) / Math.abs(up + down) < 1e-8) {
                          continue;
                        }
                        double gradient=((up - down) / (2. * eps));
                        double relError=2 * Math.abs(bpropGradient - gradient) / (Math.abs(gradient) + Math.abs(bpropGradient));
                        count++;
                        if (Math.abs(gradient) < 1e-7 || Math.abs(bpropGradient) < 1e-7) {
                          if (Math.abs(bpropGradient - gradient) < 1e-7)                           continue;
                        }
                        meanRelErr+=relError;
                        if (relError > MAX_TOLERANCE) {
                          Log.info("\nDistribution: " + dl._parms._distribution);
                          Log.info("\nRow: " + rId);
                          Log.info("weight (layer " + layer + ", row "+ row+ ", col "+ col+ "): "+ weight+ " +/- "+ eps);
                          Log.info("loss: " + loss);
                          Log.info("losses up/down: " + up + " / "+ down);
                          Log.info("=> Finite differences gradient: " + gradient);
                          Log.info("=> Back-propagation gradient  : " + bpropGradient);
                          Log.info("=> Relative error             : " + PrettyPrint.formatPct(relError));
                          failedcount++;
                        }
                        maxRelErr=Math.max(maxRelErr,relError);
                        assert(!Double.isNaN(maxRelErr));
                      }
                    }
                  }
                }
                meanLoss/=tfr.numRows();
                Log.info("Mean loss: " + meanLoss);
              }
 catch (              RuntimeException ex) {
                dl=DKV.getGet(job.dest());
                if (dl != null)                 Assert.assertTrue(dl.model_info().isUnstable());
 else                 Assert.assertTrue(job.isStopped());
              }
 finally {
                if (dl != null)                 dl.delete();
              }
            }
          }
        }
      }
    }
    Log.info("Number of tests: " + count);
    Log.info("Number of failed tests: " + failedcount);
    Log.info("Mean. relative error: " + meanRelErr / count);
    Log.info("Max. relative error: " + PrettyPrint.formatPct(maxRelErr));
    Assert.assertTrue("Error too large: " + maxRelErr + " >= "+ MAX_TOLERANCE,maxRelErr < MAX_TOLERANCE);
    Assert.assertTrue("Failed count too large: " + failedcount + " > "+ MAX_FAILED_COUNT,failedcount <= MAX_FAILED_COUNT);
  }
  finally {
    if (tfr != null)     tfr.remove();
  }
}
