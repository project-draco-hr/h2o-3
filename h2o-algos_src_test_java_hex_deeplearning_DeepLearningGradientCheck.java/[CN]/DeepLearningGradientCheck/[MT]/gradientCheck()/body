{
  Frame tfr=null;
  DeepLearningModel dl=null;
  try {
    tfr=parse_test_file("smalldata/glm_test/cancar_logIn.csv");
    for (    String s : new String[]{"Merit","Class"}) {
      Vec f=tfr.vec(s).toCategoricalVec();
      tfr.remove(s).remove();
      tfr.add(s,f);
    }
    DKV.put(tfr);
    Random rng=new Random(0xDECAF);
    int count=0;
    int failedcount=0;
    double maxRelErr=0;
    double meanRelErr=0;
    for (    Distribution.Family dist : new Distribution.Family[]{Distribution.Family.gaussian,Distribution.Family.laplace,Distribution.Family.huber,Distribution.Family.gamma,Distribution.Family.poisson,Distribution.Family.tweedie,Distribution.Family.multinomial}) {
      for (      DeepLearningParameters.Activation act : new DeepLearningParameters.Activation[]{DeepLearningParameters.Activation.Tanh,DeepLearningParameters.Activation.Rectifier}) {
        for (        String response : new String[]{"Class","Cost"}) {
          for (          boolean adaptive : new boolean[]{true,false}) {
            for (            int miniBatchSize : new int[]{1}) {
              boolean classification=response.equals("Class");
              if (classification && dist != Distribution.Family.multinomial)               continue;
              if (!classification && dist == Distribution.Family.multinomial)               continue;
              DeepLearningParameters parms=new DeepLearningParameters();
              parms._train=tfr._key;
              parms._epochs=100;
              parms._force_load_balance=false;
              parms._hidden=new int[]{10,10,10};
              parms._fast_mode=false;
              parms._response_column=response;
              parms._distribution=dist;
              parms._max_w2=10;
              parms._seed=0xaaabbb;
              parms._activation=act;
              parms._adaptive_rate=adaptive;
              parms._rate=1e-4;
              parms._momentum_start=0.9;
              parms._momentum_stable=0.99;
              DeepLearningModelInfo.gradientCheck=null;
              DeepLearning job=new DeepLearning(parms);
              try {
                dl=job.trainModel().get();
                if (!classification) {
                  Frame p=dl.score(tfr);
                  hex.ModelMetrics mm=hex.ModelMetrics.getFromDKV(dl,tfr);
                  double resdev=((ModelMetricsRegression)mm)._mean_residual_deviance;
                  Log.info("Mean residual deviance: " + resdev);
                  p.delete();
                }
                DeepLearningModelInfo modelInfo=dl.model_info().deep_clone();
                long before=dl.model_info().checksum_impl();
                float meanLoss=0;
                for (int rId=0; rId < tfr.numRows(); rId+=1) {
                  dl.set_model_info(modelInfo.deep_clone());
                  final DataInfo di=dl.model_info().data_info();
                  final DataInfo.Row[] miniBatch=new DataInfo.Row[miniBatchSize];
                  for (int i=0; i < miniBatch.length; ++i) {
                    if (0 <= rId + i && rId + i < tfr.numRows()) {
                      miniBatch[i]=new FrameTask.ExtractDenseRow(di,rId + i).doAll(di._adaptedFrame)._row;
                    }
                  }
                  long cs=dl.model_info().checksum_impl();
                  double loss=dl.loss(miniBatch);
                  assert(cs == before);
                  assert(before == dl.model_info().checksum_impl());
                  meanLoss+=loss;
                  for (int layer=0; layer <= parms._hidden.length; ++layer) {
                    int rows=dl.model_info().get_weights(layer).rows();
                    for (int row=0; row < rows; ++row) {
                      int cols=dl.model_info().get_weights(layer).cols();
                      for (int col=0; col < cols; ++col) {
                        if (rng.nextFloat() >= SAMPLE_RATE)                         continue;
                        dl.set_model_info(modelInfo.deep_clone());
                        Neurons[] neurons=DeepLearningTask.makeNeuronsForTraining(dl.model_info());
                        for (                        DataInfo.Row myRow : miniBatch) {
                          if (myRow == null)                           continue;
                          ((Neurons.Input)neurons[0]).setInput(-1,myRow.numIds,myRow.numVals,myRow.nBins,myRow.binIds);
                          DeepLearningTask.step(-1,neurons,dl.model_info(),null,true,new double[]{myRow.response[0]},myRow.offset);
                        }
                        long after=dl.model_info().checksum_impl();
                        assert(after == before);
                        DeepLearningModelInfo.gradientCheck=new DeepLearningModelInfo.GradientCheck(layer,row,col);
                        DeepLearningTask.applyModelUpdates(neurons);
                        assert(before != dl.model_info().checksum_impl());
                        dl.set_model_info(modelInfo.deep_clone());
                        assert(before == dl.model_info().checksum_impl());
                        double bpropGradient=DeepLearningModelInfo.gradientCheck.gradient;
                        DeepLearningModelInfo.gradientCheck=null;
                        final float weight=dl.model_info().get_weights(layer).get(row,col);
                        double eps=1e-4 * Math.abs(weight);
                        if (eps == 0)                         eps=1e-6;
                        dl.model_info().get_weights(layer).set(row,col,(float)(weight + eps));
                        double up=dl.loss(miniBatch);
                        dl.model_info().get_weights(layer).set(row,col,(float)(weight - eps));
                        double down=dl.loss(miniBatch);
                        if (Math.abs(up - down) / Math.abs(up + down) < 1e-8) {
                          continue;
                        }
                        double gradient=((up - down) / (2. * eps));
                        double relError=2 * Math.abs(bpropGradient - gradient) / (Math.abs(gradient) + Math.abs(bpropGradient));
                        count++;
                        if (Math.abs(gradient) < 1e-8 || Math.abs(bpropGradient) < 1e-8) {
                          if (Math.abs(bpropGradient - gradient) < 1e-8)                           continue;
                        }
                        meanRelErr+=relError;
                        if (relError > MAX_TOLERANCE) {
                          Log.info("\nRow: " + rId);
                          Log.info("weight (layer " + layer + ", row "+ row+ ", col "+ col+ "): "+ weight+ " +/- "+ eps);
                          Log.info("loss: " + loss);
                          Log.info("losses up/down: " + up + " / "+ down);
                          Log.info("=> Finite differences gradient: " + gradient);
                          Log.info("=> Back-propagation gradient  : " + bpropGradient);
                          Log.info("=> Relative error             : " + PrettyPrint.formatPct(relError));
                          failedcount++;
                        }
                        maxRelErr=Math.max(maxRelErr,relError);
                        assert(!Double.isNaN(maxRelErr));
                      }
                    }
                  }
                }
                meanLoss/=tfr.numRows();
                Log.info("Mean loss: " + meanLoss);
              }
 catch (              RuntimeException ex) {
                dl=DKV.getGet(job.dest());
                if (dl != null)                 Assert.assertTrue(dl.model_info().isUnstable());
 else                 Assert.assertTrue(job.isStopped());
              }
 finally {
                if (dl != null)                 dl.delete();
              }
            }
          }
        }
      }
    }
    Log.info("Number of tests: " + count);
    Log.info("Number of failed tests: " + failedcount);
    Log.info("Mean. relative error: " + meanRelErr / count);
    Log.info("Max. relative error: " + PrettyPrint.formatPct(maxRelErr));
    Assert.assertTrue("Error too large: " + maxRelErr + " >= "+ MAX_TOLERANCE,maxRelErr < MAX_TOLERANCE);
  }
  finally {
    if (tfr != null)     tfr.remove();
  }
}
