{
  Frame tfr=null;
  DeepLearningModel dl=null;
  Scope.enter();
  try {
    tfr=parse_test_file("smalldata/glm_test/cancar_logIn.csv");
    for (    String s : new String[]{"Merit","Class"}) {
      Scope.track(tfr.replace(tfr.find(s),tfr.vec(s).toEnum())._key);
    }
    DKV.put(tfr);
    DeepLearningParameters parms=new DeepLearningParameters();
    parms._train=tfr._key;
    parms._epochs=100;
    parms._reproducible=true;
    parms._hidden=new int[]{20,20,20};
    parms._response_column="Cost";
    parms._seed=0xdecaf + 1;
    parms._activation=DeepLearningParameters.Activation.Tanh;
    DeepLearning job=new DeepLearning(parms);
    dl=job.trainModel().get();
    long seed=new Random(parms._seed).nextLong();
    final Random rng=new Random(seed);
    dl.score(tfr);
    hex.ModelMetrics mm=hex.ModelMetrics.getFromDKV(dl,tfr);
    double resdev=((ModelMetricsRegression)mm)._mean_residual_deviance;
    Log.info("Mean residual deviance: " + resdev);
    DeepLearningModelInfo modelInfo=dl.model_info().deep_clone();
    long before=dl.model_info().checksum_impl();
    float maxRelErr=0;
    float meanLoss=0;
    for (int loop=0; loop < tfr.numRows(); ++loop) {
      dl.set_model_info(modelInfo.deep_clone());
      final DataInfo di=dl.model_info().data_info();
      final int rId=loop;
      final DataInfo.Row myRow=new FrameTask.ExtractDenseRow(di,rId).doAll(di._adaptedFrame)._row;
      Neurons[] neurons=DeepLearningTask.makeNeuronsForTraining(dl.model_info());
      ((Neurons.Input)neurons[0]).setInput(-1,myRow.numVals,myRow.nBins,myRow.binIds);
      DeepLearningTask.step(-1,neurons,dl.model_info(),null,true,new double[]{myRow.response[0]},myRow.offset);
      long after=dl.model_info().checksum_impl();
      assert(after == before);
      int layer=1 + rng.nextInt(parms._hidden.length);
      int row=rng.nextInt(dl.model_info().get_weights(layer).rows());
      int col=rng.nextInt(dl.model_info().get_weights(layer).cols());
      DeepLearningModelInfo.gradientCheck=new DeepLearningModelInfo.GradientCheck(layer,row,col);
      DeepLearningTask.applyModelUpdates(neurons);
      assert(before != dl.model_info().checksum_impl());
      dl.set_model_info(modelInfo.deep_clone());
      assert(before == dl.model_info().checksum_impl());
      float bpropGradient=DeepLearningModelInfo.gradientCheck.gradient;
      final float weight=dl.model_info().get_weights(layer).get(row,col);
      float eps=1e-2f * weight;
      double loss=dl.loss(myRow);
      meanLoss+=loss;
      long cs=dl.model_info().checksum_impl();
      dl.model_info().get_weights(layer).set(row,col,weight + eps);
      assert(dl.model_info().get_weights(layer).get(row,col) != weight);
      assert(cs != dl.model_info().checksum_impl());
      double up=dl.loss(myRow);
      dl.model_info().get_weights(layer).set(row,col,weight - eps);
      assert(dl.model_info().get_weights(layer).get(row,col) != weight);
      assert(cs != dl.model_info().checksum_impl());
      double down=dl.loss(myRow);
      float gradient=(float)((up - down) / (2. * (double)eps));
      gradient/=2;
      if (gradient < 1e-5)       continue;
      float relError=(Math.abs(bpropGradient - gradient) / Math.abs(gradient));
      if (false) {
        Log.info("\nRow: " + (loop + 1));
        Log.info("loss: " + loss);
        Log.info("weight (layer " + layer + ", row "+ row+ ", col "+ col+ "): "+ weight+ " +/- "+ eps);
        Log.info("losses up/down: " + up + " / "+ down);
        Log.info("=> Finite differences gradient: " + gradient);
        Log.info("=> Back-propagation gradient  : " + bpropGradient);
        Log.info("=> Relative error             : " + PrettyPrint.formatPct(relError));
      }
      maxRelErr=Math.max(maxRelErr,relError);
    }
    meanLoss/=tfr.numRows();
    Log.info("Mean loss: " + meanLoss);
    Log.info("Max. relative error: " + PrettyPrint.formatPct(maxRelErr));
    float tol=1e-2f;
    Assert.assertTrue("Error too large: " + maxRelErr + " >= "+ tol,maxRelErr < tol);
    job.remove();
  }
  finally {
    if (tfr != null)     tfr.remove();
    if (dl != null)     dl.delete();
    Scope.exit();
  }
}
