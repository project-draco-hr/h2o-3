'\nThe H2O Python Module\n=====================\n\nThis module provides access to the H2O JVM, as well as its extensions, objects, \nmachine-learning algorithms, and modeling support capabilities, such as basic \nmunging and feature generation.\n\nThe H2O JVM uses a web server so that all communication occurs on a socket (specified\nby an IP address and a port) via a series of REST calls (see connection.py for the REST\nlayer implementation and details). There is a single active connection to the H2O JVM at\nany time, and this handle is stashed out of sight in a singleton instance of\n:class:`H2OConnection` (this is the global  :envvar:`__H2OConn__`). In other words,\nthis package does not rely on Jython, and there is no direct manipulation of the JVM.\n\nThe H2O python module is not intended as a replacement for other popular machine learning\nframeworks such as scikit-learn, pylearn2, and their ilk, but is intended to bring H2O to\na wider audience of data and machine learning devotees who work exclusively with Python.\n\nH2O from Python is a tool for rapidly turning over models, doing data munging, and\nbuilding applications in a fast, scalable environment without any of the mental anguish\nabout parallelism and distribution of work.\n\nWhat is H2O?\n------------\n\nH2O is a Java-based software for data modeling and general computing. There are many\ndifferent perceptions of the H2O software, but the primary purpose of H2O is as a \ndistributed (many machines), parallel (many CPUs), in memory (several hundred GBs Xmx) \nprocessing engine.\n\nThere are two levels of parallelism:\n\n    * within node\n    * across (or between) nodes\n\nThe goal, remember, is to easily add more processors to a given problem in order to\nproduce a solution faster. The conceptual paradigm MapReduce (also known as\n"divide and conquer and combine"), along with a good concurrent application structure,\n(c.f. jsr166y and NonBlockingHashMap) enable this type of scaling in H2O -- we\'re really\ncooking with gas now!\n\nFor application developers and data scientists, the gritty details of thread-safety,\nalgorithm parallelism, and node coherence on a network are concealed by simple-to-use REST\ncalls that are all documented here. In addition, H2O is an open-source project under the\nApache v2 licence. All of the source code is on\n`github <https://github.com/h2oai/h2o-dev>`_, there is an active\n`google group mailing list <https://groups.google.com/forum/#!forum/h2ostream>`_, our\n`nightly tests <http://test.0xdata.com/>`_ are open for perusal, and our `JIRA ticketing\nsystem <http://jira.0xdata.com>`_ is also open for public use. Last, but not least, we\nregularly engage the machine learning community all over the nation with a very busy\n`meetup schedule <http://h2o.ai/events/>`_ (so if you\'re not in The Valley, no sweat,\nwe\'re probably coming to your area soon!), and finally, we host our very own `H2O World\nconference <http://h2o.ai/h2o-world/>`_. We also sometimes host hack-a-thons at our\ncampus in Mountain View, CA. Needless to say, H2O provides a lot of support for \napplication developers.\n\nIn order to make the most out of H2O, there are some key conceptual pieces that are important\nto know before getting started. Mainly, it\'s helpful to know about the different types of\nobjects that live in H2O and what the rules of engagement are in the context of the REST\nAPI (which is what any non-JVM interface is all about).\n\nLet\'s get started!\n\nThe H2O Object System\n+++++++++++++++++++++\n\nH2O uses a distributed key-value store (the "DKV") that contains pointers to the\nvarious objects of the H2O ecosystem. The DKV is a kind of biosphere in that it\nencapsulates all shared objects; however, it may not encapsulate all objects. Some shared\nobjects are mutable by the client; some shared objects are read-only by the client, but are\nmutable by H2O (e.g. a model being constructed will change over time); and actions by the\nclient may have side-effects on other clients (multi-tenancy is not a supported model of\nuse, but it is possible for multiple clients to attach to a single H2O cloud).\n\nBriefly, these objects are:\n\n     * :mod:`Key`:    A key is an entry in the DKV that maps to an object in H2O.\n\n     * :mod:`Frame`:  A Frame is a collection of Vec objects. It is a 2D array of elements.\n\n     * :mod:`Vec`:    A Vec is a collection of Chunk objects. It is a 1D array of elements.\n\n     * :mod:`Chunk`:  A Chunk holds a fraction of the BigData. It is a 1D array of elements.\n\n     * :mod:`ModelMetrics`:   A collection of metrics for a given category of model.\n\n     * :mod:`Model`:  A model is an immutable object having `predict` and `metrics` methods.\n\n     * :mod:`Job`:    A Job is a non-blocking task that performs a finite amount of work.\n\nMany of these objects have no meaning to a Python end-user, but to make sense of\nthe objects available in this module it is helpful to understand how these objects map to\nobjects in the JVM. After all, this module is an interface that allows the\nmanipulation of a distributed system.\n\n\nObjects In This Module\n----------------------\n\nThe objects that are of primary concern to the python user are (in order of importance)\n- IDs/Keys\n- Frames\n- Models\n- ModelMetrics\n- Jobs (to a lesser extent)\nEach of these objects are described in greater detail in this documentation,\nbut a few brief notes are provided here.\n\n\nH2OFrame\n++++++++\n\nAn H2OFrame is a 2D array of uniformly-typed columns. Data in H2O is compressed (often\nachieving 2-4x better compression than gzip on disk) and is held in the JVM heap (i.e.\ndata is "in memory"), and *not* in the python process local memory. The H2OFrame is an\niterable (supporting list comprehensions). All an H2OFrame object is, therefore, is a\nwrapper on a list that supports various types of operations that may or may not be lazy.\nHere\'s an example showing how a list comprehension is combined with lazy expressions to\ncompute the column means for all columns in the H2OFrame::\n\n  >>> df = h2o.import_file(path="smalldata/logreg/prostate.csv")  # import prostate data\n  >>>\n  >>> colmeans = [v.mean() for v in df]                            # compute column means\n  >>>\n  >>> colmeans                                                     # print the results\n  [5.843333333333335, 3.0540000000000007, 3.7586666666666693, 1.1986666666666672]\n\nLazy expressions will be discussed briefly in the coming sections, as they are not\nnecessarily going to be integral to the practicing data scientist. However, their primary\npurpose is to cut down on the chatter between the client (a.k.a the python interface) and\nH2O. Lazy expressions are `Katamari\'d <http://www.urbandictionary.com/define.php?term=Katamari>`_ \ntogether and only ever evaluated when some piece of output is requested (e.g. print-to-screen).\n\nThe set of operations on an H2OFrame is described in a dedicated chapter, but\nin general, this set of operations closely resembles those that may be\nperformed on an R data.frame. This includes all types of slicing (with complex\nconditionals), broadcasting operations, and a slew of math operations for transforming and\nmutating a Frame -- all the while the actual Big Data is sitting in the H2O cloud. The semantics \nfor modifying a Frame closely resemble R\'s copy-on-modify semantics, except when it comes\nto mutating a Frame in place. For example, it\'s possible to assign all occurrences of the\nnumber `0` in a column to missing (or `NA` in R parlance) as demonstrated in the following\nsnippet::\n\n\n  >>> df = h2o.import_file(path="smalldata/logreg/prostate.csv")  # import prostate data\n  >>>\n  >>> vol = df[\'VOL\']                                              # select the VOL column\n  >>>\n  >>> vol[vol == 0] = None                                         # 0 VOL means \'missing\'\n\nAfter this operation, `vol` has been permanently mutated in place (it is not a copy!).\n\nExprNode\n++++++++\nIn the guts of this module is the Expr class, which defines objects holding\nthe cumulative, unevaluated expressions that may become H2OFrame objects.\nFor example:\n\n  >>> fr = h2o.import_file(path="smalldata/logreg/prostate.csv")  # import prostate data\n  >>>\n  >>> a = fr + 3.14159                                             # "a" is now an Expr\n  >>>\n  >>> type(a)                                                      # <class \'h2o.expr.Expr\'>\n\nThese objects are not as important to distinguish at the user level, and all operations\ncan be performed with the mental model of operating on 2D frames (i.e. everything is an\nH2OFrame). \n\nIn the previous snippet, `a` has not yet triggered any big data evaluation and is, in\nfact, a pending computation. Once `a` is evaluated, it stays evaluated. Additionally,\nall dependent subparts composing `a` are also evaluated.\n\nThis module relies on reference counting of python objects to dispose of \nout-of-scope objects. The Expr class destroys objects and their big data \ncounterparts in the H2O cloud using a remove call:\n\n  >>> fr = h2o.import_file(path="smalldata/logreg/prostate.csv")  # import prostate data\n  >>>\n  >>> h2o.remove(fr)                                               # remove prostate data\n  >>> fr                                                           # attempting to use fr results in a ValueError\n\nNotice that attempting to use the object after a remove call has been issued will\nresult in a ValueError. Therefore, any working references may not be cleaned up,\nbut they will no longer be functional. Deleting an unevaluated expression will not\ndelete all subparts.\n\nModels\n++++++\n\nThe model-building experience with this module is unique, especially for those coming \nfrom a background in scikit-learn. Instead of using objects to build the model, \nbuilder functions are provided in the top-level module, and the result of a call\nis a model object belonging to one of the following categories:\n\n    * Regression\n    * Binomial\n    * Multinomial\n    * Clustering\n    * Autoencoder\n\nTo better demonstrate this concept, refer to the following example:\n\n  >>> fr = h2o.import_file(path="smalldata/logreg/prostate.csv")  # import prostate data\n  >>>\n  >>> fr[1] = fr[1].asfactor()                                     # make 2nd column a factor\n  >>>\n  >>> m = h2o.glm(x=fr[3:], y=fr[2])                               # build a glm with a method call\n  >>>\n  >>> m.__class__                                                  # <h2o.model.binomial.H2OBinomialModel object at 0x104659cd0>\n  >>>\n  >>> m.show()                                                     # print the model details\n  >>>\n  >>> m.summary()                                                  # print a model summary\n\nAs you can see in the example, the result of the GLM call is a binomial model. This example also showcases\nan important feature-munging step needed for GLM to perform a classification task rather than a \nregression task. Namely, the second column is initially read as a numeric column,\nbut it must be changed to a factor by way of the operation `asfactor`. Let\'s take a look\nat this more deeply:\n\n  >>> fr = h2o.import_file(path="smalldata/logreg/prostate.csv")  # import prostate data\n  >>>\n  >>> fr[1].isfactor()                                             # produces False\n  >>>\n  >>> m = h2o.gbm(x=fr[2:],y=fr[1])                                # build the gbm\n  >>>\n  >>> m.__class__                                                  # <h2o.model.regression.H2ORegressionModel object at 0x104d07590>\n  >>>\n  >>> fr[1] = fr[1].asfactor()                                     # cast the 2nd column to a factor column\n  >>>\n  >>> fr[1].isfactor()                                             # produces True\n  >>>\n  >>> m = h2o.gbm(x=fr[2:],y=fr[1])                                # build the gbm\n  >>>\n  >>> m.__class__                                                  # <h2o.model.binomial.H2OBinomialModel object at 0x104d18f50>\n\nThe above example shows how to properly deal with numeric columns you would like to use in a\nclassification setting. Additionally, H2O can perform on-the-fly scoring of validation\ndata and provide a host of metrics on the validation and training data. Here\'s an example\nof this functionality, where we additionally split the data set into three pieces for training, \nvalidation, and finally testing:\n\n  >>> fr = h2o.import_file(path="smalldata/logreg/prostate.csv")  # import prostate\n  >>>\n  >>> fr[1] = fr[1].asfactor()                                     # cast to factor\n  >>>\n  >>> r = fr[0].runif()                                            # Random UNIform numbers, one per row\n  >>>\n  >>> train = fr[ r < 0.6 ]                                        # 60% for training data\n  >>>\n  >>> valid = fr[ (0.6 <= r) & (r < 0.9) ]                         # 30% for validation\n  >>>\n  >>> test  = fr[ 0.9 <= r ]                                       # 10% for testing\n  >>>\n  >>> m = h2o.deeplearning(x=train[2:],y=train[1],validation_x=valid[2:],validation_y=valid[1])  # build a deeplearning with a validation set (yes it\'s this simple)\n  >>>\n  >>> m                                                            # display the model summary by default (can also call m.show())\n  >>>\n  >>> m.show()                                                     # equivalent to the above\n  >>>\n  >>> m.model_performance()                                        # show the performance on the training data, (can also be m.performance(train=True)\n  >>>\n  >>> m.model_performance(valid=True)                              # show the performance on the validation data\n  >>>\n  >>> m.model_performance(test_data=test)                          # score and compute new metrics on the test data!\n\nExpanding on this example, there are a number of ways of querying a model for its attributes.\nHere are some examples of how to do just that:\n\n  >>> m.mse()           # MSE on the training data\n  >>>\n  >>> m.mse(valid=True) # MSE on the validation data\n  >>>\n  >>> m.r2()            # R^2 on the training data\n  >>>\n  >>> m.r2(valid=True)  # R^2 on the validation data\n  >>>\n  >>> m.confusion_matrix()  # confusion matrix for max F1\n  >>>\n  >>> m.confusion_matrix("tpr") # confusion matrix for max true positive rate\n  >>>\n  >>> m.confusion_matrix("max_per_class_error")   # etc.\n\nAll of our models support various accessor methods such as these. The following section will\ndiscuss model metrics in greater detail.\n\nOn a final note, each of H2O\'s algorithms handles missing (colloquially: "missing" or "NA")\nand categorical data automatically differently, depending on the algorithm. You can find\nout more about each of the individual differences at the following link: http://docs2.h2o.ai/datascience/top.html\n\nMetrics\n+++++++\n\nH2O models exhibit a wide array of metrics for each of the model categories:\n- Clustering\n- Binomial\n- Multinomial\n- Regression\n- AutoEncoder\nIn turn, each of these categories is associated with a corresponding H2OModelMetrics class.\n\nAll algorithm calls return at least one type of metrics: the training set metrics. When building\na model in H2O, you can optionally provide a validation set for on-the-fly evaluation of \nholdout data. If the validation set is provided, then two types of metrics are returned:\nthe training set metrics and the validation set metrics.\n\nIn addition to the metrics that can be retrieved at model-build time, there is a\npossible third type of metrics available post-build for the final holdout test set that\ncontains data that does not appear in either the training or validation sets: the\ntest set metrics. While the returned object is an H2OModelMetrics rather than an H2O model,\nit can be queried in the same exact way. Here\'s an example:\n\n  >>> fr = h2o.import_file(path="smalldata/iris/iris_wheader.csv")   # import iris\n  >>>\n  >>> r = fr[0].runif()                       # generate a random vector for splitting\n  >>>\n  >>> train = fr[ r < 0.6 ]                   # split out 60% for training\n  >>>\n  >>> valid = fr[ 0.6 <= r & r < 0.9 ]        # split out 30% for validation\n  >>>\n  >>> test = fr[ 0.9 <= r ]                   # split out 10% for testing\n  >>>\n  >>> my_model = h2o.glm(x=train[1:], y=train[0], validation_x=valid[1:], validation_y=valid[0])  # build a GLM\n  >>>\n  >>> my_model.coef()                         # print the GLM coefficients, can also perform my_model.coef_norm() to get the normalized coefficients\n  >>>\n  >>> my_model.null_deviance()                # get the null deviance from the training set metrics\n  >>>\n  >>> my_model.residual_deviance()            # get the residual deviance from the training set metrics\n  >>>\n  >>> my_model.null_deviance(valid=True)      # get the null deviance from the validation set metrics (similar for residual deviance)\n  >>>\n  >>> # now generate a new metrics object for the test hold-out data:\n  >>>\n  >>> my_metrics = my_model.model_performance(test_data=test) # create the new test set metrics\n  >>>\n  >>> my_metrics.null_degrees_of_freedom()    # returns the test null dof\n  >>>\n  >>> my_metrics.residual_deviance()          # returns the test res. deviance\n  >>>\n  >>> my_metrics.aic()                        # returns the test aic\n\nAs you can see, the new model metrics object generated by calling `model_performance` on the\nmodel object supports all of the metric accessor methods as a model. For a complete list of\nthe available metrics for various model categories, please refer to the "Metrics in H2O" section\nof this document. \n\nExample of H2O on Hadoop\n------------------------\n\nHere is a brief example of H2O on Hadoop:\n\n.. code-block:: python\n\n  import h2o\n  h2o.init(ip="192.168.1.10", port=54321)\n  --------------------------  ------------------------------------\n  H2O cluster uptime:         2 minutes 1 seconds 966 milliseconds\n  H2O cluster version:        0.1.27.1064\n  H2O cluster name:           H2O_96762\n  H2O cluster total nodes:    4\n  H2O cluster total memory:   38.34 GB\n  H2O cluster total cores:    16\n  H2O cluster allowed cores:  80\n  H2O cluster healthy:        True\n  --------------------------  ------------------------------------\n  pathDataTrain = ["hdfs://192.168.1.10/user/data/data_train.csv"]\n  pathDataTest = ["hdfs://192.168.1.10/user/data/data_test.csv"]\n  trainFrame = h2o.import_file(path=pathDataTrain)\n  testFrame = h2o.import_file(path=pathDataTest)\n\n  #Parse Progress: [##################################################] 100%\n  #Imported [hdfs://192.168.1.10/user/data/data_train.csv\'] into cluster with 60000 rows and 500 cols\n\n  #Parse Progress: [##################################################] 100%\n  #Imported [\'hdfs://192.168.1.10/user/data/data_test.csv\'] into cluster with 10000 rows and 500 cols\n\n  trainFrame[499]._name = "label"\n  testFrame[499]._name = "label"\n\n  model = h2o.gbm(x=trainFrame.drop("label"),\n              y=trainFrame["label"],\n              validation_x=testFrame.drop("label"),\n              validation_y=testFrame["label"],\n              ntrees=100,\n              max_depth=10\n              )\n\n  #gbm Model Build Progress: [##################################################] 100%\n\n  predictFrame = model.predict(testFrame)\n  model.model_performance(testFrame)\n'
__version__ = 'SUBST_PROJECT_VERSION'
from h2o import *
from model import *
from demo import *
from logging import *
from frame import H2OFrame
from group_by import GroupBy
from two_dim_table import H2OTwoDimTable
from assembly import H2OAssembly
__all__ = ['H2OFrame', 'H2OConnection', 'H2OTwoDimTable', 'GroupBy']
