def setup(self):
    '\n        This function performs all initializations necessary:\n        1. generates all the random values for our dynamic tests like the Multinomial\n        noise std, column count and row count for data sets;\n        2. generate the training/validation/test data sets with only real values;\n        3. insert missing values into training/valid/test data sets.\n        4. take the training/valid/test data sets, duplicate random certain columns,\n            a random number of times and randomly scale each duplicated column;\n        5. generate the training/validation/test data sets with predictors containing enum\n            and real values as well***\n        6. insert missing values into the training/validation/test data sets with predictors\n            containing enum and real values as well\n\n        It was observed that the program actually crash before the tests are completed.  Hence,\n        with multinomial, we are going to save all training data into the sandbox directory.\n        If all tests are passed, the csv files will be deleted if tear_down is called.\n\n        *** according to Tomas, when working with mixed predictors (contains both enum/real\n        value columns), the encoding used is different when regularization is enabled or disabled.\n        When regularization is enabled, true one hot encoding is enabled to encode the enum\n        values to binary bits.  When regularization is disabled, a reference level plus one hot encoding\n        is enabled when encoding the enum values to binary bits.  Hence, two data sets are generated\n        when we work with mixed predictors.  One with true-one-hot set to False for no regularization\n        and one with true-one-hot set to True when regularization is enabled.\n        '
    self.sandbox_dir = pyunit_utils.make_Rsandbox_dir(self.current_dir, self.testName, True)
    self.noise_std = random.uniform(0, math.sqrt((pow((self.max_p_value - self.min_p_value), 2) / 12)))
    self.noise_var = (self.noise_std * self.noise_std)
    self.train_col_count = random.randint(3, self.max_col_count)
    self.train_row_count = round((self.train_col_count * random.uniform(self.min_col_count_ratio, self.max_col_count_ratio)))
    self.class_number = random.randint(3, self.max_class_number)
    self.y_index = self.train_col_count
    self.x_indices = list(range(self.train_col_count))
    self.enum_col = random.randint(1, (self.train_col_count - 1))
    self.enum_level_vec = np.random.random_integers(2, (self.enum_levels - 1), [self.enum_col, 1])
    pyunit_utils.write_syn_floating_point_dataset_glm(self.training_data_file, self.validation_data_file, self.test_data_file, self.weight_data_file, self.train_row_count, self.train_col_count, self.data_type, self.max_p_value, self.min_p_value, self.max_w_value, self.min_w_value, self.noise_std, self.family, self.train_row_count, self.train_row_count, class_number=self.class_number, class_method=[self.class_method, self.class_method, self.test_class_method], class_margin=[self.margin, self.margin, self.test_class_margin])
    pyunit_utils.insert_nan_in_data(self.training_data_file, self.training_data_file_nans, self.nan_fraction)
    pyunit_utils.insert_nan_in_data(self.test_data_file, self.test_data_file_nans, self.nan_fraction)
    pyunit_utils.write_syn_mixed_dataset_glm(self.training_data_file_enum, self.training_data_file_enum_true_one_hot, self.validation_data_file_enum, self.validation_data_file_enum_true_one_hot, self.test_data_file_enum, self.test_data_file_enum_true_one_hot, self.weight_data_file_enum, self.train_row_count, self.train_col_count, self.max_p_value, self.min_p_value, self.max_w_value, self.min_w_value, self.noise_std, self.family, self.train_row_count, self.train_row_count, self.enum_col, self.enum_level_vec, class_number=self.class_number, class_method=[self.class_method, self.class_method, self.test_class_method], class_margin=[self.margin, self.margin, self.test_class_margin])
    pyunit_utils.insert_nan_in_data(self.training_data_file_enum, self.training_data_file_enum_nans, self.nan_fraction)
    pyunit_utils.insert_nan_in_data(self.validation_data_file_enum, self.validation_data_file_enum_nans, self.nan_fraction)
    pyunit_utils.insert_nan_in_data(self.test_data_file_enum, self.test_data_file_enum_nans, self.nan_fraction)
    pyunit_utils.insert_nan_in_data(self.training_data_file_enum_true_one_hot, self.training_data_file_enum_nans_true_one_hot, self.nan_fraction)
    pyunit_utils.insert_nan_in_data(self.validation_data_file_enum_true_one_hot, self.validation_data_file_enum_nans_true_one_hot, self.nan_fraction)
    pyunit_utils.insert_nan_in_data(self.test_data_file_enum_true_one_hot, self.test_data_file_enum_nans_true_one_hot, self.nan_fraction)
    self.training_data = h2o.import_file(pyunit_utils.locate(self.training_data_file))
    self.training_data[self.y_index] = self.training_data[self.y_index].asfactor()
    self.valid_data = h2o.import_file(pyunit_utils.locate(self.validation_data_file))
    self.valid_data[self.y_index] = self.valid_data[self.y_index].asfactor()
    self.test_data = h2o.import_file(pyunit_utils.locate(self.test_data_file))
    self.test_data[self.y_index] = self.test_data[self.y_index].asfactor()
    self.training_data_grid = self.training_data.rbind(self.valid_data)
    self.training_data_grid[self.y_index] = self.training_data_grid[self.y_index].asfactor()
    for ind in range(self.class_number):
        self.sklearn_class_weight[ind] = 1.0
    pyunit_utils.remove_csv_files(self.current_dir, '.csv', action='copy', new_dir_path=self.sandbox_dir)
