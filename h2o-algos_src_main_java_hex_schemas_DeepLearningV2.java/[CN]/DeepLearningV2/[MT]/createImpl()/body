{
  if (parameters.source.numCols() <= 1)   throw new IllegalArgumentException("Training data must have at least 2 features (incl. response).");
  if (parameters.hidden == null)   throw new IllegalArgumentException("There must be at least one hidden layer.");
  for (  int aHidden : parameters.hidden) {
    if (aHidden == 0)     throw new IllegalArgumentException("Hidden layer size must be >0.");
  }
  if (parameters.hidden_dropout_ratios == null) {
    if (parameters.activation == DeepLearningModel.DeepLearningParameters.Activation.TanhWithDropout || parameters.activation == DeepLearningModel.DeepLearningParameters.Activation.MaxoutWithDropout || parameters.activation == DeepLearningModel.DeepLearningParameters.Activation.RectifierWithDropout) {
      parameters.hidden_dropout_ratios=new double[parameters.hidden.length];
      if (!parameters.quiet_mode)       Log.info("Automatically setting all hidden dropout ratios to 0.5.");
      Arrays.fill(parameters.hidden_dropout_ratios,0.5);
    }
  }
 else   if (parameters.hidden_dropout_ratios.length != parameters.hidden.length)   throw new IllegalArgumentException("Must have " + parameters.hidden.length + " hidden layer dropout ratios.");
 else   if (parameters.activation != DeepLearningModel.DeepLearningParameters.Activation.TanhWithDropout && parameters.activation != DeepLearningModel.DeepLearningParameters.Activation.MaxoutWithDropout && parameters.activation != DeepLearningModel.DeepLearningParameters.Activation.RectifierWithDropout) {
    if (!parameters.quiet_mode)     Log.info("Ignoring hidden_dropout_ratios because a non-Dropout activation function was specified.");
  }
  if (!parameters.quiet_mode) {
    if (parameters.adaptive_rate) {
      Log.info("Using automatic learning rate.  Ignoring the following input parameters:");
      Log.info("  rate, rate_decay, rate_annealing, momentum_start, momentum_ramp, momentum_stable, nesterov_accelerated_gradient.");
    }
 else {
      Log.info("Using manual learning rate.  Ignoring the following input parameters:");
      Log.info("  rho, epsilon.");
    }
    if (parameters.initial_weight_distribution == DeepLearningModel.DeepLearningParameters.InitialWeightDistribution.UniformAdaptive) {
      Log.info("Ignoring initial_weight_scale for UniformAdaptive weight distribution.");
    }
    if (parameters.n_folds != 0) {
      if (parameters.override_with_best_model) {
        Log.info("Automatically setting override_with_best_model to false, since the final model is the only scored model with n-fold cross-validation.");
        parameters.override_with_best_model=false;
      }
    }
  }
  if (parameters.loss == DeepLearningModel.DeepLearningParameters.Loss.Automatic) {
    if (!parameters.classification) {
      if (!parameters.quiet_mode)       Log.info("Automatically setting loss to MeanSquare for regression.");
      parameters.loss=DeepLearningModel.DeepLearningParameters.Loss.MeanSquare;
    }
 else     if (parameters.autoencoder) {
      if (!parameters.quiet_mode)       Log.info("Automatically setting loss to MeanSquare for auto-encoder.");
      parameters.loss=DeepLearningModel.DeepLearningParameters.Loss.MeanSquare;
    }
 else {
      if (!parameters.quiet_mode)       Log.info("Automatically setting loss to Cross-Entropy for classification.");
      parameters.loss=DeepLearningModel.DeepLearningParameters.Loss.CrossEntropy;
    }
  }
  if (parameters.autoencoder && parameters.sparsity_beta > 0) {
    if (parameters.activation == DeepLearningModel.DeepLearningParameters.Activation.Tanh || parameters.activation == DeepLearningModel.DeepLearningParameters.Activation.TanhWithDropout) {
      if (parameters.average_activation >= 1 || parameters.average_activation <= -1)       throw new IllegalArgumentException("Tanh average activation must be in (-1,1).");
    }
 else     if (parameters.activation == DeepLearningModel.DeepLearningParameters.Activation.Rectifier || parameters.activation == DeepLearningModel.DeepLearningParameters.Activation.RectifierWithDropout) {
      if (parameters.average_activation <= 0)       throw new IllegalArgumentException("Rectifier average activation must be positive.");
    }
  }
  if (!parameters.classification && parameters.loss == DeepLearningModel.DeepLearningParameters.Loss.CrossEntropy)   throw new IllegalArgumentException("Cannot use CrossEntropy loss function for regression.");
  if (parameters.autoencoder && parameters.loss != DeepLearningModel.DeepLearningParameters.Loss.MeanSquare)   throw new IllegalArgumentException("Must use MeanSquare loss function for auto-encoder.");
  if (parameters.autoencoder && parameters.classification) {
    parameters.classification=false;
    Log.info("Using regression mode for auto-encoder.");
  }
  if (parameters.autoencoder && parameters.validation != null)   throw new UnsupportedOperationException("Cannot specify a validation dataset for auto-encoder.");
  if (parameters.autoencoder && parameters.activation == DeepLearningModel.DeepLearningParameters.Activation.Maxout)   throw new UnsupportedOperationException("Maxout activation is not supported for auto-encoder.");
  if (!parameters.sparse && parameters.col_major) {
    if (!parameters.quiet_mode)     throw new IllegalArgumentException("Cannot use column major storage for non-sparse data handling.");
  }
  DeepLearningParameters parms=parameters.createImpl();
  return new DeepLearning(parms);
}
