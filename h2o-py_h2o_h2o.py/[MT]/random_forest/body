def random_forest(x, y, validation_x=None, validation_y=None, training_frame=None, model_id=None, mtries=None, sample_rate=None, col_sample_rate_per_tree=None, build_tree_one_node=None, ntrees=None, max_depth=None, min_rows=None, nbins=None, nbins_top_level=None, nbins_cats=None, binomial_double_trees=None, validation_frame=None, balance_classes=None, max_after_balance_size=None, seed=None, offset_column=None, weights_column=None, nfolds=None, fold_column=None, fold_assignment=None, keep_cross_validation_predictions=None, score_each_iteration=None, checkpoint=None, stopping_rounds=None, stopping_metric=None, stopping_tolerance=None, max_runtime_secs=None):
    '\n  Build a Big Data Random Forest Model\n  Builds a Random Forest Model on an H2OFrame\n\n\n  Parameters\n  ----------\n\n  x : H2OFrame\n    An H2OFrame containing the predictors in the model.\n  y : H2OFrame\n    An H2OFrame of the response variable in the model.\n  training_frame : H2OFrame\n    (Optional) An H2OFrame. Only used to retrieve weights, offset, or nfolds columns, if they aren\'t already provided in x.\n  model_id : str\n    (Optional) The unique id assigned to the resulting model. If none is given, an id will automatically be generated.\n  mtries : int\n    Number of variables randomly sampled as candidates at each split. If set to -1, defaults to sqrt{p} for classification, and p/3 for regression,\n    where p is the number of predictors.\n  sample_rate : float\n    Row sample rate (from 0.0 to 1.0)\n  col_sample_rate_per_tree : float\n    Column sample rate per tree (from 0.0 to 1.0)\n  build_tree_one_node : bool\n    Run on one node only; no network overhead but fewer cpus used.  Suitable for small datasets.\n  ntrees : int\n    A nonnegative integer that determines the number of trees to grow.\n  max_depth : int\n    Maximum depth to grow the tree.\n  min_rows : int\n    Minimum number of rows to assign to teminal nodes.\n  nbins : int\n    For numerical columns (real/int), build a histogram of (at least) this many bins, then split at the best point.\n  nbins_top_level : int\n    For numerical columns (real/int), build a histogram of (at most) this many bins at the root level, then decrease by factor of two per level.\n  nbins_cats : int\n    For categorical columns (factors), build a histogram of this many bins, then split at the best point. Higher values can lead to more overfitting.\n  binomial_double_trees : bool\n    or binary classification: Build 2x as many trees (one per class) - can lead to higher accuracy.\n  validation_frame : H2OFrame\n     An H2OFrame object containing the variables in the model.\n  balance_classes : bool\n    logical, indicates whether or not to balance training data class counts via over/under-sampling (for imbalanced data)\n  max_after_balance_size : float\n    Maximum relative size of the training data after balancing class counts (can be less than 1.0). Ignored if balance_classes is False, which is the default behavior.\n  seed : int\n    Seed for random numbers (affects sampling) - Note: only reproducible when running single threaded\n  offset_column : H2OFrame\n    Specify the offset column.\n  weights_column : H2OFrame\n    Specify the weights column.\n  nfolds : int\n    (Optional) Number of folds for cross-validation. If nfolds >= 2, then validation must remain empty.\n  fold_column : H2OFrame\n    (Optional) Column with cross-validation fold index assignment per observation\n  fold_assignment : str\n    Cross-validation fold assignment scheme, if fold_column is not specified Must be "AUTO", "Random" or "Modulo"\n  keep_cross_validation_predictions : bool\n    Whether to keep the predictions of the cross-validation models\n  score_each_iteration : bool\n    Attempts to score each tree.\n  stopping_rounds : int\n    Early stopping based on convergence of stopping_metric.\n    Stop if simple moving average of length k of the stopping_metric does not improve\n    (by stopping_tolerance) for k=stopping_rounds scoring events.\n    Can only trigger after at least 2k scoring events. Use 0 to disable.\n  stopping_metric : str\n    Metric to use for convergence checking, only for _stopping_rounds > 0\n    Can be one of "AUTO", "deviance", "logloss", "MSE", "AUC", "r2", "misclassification".\n  stopping_tolerance : float\n    Relative tolerance for metric-based stopping criterion (stop if relative improvement is not at least this much)\n\n  :return: A new classifier or regression model.\n  '
    warnings.warn('`h2o.random_forest` is deprecated. Use the estimators sub module to build an H2ORandomForestEstimator.', category=DeprecationWarning, stacklevel=2)
    parms = {k: v for (k, v) in locals().items() if ((k in ['training_frame', 'validation_frame', 'validation_x', 'validation_y', 'offset_column', 'weights_column', 'fold_column']) or (v is not None))}
    parms['algo'] = 'drf'
    return supervised(parms)
