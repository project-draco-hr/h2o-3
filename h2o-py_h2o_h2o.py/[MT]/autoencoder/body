def autoencoder(x, training_frame=None, model_id=None, overwrite_with_best_model=None, checkpoint=None, use_all_factor_levels=None, activation=None, hidden=None, epochs=None, train_samples_per_iteration=None, seed=None, adaptive_rate=None, rho=None, epsilon=None, rate=None, rate_annealing=None, rate_decay=None, momentum_start=None, momentum_ramp=None, momentum_stable=None, nesterov_accelerated_gradient=None, input_dropout_ratio=None, hidden_dropout_ratios=None, l1=None, l2=None, max_w2=None, initial_weight_distribution=None, initial_weight_scale=None, loss=None, distribution=None, tweedie_power=None, score_interval=None, score_training_samples=None, score_duty_cycle=None, classification_stop=None, regression_stop=None, quiet_mode=None, max_confusion_matrix_size=None, max_hit_ratio_k=None, balance_classes=None, class_sampling_factors=None, max_after_balance_size=None, diagnostics=None, variable_importances=None, fast_mode=None, ignore_const_cols=None, force_load_balance=None, replicate_training_data=None, single_node_mode=None, shuffle_training_data=None, sparse=None, col_major=None, average_activation=None, sparsity_beta=None, max_categorical_features=None, reproducible=None, export_weights_and_biases=None):
    '\n  Build unsupervised auto encoder using H2O Deeplearning\n\n  :param x: An H2OFrame containing the predictors in the model.\n  :param training_frame: (Optional) An H2OFrame. Only used to retrieve weights, offset, or nfolds columns, if they aren\'t already provided in x.\n  :param model_id: (Optional) The unique id assigned to the resulting model. If none is given, an id will automatically be generated.\n  :param overwrite_with_best_model: Logical. If True, overwrite the final model with the best model found during training. Defaults to True.\n  :param checkpoint: "Model checkpoint (either key or H2ODeepLearningModel) to resume training with."\n  :param use_all_factor_levels: Logical. Use all factor levels of categorical variance. Otherwise the first factor level is omitted (without loss of accuracy). Useful for variable importances and auto-enabled for autoencoder.\n  :param activation: A string indicating the activation function to use. Must be either "Tanh", "TanhWithDropout", "Rectifier", "RectifierWithDropout", "Maxout", or "MaxoutWithDropout"\n  :param hidden: Hidden layer sizes (e.g. c(100,100))\n  :param epochs: How many times the dataset should be iterated (streamed), can be fractional\n  :param train_samples_per_iteration: Number of training samples (globally) per MapReduce iteration. Special values are: 0 one epoch; -1 all available data (e.g., replicated training data); or -2 auto-tuning (default)\n  :param seed: Seed for random numbers (affects sampling) - Note: only reproducible when running single threaded\n  :param adaptive_rate: Logical. Adaptive learning rate (ADAELTA)\n  :param rho: Adaptive learning rate time decay factor (similarity to prior updates)\n  :param epsilon: Adaptive learning rate parameter, similar to learn rate annealing during initial training phase. Typical values are between 1.0e-10 and 1.0e-4\n  :param rate: Learning rate (higher => less stable, lower => slower convergence)\n  :param rate_annealing: Learning rate annealing: \\eqn{(rate)/(1 + rate_annealing*samples)\n  :param rate_decay: Learning rate decay factor between layers (N-th layer: \\eqn{rate*\x07lpha^(N-1))\n  :param momentum_start: Initial momentum at the beginning of training (try 0.5)\n  :param momentum_ramp: Number of training samples for which momentum increases\n  :param momentum_stable: Final momentum after the amp is over (try 0.99)\n  :param nesterov_accelerated_gradient: Logical. Use Nesterov accelerated gradient (recommended)\n  :param input_dropout_ratio: A fraction of the features for each training row to be omitted from training in order to improve generalization (dimension sampling).\n  :param hidden_dropout_ratios: Input layer dropout ratio (can improve generalization) specify one value per hidden layer, defaults to 0.5\n  :param l1: L1 regularization (can add stability and improve generalization, causes many weights to become 0)\n  :param l2: L2 regularization (can add stability and improve generalization, causes many weights to be small)\n  :param max_w2: Constraint for squared sum of incoming weights per unit (e.g. Rectifier)\n  :param initial_weight_distribution: Can be "Uniform", "UniformAdaptive", or "Normal"\n  :param initial_weight_scale: Uniform: -value ... value, Normal: stddev\n  :param loss: Loss function: "Automatic", "CrossEntropy" (for classification only), "MeanSquare", "Absolute" (experimental) or "Huber" (experimental)\n  :param distribution: A character string. The distribution function of the response. Must be "AUTO", "bernoulli", "multinomial", "poisson", "gamma", "tweedie", "laplace", "huber" or "gaussian"\n  :param tweedie_power: Tweedie power (only for Tweedie distribution, must be between 1 and 2)\n  :param score_interval: Shortest time interval (in secs) between model scoring\n  :param score_training_samples: Number of training set samples for scoring (0 for all)\n  :param score_duty_cycle: Maximum duty cycle fraction for scoring (lower: more training, higher: more scoring)\n  :param classification_stop: Stopping criterion for classification error fraction on training data (-1 to disable)\n  :param regression_stop: Stopping criterion for regression error (MSE) on training data (-1 to disable)\n  :param quiet_mode: Enable quiet mode for less output to standard output\n  :param max_confusion_matrix_size: Max. size (number of classes) for confusion matrices to be shown\n  :param max_hit_ratio_k: Max number (top K) of predictions to use for hit ratio computation(for multi-class only, 0 to disable)\n  :param balance_classes: Balance training data class counts via over/under-sampling (for imbalanced data)\n  :param class_sampling_factors: Desired over/under-sampling ratios per class (in lexicographic order). If not specified, sampling factors will be automatically computed to obtain class balance during training. Requires balance_classes.\n  :param max_after_balance_size: Maximum relative size of the training data after balancing class counts (can be less than 1.0)\n  :param diagnostics: Enable diagnostics for hidden layers\n  :param variable_importances: Compute variable importances for input features (Gedeon method) - can be slow for large networks)\n  :param fast_mode: Enable fast mode (minor approximations in back-propagation)\n  :param ignore_const_cols: Ignore constant columns (no information can be gained anyway)\n  :param force_load_balance: Force extra load balancing to increase training speed for small datasets (to keep all cores busy)\n  :param replicate_training_data: Replicate the entire training dataset onto every node for faster training\n  :param single_node_mode: Run on a single node for fine-tuning of model parameters\n  :param shuffle_training_data: Enable shuffling of training data (recommended if training data is replicated and train_samples_per_iteration is close to \\eqn{numRows*numNodes\n  :param sparse: Sparse data handling (Experimental)\n  :param col_major: Use a column major weight matrix for input layer. Can speed up forward propagation, but might slow down backpropagation (Experimental)\n  :param average_activation: Average activation for sparse auto-encoder (Experimental)\n  :param sparsity_beta: Sparsity regularization (Experimental)\n  :param max_categorical_features: Max. number of categorical features, enforced via hashing Experimental)\n  :param reproducible: Force reproducibility on small data (will be slow - only uses 1 thread)\n  :param export_weights_and_biases: Whether to export Neural Network weights and biases to H2O Frames"\n  :return: Return a new autoencoder\n  '
    parms = {k: v for (k, v) in locals().items() if ((k in ['training_frame', 'validation_frame', 'validation_x', 'validation_y', 'offset_column', 'weights_column', 'fold_column']) or (v is not None))}
    parms['algo'] = 'deeplearning'
    parms['autoencoder'] = True
    return h2o_model_builder.unsupervised(parms)
