def autoencoder(x, training_frame=None, model_id=None, overwrite_with_best_model=None, checkpoint=None, use_all_factor_levels=None, activation=None, hidden=None, epochs=None, train_samples_per_iteration=None, target_ratio_comm_to_comp=None, seed=None, adaptive_rate=None, rho=None, epsilon=None, rate=None, rate_annealing=None, rate_decay=None, momentum_start=None, momentum_ramp=None, momentum_stable=None, nesterov_accelerated_gradient=None, input_dropout_ratio=None, hidden_dropout_ratios=None, l1=None, l2=None, max_w2=None, initial_weight_distribution=None, initial_weight_scale=None, loss=None, distribution=None, tweedie_power=None, score_interval=None, score_training_samples=None, score_duty_cycle=None, classification_stop=None, regression_stop=None, quiet_mode=None, max_confusion_matrix_size=None, max_hit_ratio_k=None, balance_classes=None, class_sampling_factors=None, max_after_balance_size=None, diagnostics=None, variable_importances=None, fast_mode=None, ignore_const_cols=None, force_load_balance=None, replicate_training_data=None, single_node_mode=None, shuffle_training_data=None, sparse=None, col_major=None, average_activation=None, sparsity_beta=None, max_categorical_features=None, reproducible=None, export_weights_and_biases=None, max_runtime_secs=None):
    '\n  Build unsupervised auto encoder using H2O Deeplearning\n\n  Parameters\n  ----------\n    x : H2OFrame\n      An H2OFrame containing the predictors in the model.\n    training_frame : H2OFrame\n      (Optional) An H2OFrame. Only used to retrieve weights, offset, or nfolds columns, if they aren\'t already provided in x.\n    model_id : str\n      (Optional) The unique id assigned to the resulting model. If none is given, an id will automatically be generated.\n    overwrite_with_best_model : bool\n      Logical. If True, overwrite the final model with the best model found during training. Defaults to True.\n    checkpoint : H2ODeepLearningModel\n      "Model checkpoint (either key or H2ODeepLearningModel) to resume training with."\n    use_all_factor_levels : bool\n      Logical. Use all factor levels of categorical variance. Otherwise the first factor level is omitted (without loss of accuracy).\n      Useful for variable importances and auto-enabled for autoencoder.\n    activation : str\n      A string indicating the activation function to use. Must be either "Tanh", "TanhWithDropout", "Rectifier", "RectifierWithDropout", "Maxout", or "MaxoutWithDropout"\n    hidden : list\n      Hidden layer sizes (e.g. c(100,100))\n    epochs : float\n      How many times the dataset should be iterated (streamed), can be fractional\n    train_samples_per_iteration : int\n      Number of training samples (globally) per MapReduce iteration. Special values are: 0 one epoch; -1 all available data\n      (e.g., replicated training data); or -2 auto-tuning (default)\n    target_ratio_comm_to_comp : float\n      Target ratio of communication overhead to computation. Only for multi-node operation and train_samples_per_iteration=-2 (auto-tuning).\n      Higher values can lead to faster convergence.\n    seed : int\n      Seed for random numbers (affects sampling) - Note: only reproducible when running single threaded\n    adaptive_rate : bool\n      Logical. Adaptive learning rate (ADAELTA)\n    rho : float\n      Adaptive learning rate time decay factor (similarity to prior updates)\n    epsilon : float\n      Adaptive learning rate parameter, similar to learn rate annealing during initial training phase. Typical values are between 1.0e-10 and 1.0e-4\n    rate : float\n      Learning rate (higher => less stable, lower => slower convergence)\n    rate_annealing : float\n      Learning rate annealing: \\eqn{(rate)/(1 + rate_annealing*samples)\n    rate_decay : float\n      Learning rate decay factor between layers (N-th layer: \\eqn{rate*\x07lpha^(N-1))\n    momentum_start : float\n      Initial momentum at the beginning of training (try 0.5)\n    momentum_ramp : int\n      Number of training samples for which momentum increases\n    momentum_stable : float\n      Final momentum after the amp is over (try 0.99)\n    nesterov_accelerated_gradient : bool\n      Logical. Use Nesterov accelerated gradient (recommended)\n    input_dropout_ratio : float\n      A fraction of the features for each training row to be omitted from training in order to improve generalization (dimension sampling).\n    hidden_dropout_ratios : float\n      Input layer dropout ratio (can improve generalization) specify one value per hidden layer, defaults to 0.5\n    l1 : float\n      L1 regularization (can add stability and improve generalization, causes many weights to become 0)\n    l2:  float\n      L2 regularization (can add stability and improve generalization, causes many weights to be small)\n    max_w2 : float\n      Constraint for squared sum of incoming weights per unit (e.g. Rectifier)\n    initial_weight_distribution : str\n      Can be "Uniform", "UniformAdaptive", or "Normal"\n    initial_weight_scale : str\n      Uniform: -value ... value, Normal: stddev\n    loss : str\n      Loss function: "Automatic", "CrossEntropy" (for classification only), "Quadratic", "Absolute" (experimental) or "Huber" (experimental)\n    distribution : str\n      A character string. The distribution function of the response. Must be "AUTO", "bernoulli", "multinomial", "poisson", "gamma",\n      "tweedie", "laplace", "huber" or "gaussian"\n    tweedie_power : float\n      Tweedie power (only for Tweedie distribution, must be between 1 and 2)\n    score_interval : int\n      Shortest time interval (in secs) between model scoring\n    score_training_samples : int\n      Number of training set samples for scoring (0 for all)\n    score_duty_cycle : float\n      Maximum duty cycle fraction for scoring (lower: more training, higher: more scoring)\n    classification_stop : float\n      Stopping criterion for classification error fraction on training data (-1 to disable)\n    regression_stop : float\n      Stopping criterion for regression error (MSE) on training data (-1 to disable)\n    stopping_rounds : int\n      Early stopping based on convergence of stopping_metric.\n      Stop if simple moving average of length k of the stopping_metric does not improve\n      (by stopping_tolerance) for k=stopping_rounds scoring events.\n      Can only trigger after at least 2k scoring events. Use 0 to disable.\n    stopping_metric : str\n      Metric to use for convergence checking, only for _stopping_rounds > 0\n      Can be one of "AUTO", "MSE".\n    stopping_tolerance : float\n      Relative tolerance for metric-based stopping criterion (stop if relative improvement is not at least this much)\n    quiet_mode : bool\n      Enable quiet mode for less output to standard output\n    max_confusion_matrix_size : int\n      Max. size (number of classes) for confusion matrices to be shown\n    max_hit_ratio_k : float\n      Max number (top K) of predictions to use for hit ratio computation(for multi-class only, 0 to disable)\n    balance_classes : bool\n      Balance training data class counts via over/under-sampling (for imbalanced data)\n    class_sampling_factors : list\n      Desired over/under-sampling ratios per class (in lexicographic order). If not specified, sampling factors will be automatically computed to obtain\n      class balance during training. Requires balance_classes.\n    max_after_balance_size : float\n      Maximum relative size of the training data after balancing class counts (can be less than 1.0)\n    diagnostics : bool\n      Enable diagnostics for hidden layers\n    variable_importances : bool\n      Compute variable importances for input features (Gedeon method) - can be slow for large networks)\n    fast_mode : bool\n      Enable fast mode (minor approximations in back-propagation)\n    ignore_const_cols : bool\n      Ignore constant columns (no information can be gained anyway)\n    force_load_balance : bool\n      Force extra load balancing to increase training speed for small datasets (to keep all cores busy)\n    replicate_training_data : bool\n      Replicate the entire training dataset onto every node for faster training\n    single_node_mode : bool\n      Run on a single node for fine-tuning of model parameters\n    shuffle_training_data : bool\n      Enable shuffling of training data (recommended if training data is replicated and train_samples_per_iteration is close to \\eqn{numRows*numNodes\n    sparse : bool\n      Sparse data handling (Experimental)\n    col_major : bool\n      Use a column major weight matrix for input layer. Can speed up forward propagation, but might slow down backpropagation (Experimental)\n    average_activation : float\n      Average activation for sparse auto-encoder (Experimental)\n    sparsity_beta : float\n      Sparsity regularization (Experimental)\n    max_categorical_features : int\n      Max. number of categorical features, enforced via hashing Experimental)\n    reproducible : bool\n      Force reproducibility on small data (will be slow - only uses 1 thread)\n    export_weights_and_biases : bool\n      Whether to export Neural Network weights and biases to H2O Frames"\n\n  :return: H2OAutoEncoderModel\n\n\n\n  '
    warnings.warn('`h2o.autoencoder` is deprecated. Use the estimators sub module to build an H2OAutoEncoderEstimator.', category=DeprecationWarning, stacklevel=2)
    parms = {k: v for (k, v) in locals().items() if ((k in ['training_frame', 'validation_frame', 'validation_x', 'validation_y', 'offset_column', 'weights_column', 'fold_column']) or (v is not None))}
    parms['algo'] = 'deeplearning'
    parms['autoencoder'] = True
    return unsupervised(parms)
