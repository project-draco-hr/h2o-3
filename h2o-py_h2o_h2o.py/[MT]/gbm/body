def gbm(x, y, validation_x=None, validation_y=None, training_frame=None, model_id=None, distribution=None, tweedie_power=None, ntrees=None, max_depth=None, min_rows=None, learn_rate=None, sample_rate=None, col_sample_rate=None, col_sample_rate_per_tree=None, nbins=None, nbins_top_level=None, nbins_cats=None, validation_frame=None, balance_classes=None, max_after_balance_size=None, seed=None, build_tree_one_node=None, nfolds=None, fold_column=None, fold_assignment=None, keep_cross_validation_predictions=None, score_each_iteration=None, offset_column=None, weights_column=None, do_future=None, checkpoint=None, stopping_rounds=None, stopping_metric=None, stopping_tolerance=None):
    '\n  Builds gradient boosted classification trees, and gradient boosted regression trees on a parsed data set.\n  The default distribution function will guess the model type based on the response column typerun properly the\n  response column must be an numeric for "gaussian" or an enum for "bernoulli" or "multinomial".\n\n  Parameters\n  ----------\n\n  x : H2OFrame\n    An H2OFrame containing the predictors in the model.\n  y : H2OFrame\n    An H2OFrame of the response variable in the model.\n  training_frame : H2OFrame\n    (Optional) An H2OFrame. Only used to retrieve weights, offset, or nfolds columns, if they aren\'t already provided in x.\n  model_id : str\n    (Optional) The unique id assigned to the resulting model. If none is given, an id will automatically be generated.\n  distribution : str\n     A character string. The distribution function of the response. Must be "AUTO", "bernoulli", "multinomial", "poisson", "gamma", "tweedie" or "gaussian"\n  tweedie_power : float\n    Tweedie power (only for Tweedie distribution, must be between 1 and 2)\n  ntrees : int\n    A non-negative integer that determines the number of trees to grow.\n  max_depth : int\n    Maximum depth to grow the tree.\n  min_rows : int\n    Minimum number of rows to assign to terminal nodes.\n  learn_rate : float\n    Learning rate (from 0.0 to 1.0)\n  sample_rate : float\n    Row sample rate (from 0.0 to 1.0)\n  col_sample_rate : float\n    Column sample rate (from 0.0 to 1.0)\n  col_sample_rate_per_tree : float\n    Column sample rate per tree (from 0.0 to 1.0)\n  nbins : int\n    For numerical columns (real/int), build a histogram of (at least) this many bins, then split at the best point.\n  nbins_top_level : int\n    For numerical columns (real/int), build a histogram of (at most) this many bins at the root level, then decrease by factor of two per level.\n  nbins_cats : int\n    For categorical columns (factors), build a histogram of this many bins, then split at the best point. Higher values can lead to more overfitting.\n  validation_frame : H2OFrame\n    An H2OFrame object indicating the validation dataset used to contruct the confusion matrix. If left blank, this defaults to the training data when nfolds = 0\n  balance_classes : bool\n    logical, indicates whether or not to balance training data class counts via over/under-sampling (for imbalanced data)\n  max_after_balance_size : float\n    Maximum relative size of the training data after balancing class counts (can be less than 1.0). Ignored if balance_classes is False, which is the default behavior.\n  seed : int\n    Seed for random numbers (affects sampling when balance_classes=T)\n  build_tree_one_node : bool\n    Run on one node only; no network overhead but fewer cpus used.  Suitable for small datasets.\n  nfolds : int\n    (Optional) Number of folds for cross-validation. If nfolds >= 2, then validation must remain empty.\n  fold_column : H2OFrame\n    (Optional) Column with cross-validation fold index assignment per observation\n  fold_assignment : str\n    Cross-validation fold assignment scheme, if fold_column is not specified Must be "AUTO", "Random" or "Modulo"\n  keep_cross_validation_predictions : bool\n    Whether to keep the predictions of the cross-validation models\n  score_each_iteration : bool\n    Attempts to score each tree.\n  offset_column : H2OFrame\n    Specify the offset column.\n  weights_column : H2OFrame\n    Specify the weights column.\n  stopping_rounds : int\n    Early stopping based on convergence of stopping_metric.\n    Stop if simple moving average of length k of the stopping_metric does not improve\n    (by stopping_tolerance) for k=stopping_rounds scoring events.\n    Can only trigger after at least 2k scoring events. Use 0 to disable.\n  stopping_metric : str\n    Metric to use for convergence checking, only for _stopping_rounds > 0\n    Can be one of "AUTO", "deviance", "logloss", "MSE", "AUC", "r2", "misclassification".\n  stopping_tolerance : float\n    Relative tolerance for metric-based stopping criterion (stop if relative improvement is not at least this much)\n\n  :return: A new classifier or regression model.\n  '
    warnings.warn('`h2o.gbm` is deprecated. Use the estimators sub module to build an H2OGradientBoostedEstimator.', category=DeprecationWarning, stacklevel=2)
    parms = {k: v for (k, v) in locals().items() if ((k in ['training_frame', 'validation_frame', 'validation_x', 'validation_y', 'offset_column', 'weights_column', 'fold_column']) or (v is not None))}
    parms['algo'] = 'gbm'
    return supervised(parms)
