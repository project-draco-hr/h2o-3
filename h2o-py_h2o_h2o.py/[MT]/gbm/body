def gbm(x, y, validation_x=None, validation_y=None, training_frame=None, model_id=None, distribution=None, tweedie_power=None, ntrees=None, max_depth=None, min_rows=None, learn_rate=None, nbins=None, nbins_cats=None, validation_frame=None, balance_classes=None, max_after_balance_size=None, seed=None, build_tree_one_node=None, nfolds=None, fold_column=None, fold_assignment=None, keep_cross_validation_predictions=None, score_each_iteration=None, offset_column=None, weights_column=None, do_future=None, checkpoint=None):
    '\n  Builds gradient boosted classification trees, and gradient boosted regression trees on a parsed data set.\n  The default distribution function will guess the model type based on the response column typerun properly the\n  response column must be an numeric for "gaussian" or an enum for "bernoulli" or "multinomial".\n\n  :param x: An H2OFrame containing the predictors in the model.\n  :param y: An H2OFrame of the response variable in the model.\n  :param training_frame: (Optional) An H2OFrame. Only used to retrieve weights, offset, or nfolds columns, if they aren\'t already provided in x.\n  :param model_id: (Optional) The unique id assigned to the resulting model. If none is given, an id will automatically be generated.\n  :param distribution: A character string. The distribution function of the response. Must be "AUTO", "bernoulli", "multinomial", "poisson", "gamma", "tweedie" or "gaussian"\n  :param tweedie_power: Tweedie power (only for Tweedie distribution, must be between 1 and 2)\n  :param ntrees: A non-negative integer that determines the number of trees to grow.\n  :param max_depth: Maximum depth to grow the tree.\n  :param min_rows: Minimum number of rows to assign to terminal nodes.\n  :param learn_rate: An integer from 0.0 to 1.0\n  :param nbins: For numerical columns (real/int), build a histogram of this many bins, then split at the best point\n  :param nbins_cats: For categorical columns (enum), build a histogram of this many bins, then split at the best point. Higher values can lead to more overfitting.\n  :param validation_frame: An H2OFrame object indicating the validation dataset used to contruct the confusion matrix. If left blank, this defaults to the training data when nfolds = 0\n  :param balance_classes: logical, indicates whether or not to balance training data class counts via over/under-sampling (for imbalanced data)\n  :param max_after_balance_size: Maximum relative size of the training data after balancing class counts (can be less than 1.0)\n  :param seed: Seed for random numbers (affects sampling when balance_classes=T)\n  :param build_tree_one_node: Run on one node only; no network overhead but fewer cpus used.  Suitable for small datasets.\n  :param nfolds: (Optional) Number of folds for cross-validation. If nfolds >= 2, then validation must remain empty.\n  :param fold_column: (Optional) Column with cross-validation fold index assignment per observation\n  :param fold_assignment: Cross-validation fold assignment scheme, if fold_column is not specified Must be "AUTO", "Random" or "Modulo"\n  :param keep_cross_validation_predictions: Whether to keep the predictions of the cross-validation models\n  :param score_each_iteration: Attempts to score each tree.\n  :param offset_column: Specify the offset column.\n  :param weights_column: Specify the weights column.\n  :return: A new classifier or regression model.\n  '
    parms = {k: v for (k, v) in locals().items() if ((k in ['training_frame', 'validation_frame', 'validation_x', 'validation_y', 'offset_column', 'weights_column', 'fold_column']) or (v is not None))}
    parms['algo'] = 'gbm'
    return h2o_model_builder.supervised(parms)
