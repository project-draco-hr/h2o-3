def glm(x, y, validation_x=None, validation_y=None, training_frame=None, model_id=None, validation_frame=None, max_iterations=None, beta_epsilon=None, solver=None, standardize=None, family=None, link=None, tweedie_variance_power=None, tweedie_link_power=None, alpha=None, prior=None, lambda_search=None, nlambdas=None, lambda_min_ratio=None, beta_constraints=None, offset_column=None, weights_column=None, nfolds=None, fold_column=None, fold_assignment=None, keep_cross_validation_predictions=None, intercept=None, Lambda=None, max_active_predictors=None, do_future=None, checkpoint=None, max_runtime_secs=None):
    '\n  Build a Generalized Linear Model\n  Fit a generalized linear model, specified by a response variable, a set of predictors, and a description of the error\n  distribution.\n\n  Parameters\n  ----------\n\n  x : H2OFrame\n    An H2OFrame containing the predictors in the model.\n  y : H2OFrame\n    An H2OFrame of the response variable in the model.\n  training_frame : H2OFrame\n    (Optional) An H2OFrame. Only used to retrieve weights, offset, or nfolds columns, if they aren\'t already provided in x.\n  model_id : str\n    (Optional) The unique id assigned to the resulting model. If none is given, an id will automatically be generated.\n  validation_frame : H2OFrame\n    An H2OFrame object containing the variables in the model.\n  max_iterations : int\n    A non-negative integer specifying the maximum number of iterations.\n  beta_epsilon : int\n     A non-negative number specifying the magnitude of the maximum difference between the coefficient estimates from successive iterations.\n     Defines the convergence criterion for h2o.glm.\n  solver : str\n    A character string specifying the solver used: IRLSM (supports more features), L_BFGS (scales better for datasets with many columns)\n  standardize : bool\n    A logical value indicating whether the numeric predictors should be standardized to have a mean of 0 and a variance of 1 prior to training the models.\n  family : str\n    A character string specifying the distribution of the model:  gaussian, binomial, poisson, gamma, tweedie.\n  link : str\n    A character string specifying the link function. The default is the canonical link for the family.\n\n\n  The supported links for each of the family specifications are:\n\n          "gaussian": "identity", "log", "inverse"\n\n          "binomial": "logit", "log"\n          "poisson": "log", "identity"\n          "gamma": "inverse", "log", "identity"\n          "tweedie": "tweedie"\n\n  tweedie_variance_power : int\n     numeric specifying the power for the variance function when family = "tweedie".\n  tweedie_link_power : int\n    A numeric specifying the power for the link function when family = "tweedie".\n  alpha : float\n    A numeric in [0, 1] specifying the elastic-net mixing parameter.\n\n  The elastic-net penalty is defined to be:\n  eqn{P(\x07lpha,\x08eta) = (1-\x07lpha)/2||\x08eta||_2^2 + \x07lpha||\x08eta||_1 = \\sum_j [(1-\x07lpha)/2 \x08eta_j^2 + \x07lpha|\x08eta_j|],\n  making alpha = 1 the lasso penalty and alpha = 0 the ridge penalty.\n\n  Lambda : float\n    A non-negative shrinkage parameter for the elastic-net, which multiplies \\eqn{P(\x07lpha,\x08eta) in the objective function.\n    When Lambda = 0, no elastic-net penalty is applied and ordinary generalized linear models are fit.\n  prior : float\n    (Optional) A numeric specifying the prior probability of class 1 in the response when family = "binomial". The default prior is the observational frequency of class 1.\n  lambda_search : bool\n    A logical value indicating whether to conduct a search over the space of lambda values starting from the lambda max, given lambda is interpreted as lambda minself.\n  nlambdas : int\n    The number of lambda values to use when lambda_search = TRUE.\n  lambda_min_ratio : float\n    Smallest value for lambda as a fraction of lambda.max. By default if the number of observations is greater than the the number of\n    variables then lambda_min_ratio = 0.0001; if the number of observations is less than the number of variables then lambda_min_ratio = 0.01.\n  beta_constraints : H2OFrame\n    A data.frame or H2OParsedData object with the columns ["names", "lower_bounds", "upper_bounds", "beta_given"], where each row corresponds to a predictor\n    in the GLM. "names" contains the predictor names, "lower"/"upper_bounds", are the lower and upper bounds of beta, and "beta_given" is some supplied starting\n    values for the\n  offset_column : H2OFrame\n    Specify the offset column.\n  weights_column : H2OFrame\n    Specify the weights column.\n  nfolds : int\n    (Optional) Number of folds for cross-validation. If nfolds >= 2, then validation must remain empty.\n  fold_column : H2OFrame\n    (Optional) Column with cross-validation fold index assignment per observation\n  fold_assignment : str\n    Cross-validation fold assignment scheme, if fold_column is not specified Must be "AUTO", "Random" or "Modulo"\n  keep_cross_validation_predictions : bool\n    Whether to keep the predictions of the cross-validation models\n  intercept : bool\n    Logical, include constant term (intercept) in the model\n  max_active_predictors : int\n    (Optional) Convergence criteria for number of predictors when using L1 penalty.\n\n\n\n  Returns: A subclass of ModelBase is returned. The specific subclass depends on the machine learning task at hand (if\n  it\'s binomial classification, then an H2OBinomialModel is returned, if it\'s regression then a H2ORegressionModel is\n  returned). The default print-out of the models is shown, but further GLM-specifc information can be queried out of\n  the object.\n  Upon completion of the GLM, the resulting object has coefficients, normalized coefficients, residual/null deviance,\n  aic, and a host of model metrics including MSE, AUC (for logistic regression), degrees of freedom, and confusion\n  matrices.\n  '
    warnings.warn('`h2o.glm` is deprecated. Use the estimators sub module to build an H2OGeneralizedLinearEstimator.', category=DeprecationWarning, stacklevel=2)
    parms = {k.lower(): v for (k, v) in locals().items() if ((k in ['training_frame', 'validation_frame', 'validation_x', 'validation_y', 'offset_column', 'weights_column', 'fold_column']) or (v is not None))}
    if (('alpha' in parms) and (not isinstance(parms['alpha'], (list, tuple)))):
        parms['alpha'] = [parms['alpha']]
    parms['algo'] = 'glm'
    return supervised(parms)
