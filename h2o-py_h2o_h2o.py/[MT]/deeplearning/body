def deeplearning(x, y=None, validation_x=None, validation_y=None, training_frame=None, model_id=None, overwrite_with_best_model=None, validation_frame=None, checkpoint=None, autoencoder=None, use_all_factor_levels=None, activation=None, hidden=None, epochs=None, train_samples_per_iteration=None, target_ratio_comm_to_comp=None, seed=None, adaptive_rate=None, rho=None, epsilon=None, rate=None, rate_annealing=None, rate_decay=None, momentum_start=None, momentum_ramp=None, momentum_stable=None, nesterov_accelerated_gradient=None, input_dropout_ratio=None, hidden_dropout_ratios=None, l1=None, l2=None, max_w2=None, initial_weight_distribution=None, initial_weight_scale=None, loss=None, distribution=None, tweedie_power=None, score_interval=None, score_training_samples=None, score_validation_samples=None, score_duty_cycle=None, classification_stop=None, regression_stop=None, quiet_mode=None, max_confusion_matrix_size=None, max_hit_ratio_k=None, balance_classes=None, class_sampling_factors=None, max_after_balance_size=None, score_validation_sampling=None, diagnostics=None, variable_importances=None, fast_mode=None, ignore_const_cols=None, force_load_balance=None, replicate_training_data=None, single_node_mode=None, shuffle_training_data=None, sparse=None, col_major=None, average_activation=None, sparsity_beta=None, max_categorical_features=None, reproducible=None, export_weights_and_biases=None, offset_column=None, weights_column=None, nfolds=None, fold_column=None, fold_assignment=None, keep_cross_validation_predictions=None):
    '\n  Build a supervised Deep Learning model\n  Performs Deep Learning neural networks on an H2OFrame\n\n Parameters\n ----------\n\n  x : H2OFrame\n    An H2OFrame containing the predictors in the model.\n  y : H2OFrame\n    An H2OFrame of the response variable in the model.\n  training_frame : H2OFrame\n    (Optional) An H2OFrame. Only used to retrieve weights, offset, or nfolds columns, if they aren\'t already provided in x.\n  model_id : str\n    (Optional) The unique id assigned to the resulting model. If none is given, an id will automatically be generated.\n  overwrite_with_best_model : bool\n    Logical. If True, overwrite the final model with the best model found during training. Defaults to True.\n  validation_frame : H2OFrame\n    (Optional) An H2OFrame object indicating the validation dataset used to construct the confusion matrix. If left blank, this defaults to the\n    training data when nfolds = 0\n  checkpoint : H2ODeepLearningModel\n    "Model checkpoint (either key or H2ODeepLearningModel) to resume training with."\n  autoencoder : bool\n    Enable auto-encoder for model building.\n  use_all_factor_levels : bool\n    Logical. Use all factor levels of categorical variance. Otherwise the first factor level is omitted (without loss of accuracy). Useful for variable\n    importances and auto-enabled for autoencoder.\n  activation : str\n    A string indicating the activation function to use. Must be either "Tanh", "TanhWithDropout", "Rectifier", "RectifierWithDropout", "Maxout", or "MaxoutWithDropout"\n  hidden : list\n    Hidden layer sizes (e.g. c(100,100))\n  epochs : float\n    How many times the dataset should be iterated (streamed), can be fractional\n  train_samples_per_iteration : int\n    Number of training samples (globally) per MapReduce iteration. Special values are: 0 one epoch; -1 all available data (e.g., replicated training data);\n    or -2 auto-tuning (default)\n  target_ratio_comm_to_comp : float\n    Target ratio of communication overhead to computation. Only for multi-node operation and train_samples_per_iteration=-2 (auto-tuning).\n    Higher values can lead to faster convergence.\n  seed : int\n    Seed for random numbers (affects sampling) - Note: only reproducible when running single threaded\n  adaptive_rate : bool\n    Logical. Adaptive learning rate (ADAELTA)\n  rho : float\n    Adaptive learning rate time decay factor (similarity to prior updates)\n  epsilon : float\n    Adaptive learning rate parameter, similar to learn rate annealing during initial training phase. Typical values are between 1.0e-10 and 1.0e-4\n  rate : float\n    Learning rate (higher => less stable, lower => slower convergence)\n  rate_annealing : float\n    Learning rate annealing: \\eqn{(rate)/(1 + rate_annealing*samples)\n  rate_decay : float\n    Learning rate decay factor between layers (N-th layer: \\eqn{rate*\x07lpha^(N-1))\n  momentum_start : float\n    Initial momentum at the beginning of training (try 0.5)\n  momentum_ramp : float\n    Number of training samples for which momentum increases\n  momentum_stable : float\n    Final momentum after the amp is over (try 0.99)\n  nesterov_accelerated_gradient : bool\n    Logical. Use Nesterov accelerated gradient (recommended)\n  input_dropout_ratio : float\n    A fraction of the features for each training row to be omitted from training in order to improve generalization (dimension sampling).\n  hidden_dropout_ratios : float\n    Input layer dropout ratio (can improve generalization) specify one value per hidden layer, defaults to 0.5\n  l1 : float\n    L1 regularization (can add stability and improve generalization, causes many weights to become 0)\n  l2 : float\n    L2 regularization (can add stability and improve generalization, causes many weights to be small)\n  max_w2 : float\n    Constraint for squared sum of incoming weights per unit (e.g. Rectifier)\n  initial_weight_distribution : str\n    Can be "Uniform", "UniformAdaptive", or "Normal"\n  initial_weight_scale : str\n    Uniform: -value ... value, Normal: stddev\n  loss : str\n    Loss function: "Automatic", "CrossEntropy" (for classification only), "Quadratic", "Absolute" (experimental) or "Huber" (experimental)\n  distribution : str\n     A character string. The distribution function of the response. Must be "AUTO", "bernoulli", "multinomial", "poisson", "gamma", "tweedie", "laplace",\n     "huber" or "gaussian"\n  tweedie_power : float\n    Tweedie power (only for Tweedie distribution, must be between 1 and 2)\n  score_interval : int\n    Shortest time interval (in secs) between model scoring\n  score_training_samples : int\n    Number of training set samples for scoring (0 for all)\n  score_validation_samples : int\n    Number of validation set samples for scoring (0 for all)\n  score_duty_cycle : float\n    Maximum duty cycle fraction for scoring (lower: more training, higher: more scoring)\n  classification_stop : float\n    Stopping criterion for classification error fraction on training data (-1 to disable)\n  regression_stop : float\n    Stopping criterion for regression error (MSE) on training data (-1 to disable)\n  quiet_mode : bool\n    Enable quiet mode for less output to standard output\n  max_confusion_matrix_size : int\n    Max. size (number of classes) for confusion matrices to be shown\n  max_hit_ratio_k : float\n    Max number (top K) of predictions to use for hit ratio computation(for multi-class only, 0 to disable)\n  balance_classes : bool\n    Balance training data class counts via over/under-sampling (for imbalanced data)\n  class_sampling_factors : list\n    Desired over/under-sampling ratios per class (in lexicographic order). If not specified, sampling factors will be automatically computed to\n    obtain class balance during training. Requires balance_classes.\n  max_after_balance_size : float\n    Maximum relative size of the training data after balancing class counts (can be less than 1.0)\n  score_validation_sampling :\n    Method used to sample validation dataset for scoring\n  diagnostics : bool\n    Enable diagnostics for hidden layers\n  variable_importances : bool\n    Compute variable importances for input features (Gedeon method) - can be slow for large networks)\n  fast_mode : bool\n    Enable fast mode (minor approximations in back-propagation)\n  ignore_const_cols : bool\n    Ignore constant columns (no information can be gained anyway)\n  force_load_balance : bool\n    Force extra load balancing to increase training speed for small datasets (to keep all cores busy)\n  replicate_training_data : bool\n    Replicate the entire training dataset onto every node for faster training\n  single_node_mode : bool\n    Run on a single node for fine-tuning of model parameters\n  shuffle_training_data : bool\n    Enable shuffling of training data (recommended if training data is replicated and train_samples_per_iteration is close to \\eqn{numRows*numNodes\n  sparse : bool\n    Sparse data handling (Experimental)\n  col_major : bool\n    Use a column major weight matrix for input layer. Can speed up forward propagation, but might slow down backpropagation (Experimental)\n  average_activation : float\n    Average activation for sparse auto-encoder (Experimental)\n  sparsity_beta : bool\n    Sparsity regularization (Experimental)\n  max_categorical_features : int\n    Max. number of categorical features, enforced via hashing Experimental)\n  reproducible : bool\n    Force reproducibility on small data (will be slow - only uses 1 thread)\n  export_weights_and_biases : bool\n    Whether to export Neural Network weights and biases to H2O Frames"\n  offset_column : H2OFrame\n    Specify the offset column.\n  weights_column : H2OFrame\n    Specify the weights column.\n  nfolds : int\n    (Optional) Number of folds for cross-validation. If nfolds >= 2, then validation must remain empty.\n  fold_column : H2OFrame\n    (Optional) Column with cross-validation fold index assignment per observation\n  fold_assignment : str\n    Cross-validation fold assignment scheme, if fold_column is not specified Must be "AUTO", "Random" or "Modulo"\n  keep_cross_validation_predictions : bool\n    Whether to keep the predictions of the cross-validation models\n\n\n :return: Return a new classifier or regression model.\n  '
    parms = {k: v for (k, v) in locals().items() if ((k in ['y', 'training_frame', 'validation_frame', 'validation_x', 'validation_y', 'offset_column', 'weights_column', 'fold_column']) or (v is not None))}
    parms['algo'] = 'deeplearning'
    return h2o_model_builder.supervised(parms)
